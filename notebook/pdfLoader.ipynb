{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "17d1f940",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader, PyMuPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "88840e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 PDF files to process\n",
      "\n",
      "Processing: attention.pdf\n",
      "  ✓ Loaded 22 pages\n",
      "\n",
      "Processing: embeddings.pdf\n",
      "  ✓ Loaded 27 pages\n",
      "\n",
      "Processing: objectdetection.pdf\n",
      "  ✓ Loaded 11 pages\n",
      "\n",
      "Processing: proposal.pdf\n",
      "  ✓ Loaded 8 pages\n",
      "\n",
      "Total documents loaded: 68\n"
     ]
    }
   ],
   "source": [
    "### Read all the pdf's inside the directory\n",
    "def process_all_pdfs(pdf_directory):\n",
    "    \"\"\"Process all PDF files in a directory\"\"\"\n",
    "    all_documents = []\n",
    "    pdf_dir = Path(pdf_directory)\n",
    "    \n",
    "    # Find all PDF files recursively\n",
    "    pdf_files = list(pdf_dir.glob(\"**/*.pdf\"))\n",
    "    \n",
    "    print(f\"Found {len(pdf_files)} PDF files to process\")\n",
    "    \n",
    "    for pdf_file in pdf_files:\n",
    "        print(f\"\\nProcessing: {pdf_file.name}\")\n",
    "        try:\n",
    "            loader = PyPDFLoader(str(pdf_file))\n",
    "            documents = loader.load()\n",
    "            \n",
    "            # Add source information to metadata\n",
    "            for doc in documents:\n",
    "                doc.metadata['source_file'] = pdf_file.name\n",
    "                doc.metadata['file_type'] = 'pdf'\n",
    "            \n",
    "            all_documents.extend(documents)\n",
    "            print(f\"  ✓ Loaded {len(documents)} pages\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ✗ Error: {e}\")\n",
    "    \n",
    "    print(f\"\\nTotal documents loaded: {len(all_documents)}\")\n",
    "    return all_documents\n",
    "\n",
    "# Process all PDFs in the data directory\n",
    "all_pdf_documents = process_all_pdfs(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6daba794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 0, 'page_label': '1', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Attention Mechanism in Neural Networks:\\nWhere it Comes and Where it Goes\\nDerya Soydaner\\nReceived: 22 July 2021 / Accepted: 27 April 2022\\nAbstract A long time ago in the machine learning literature, the idea of\\nincorporating a mechanism inspired by the human visual system into neural\\nnetworks was introduced. This idea is named the attention mechanism, and it\\nhas gone through a long development period. Today, many works have been\\ndevoted to this idea in a variety of tasks. Remarkable performance has re-\\ncently been demonstrated. The goal of this paper is to provide an overview\\nfrom the early work on searching for ways to implement attention idea with\\nneural networks until the recent trends. This review emphasizes the impor-\\ntant milestones during this progress regarding diﬀerent tasks. By this way,\\nthis study aims to provide a road map for researchers to explore the current\\ndevelopment and get inspired for novel approaches beyond the attention.\\nKeywords Attention mechanism ·Neural networks ·Deep learning ·Survey\\n1 Introduction\\nHuman eye sees the world in an interesting way. We suppose as if we see the\\nentire scene at once, but this is an illusion created by the subconscious part\\nof our brain [1]. According to the Scanpath theory [2,3], when the human eye\\nlooks at an image, it can see only a small patch in high resolution. This small\\npatch is called thefovea. It can see the rest of the image in low resolution which\\nis called the periphery. To recognize the entire scene, the eye performs feature\\nextraction based on the fovea. The eye is moved to diﬀerent parts of the image\\nuntil the information obtained from the fovea is suﬃcient for recognition [4].\\nThese eye movements are called saccades. The eye makes successive ﬁxations\\nDerya Soydaner\\nDepartment of Brain and Cognition, University of Leuven (KU Leuven), Leuven, Belgium\\nTel.: +32-16710471\\nE-mail: derya.soydaner@kuleuven.be\\narXiv:2204.13154v1  [cs.LG]  27 Apr 2022'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='2 Derya Soydaner\\nuntil the recognition task is complete. This sequential process happens so\\nquickly that we feel as if it happens all at once.\\nBiologically, this is called visual attention system . Visual attention is de-\\nﬁned as the ability to dynamically restrict processing to a subset of the visual\\nﬁeld [5]. It seeks answers for two main questions: What and where to look?\\nVisual attention has been extensively studied in psychology and neuroscience;\\nfor reviews see [6,7,8,9,10]. Besides, there is a large amount of literature on\\nmodeling eye movements [11,12,13,14]. These studies have been a source of\\ninspiration for many artiﬁcial intelligence tasks. It has been discovered that\\nthe attention idea is useful from image recognition to machine translation.\\nTherefore, diﬀerent types of attention mechanisms inspired from the human\\nvisual system have been developed for years. Since the success of deep neural\\nnetworks has been at the forefront for these artiﬁcial intelligence tasks, these\\nmechanisms have been integrated into neural networks for a long time.\\nThis survey is about the journey of attention mechanisms used with neu-\\nral networks. Researchers have been investigating ways to strengthen neural\\nnetwork architectures with attention mechanisms for many years. The pri-\\nmary aim of these studies is to reduce computational burden and to improve\\nthe model performance as well. Previous work reviewed the attention mecha-\\nnisms from diﬀerent perspectives [15], or examined them in context of natural\\nlanguage processing (NLP) [16,17]. However, in this study, we examine the\\ndevelopment of attention mechanisms over the years, and recent trends. We\\nbegin with the ﬁrst attempts to integrate the visual attention idea to neural\\nnetworks, and continue until the most modern neural networks armed with at-\\ntention mechanisms. One of them is the Transformer, which is used for many\\nstudies including the GPT-3 language model [18], goes beyond convolutions\\nand recurrence by replacing them with only attention layers [19]. Finally, we\\ndiscuss how much more can we move forward, and what’s next?\\n2 From the Late 1980s to Early 2010s: The Attention Awakens\\nThe ﬁrst attempts at adapting attention mechanisms to neural networks go\\nback to the late 1980s. One of the early studies is the improved version of\\nthe Neocognitron [20] with selective attention [21]. This study is then mod-\\niﬁed to recognize and segment connected characters in cursive handwriting\\n[22]. Another study describes VISIT, a novel model that concentrates on its\\nrelationship to a number of visual areas of the brain [5]. Also, a novel archi-\\ntecture named Signal Channelling Attentional Network (SCAN) is presented\\nfor attentional scanning [23].\\nEarly work on improving the attention idea for neural networks includes\\na variety of tasks such as target detection [24]. In another study, a visual at-\\ntention system extracts regions of interest by combining the bottom-up and\\ntop-down information from the image [25]. A recognition model based on se-\\nlective attention which analyses only a small part of the image at each step,\\nand combines results in time is described [4]. Besides, a model based on the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 2, 'page_label': '3', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Attention Mechanism in Neural Networks: 3\\nconcept of selective tuning is proposed [26]. As the years go by, several studies\\nthat use the attention idea in diﬀerent ways have been presented for visual\\nperception and recognition [27,28,29,30].\\nBy the 2000s, the studies on making attention mechanisms more useful for\\nneural networks continued. In the early years, a model that integrates an at-\\ntentional orienting where pathway and an object recognition what pathway is\\npresented [31]. A computational model of human eye movements is proposed\\nfor an object class detection task [32]. A serial model is presented for visual pat-\\ntern recognition gathering Markov models and neural networks with selective\\nattention on the handwritten digit recognition and face recognition problems\\n[33]. In that study, a neural network analyses image parts and generates pos-\\nterior probabilities as observations to the Markov model. Also, attention idea\\nis used for object recognition [34], and the analysis of a scene [35]. An inter-\\nesting study proposes to learn sequential attention in real-world visual object\\nrecognition using a Q-learner [36]. Besides, a computational model of visual\\nselective attention is described to automatically detect the most relevant parts\\nof a color picture displayed on a television screen [37]. The attention idea is\\nalso used for identifying and tracking objects in multi-resolution digital video\\nof partially cluttered environments [38].\\nIn 2010, the ﬁrst implemented system inspired by the fovea of human retina\\nwas presented for image classiﬁcation [39]. This system jointly trains a re-\\nstricted Boltzmann machine (RBM) and an attentional component called the\\nﬁxation controller. Similarly, a novel attentional model is implemented for si-\\nmultaneous object tracking and recognition that is driven by gaze data [40].\\nBy taking advantage of reinforcement learning, a novel recurrent neural net-\\nwork (RNN) is described for image classiﬁcation [41]. Deep Attention Selective\\nNetwork (DasNet), a deep neural network with feedback connections that are\\nlearned through reinforcement learning to direct selective attention to certain\\nfeatures extracted from images, is presented [42]. Additionally, a deep learning\\nbased framework using attention has been proposed for generative modeling\\n[43].\\n3 2015: The Rise of Attention\\nIt can be said that 2015 is the golden year of attention mechanisms. Because\\nthe number of attention studies has grown like an avalanche after three main\\nstudies presented in that year. The ﬁrst one proposed a novel approach for\\nneural machine translation (NMT) [44]. As it is known, most of the NMT\\nmodels belong to a family of encoder-decoders [45,46], with an encoder and a\\ndecoder for each language. However, compressing all the necessary information\\nof a source sentence into a ﬁxed-length vector is an important disadvantage of\\nthis encoder-decoder approach. This usually makes it diﬃcult for the neural\\nnetwork to capture all the semantic details of a very long sentence [1].\\nThe idea that [44] introduced is an extension to the conventional NMT\\nmodels. This extension is composed of an encoder and decoder as shown in'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 3, 'page_label': '4', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='4 Derya Soydaner\\nFig. 1 The extension to the conventional NMT models that is proposed by [44]. It generates\\nthe t-th target word yt given a source sentence ( x1,x2,...,x T ).\\nFig 1. The ﬁrst part, encoder, is a bidirectional RNN (BiRNN) [47] that takes\\nword vectors as input. The forward and backward states of BiRNN are com-\\nputed. Then, an annotation aj for each word xj is obtained by concatenating\\nthese forward and backward hidden states. Thus, the encoder maps the input\\nsentence to a sequence of annotations ( a1,...,a Tx ). By using a BiRNN rather\\nthan conventional RNN, the annotation of each word can summarize both\\nthe preceding words and the following words. Besides, the annotation aj can\\nfocus on the words around xj because of the inherent nature of RNNs that\\nrepresenting recent inputs better.\\nIn decoder, a weight αij of each annotation aj is obtained by using its\\nassociated energy eij that is computed by a feedforward neural network f as\\nin Eq. (1). This neural network f is deﬁned as an alignment model that can\\nbe jointly trained with the proposed architecture. In order to reduce compu-\\ntational burden, a multilayer perceptron (MLP) with a single hidden layer is\\nproposed as f. This alignment model tells us about the relation between the\\ninputs around position j and the output at position i. By this way, the decoder\\napplies an attention mechanism. As it is seen in Eq. (2), the αij is the output\\nof softmax function:\\neij = f(hi−1,aj) (1)\\nαij = exp(eij)∑Tx\\nk=1 exp(eik)\\n(2)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 4, 'page_label': '5', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Attention Mechanism in Neural Networks: 5\\nHere, the probability αij determines the importance of annotation aj with\\nrespect to the previous hidden state hi−1. Finally, the context vector ci is\\ncomputed as a weighted sum of these annotations as follows [44]:\\nci =\\nTx∑\\nj=1\\nαijaj (3)\\nBased on the decoder state, the context and the last generated word, the\\ntarget word yt is predicted. In order to generate a word in a translation, the\\nmodel searches for the most relevant information in the source sentence to\\nconcentrate. When it ﬁnds the appropriate source positions, it makes the pre-\\ndiction. By this way, the input sentence is encoded into a sequence of vectors\\nand a subset of these vectors is selected adaptively by the decoder that is rel-\\nevant to predicting the target [44]. Thus, it is no longer necessary to compress\\nall the information of a source sentence into a ﬁxed-length vector.\\nThe second study is the ﬁrst visual attention model in image captioning\\n[48]. Diﬀerent from the previous study [44], it uses a deep convolutional neural\\nnetwork (CNN) as an encoder. This architecture is an extension of the neural\\nnetwork [49] that encodes an image into a compact representation, followed by\\nan RNN that generates a corresponding sentence. Here, the annotation vectors\\nai ∈RD are extracted from a lower convolutional layer, each of which is a D-\\ndimensional representation corresponding to a part of the image. Thus, the\\ndecoder selectively focuses on certain parts of an image by weighting a subset\\nof all the feature vectors [48]. This extended architecture uses attention for\\nsalient features to dynamically come to the forefront instead of compressing\\nthe entire image into a static representation.\\nThe context vector ct represents the relevant part of the input image at\\ntime t. The weight αi of each annotation vector is computed similar to Eq. (2),\\nwhereas its associated energy is computed similar to Eq. (1) by using an MLP\\nconditioned on the previous hidden state ht−1. The remarkable point of this\\nstudy is a new mechanism φ that computes ct from the annotation vectors ai\\ncorresponding to the features extracted at diﬀerent image locations:\\nct = φ(\\n{\\nai\\n}\\n,\\n{\\nαi\\n}\\n) (4)\\nThe deﬁnition of the φ function causes two variants of attention mecha-\\nnisms: The hard (stochastic) attention mechanism is trainable by maximizing\\nan approximate variational lower bound, i.e., by REINFORCE [50]. On the\\nother side, the soft (deterministic) attention mechanism is trainable by stan-\\ndard backpropagation methods. The hard attention deﬁnes a location variable\\nst, and uses it to decide where to focus attention when generating the t-th\\nword. When the hard attention is applied, the attention locations are con-\\nsidered as intermediate latent variables. It assigns a multinoulli distribution\\nparametrized by αi, and ct becomes a random variable. Here, st,i is deﬁned\\nas a one-hot variable which is set to 1 if the i-th location is used to extract\\nvisual features [48]:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 5, 'page_label': '6', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='6 Derya Soydaner\\np(st,i = 1|sj<t,a) = αt,i (5)\\nct =\\n∑\\ni\\nst,iai (6)\\nWhereas learning hard attention requires sampling the attention location\\nst each time, the soft attention mechanism computes a weighted annotation\\nvector similar to [44] and takes the expectation of the context vectorct directly:\\nEp(st|α)[ct] =\\nL∑\\ni=1\\nαt,iai (7)\\nFurthermore, in training the deterministic version of the model, an alterna-\\ntive method namely doubly stochastic attention, is proposed with an additional\\nconstraint added to the training objective to encourage the model to pay equal\\nattention to all parts of the image.\\nThe third study should be emphasized presents two classes of attention\\nmechanisms for NMT: the global attention that always attends to all source\\nwords, and the local attention that only looks at a subset of source words at\\na time [51]. These mechanisms derive the context vector ct in diﬀerent ways:\\nWhereas the global attention considers all the hidden states of the encoder, the\\nlocal one selectively focuses on a small window of context. In global attention, a\\nvariable-length alignment vector is derived similar to Eq. (2). Here, the current\\ntarget hidden state ht is compared with each source hidden state ¯hs by using a\\nscore function instead of the associated energy eij. Thus, the alignment vector\\nwhose size equals the number of time steps on the source side is derived.\\nGiven the alignment vector as weights, the context vector ct is computed as\\nthe weighted average over all the source hidden states. Here, score is referred\\nas a content-based function, and three diﬀerent alternatives are considered [51].\\nOn the other side, the local attention is diﬀerentiable. Firstly, an aligned\\nposition pt is generated for each target word at a time t. Then, a window\\ncentered around the source position pt is used to compute the context vector\\nas a weighted average of the source hidden states within the window. The\\nlocal attention selectively focuses on a small window of context, and obtains\\nthe alignment vector from the current target state ht and the source states ¯hs\\nin the window [51].\\nThe introduction of these novel mechanisms in 2015 triggered the rise of\\nattention for neural networks. Based on the proposed attention mechanisms,\\nsigniﬁcant research has been conducted in a variety of tasks. In order to imag-\\nine the attention idea in neural networks better, two visual examples are shown\\nin Fig. 2. A neural image caption generation task is seen in the top row that\\nimplements an attention mechanism [48]. Then, the second example shows\\nhow the attention mechanisms can be used for visual question answering [52].\\nBoth examples demonstrate how attention mechanisms focus on parts of input\\nimages.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 6, 'page_label': '7', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Attention Mechanism in Neural Networks: 7\\nFig. 2 Examples of the attention mechanism in visual. (Top) Attending to the correct\\nobject in neural image caption generation [48]. (Bottom) Visualization of original image\\nand question pairs, and co-attention maps namely word-level, phrase-level and question-\\nlevel, respectively [52].\\n4 2015-2016: Attack of the Attention\\nDuring two years from 2015, the attention mechanisms were used for diﬀerent\\ntasks, and novel neural network architectures were presented applying these\\nmechanisms. After the memory networks [53] that require a supervision signal\\ninstructing them how to use their memory cells, the introduction of the neural\\nTuring machine [54] allows end-to-end training without this supervision signal,\\nvia the use of a content-based soft attention mechanism [1]. Then, end-to-end\\nmemory network [55] that is a form of memory network based on a recurrent\\nattention mechanism is proposed.\\nIn these years, an attention mechanism called self-attention, sometimes\\ncalled intra-attention, was successfully implemented within a neural network\\narchitecture namely Long Short-Term Memory-Networks (LSTMN) [56]. It\\nmodiﬁes the standard LSTM structure by replacing the memory cell with a\\nmemory network [53]. This is because memory networks have a set of key\\nvectors and a set of value vectors, whereas LSTMs maintain a hidden vector\\nand a memory vector [56]. In contrast to attention idea in [44], memory and\\nattention are added within a sequence encoder in LSTMN. In order to compute\\na representation of a sequence, self-attention is described as relating diﬀerent\\npositions of it [19]. One of the ﬁrst approaches of self-attention is applied for\\nnatural language inference [57].\\nMany attention-based models have been proposed for neural image cap-\\ntioning [58], abstractive sentence summarization [59], speech recognition [60,\\n61], automatic video captioning [62], neural machine translation [63], and rec-\\nognizing textual entailment [64]. Diﬀerent attention-based models perform vi-\\nsual question answering [65,66,67]. An attention-based CNN is presented for\\nmodeling sentence pairs [68]. A recurrent soft attention based model learns to\\nfocus selectively on parts of the video frames and classiﬁes videos [69].\\nOn the other side, several neural network architectures have been pre-\\nsented in a variety of tasks. For instance, Stacked Attention Network (SAN)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 7, 'page_label': '8', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='8 Derya Soydaner\\nis described for image question answering [70]. Deep Attention Recurrent Q-\\nNetwork (DARQN) integrates soft and hard attention mechanisms into the\\nstructure of Deep Q-Network (DQN) [71]. Wake-Sleep Recurrent Attention\\nModel (WS-RAM) speeds up the training time for image classiﬁcation and\\ncaption generation tasks [72]. alignDRAW model, an extension of the Deep\\nRecurrent Attention Writer (DRAW) [73], is a generative model of images\\nfrom captions using a soft attention mechanism [74]. Generative Adversarial\\nWhat-Where Network (GAWWN) synthesizes images given instructions de-\\nscribing what content to draw in which location [75].\\n5 The Transformer: Return of the Attention\\nAfter the proposed attention mechanisms in 2015, researchers published stud-\\nies that mostly modifying or implementing them to diﬀerent tasks. However,\\nin 2017, a novel neural network architecture, namely the Transformer, based\\nentirely on self-attention was presented [19]. The Transformer achieved great\\nresults on two machine translation tasks in addition to English constituency\\nparsing. The most impressive point about this architecture is that it contains\\nneither recurrence nor convolution. The Transformer performs well by replac-\\ning the conventional recurrent layers in encoder-decoder architecture used for\\nNMT with self-attention.\\nThe Transformer is composed of encoder-decoder stacks each of which has\\nsix identical layers within itself. In Fig. 3, one encoder-decoder stack is shown\\nto illustrate the model [19]. Each stack includes only attention mechanisms\\nand feedforward neural networks. As this architecture does not include any\\nrecurrent or convolutional layer, information about the relative or absolute\\npositions in the input sequence is given at the beginning of both encoder and\\ndecoder using positional encodings.\\nThe calculations of self-attention are slightly diﬀerent from the mechanisms\\ndescribed so far in this paper. It uses three vectors namely query, key and\\nvalue for each word. These vectors are computed by multiplying the input with\\nweight matrices Wq, Wk and Wv which are learned during training. In general,\\neach value is weighted by a function of the query with the corresponding key.\\nThe output is computed as a weighted sum of the values. Based on this idea,\\ntwo attention mechanisms are proposed: In the ﬁrst one, called scaled dot-\\nproduct attention, the dot products of the query with all keys are computed\\nas given in the right side of Fig. 3. Each result is divided to the square root of\\nthe dimension of the keys to have more stable gradients. They pass into the\\nsoftmax function, thus the weights for the values are obtained. Finally each\\nsoftmax score is multiplied with the value as given in Eq. (8). The authors\\npropose computing the attention on a set of queries simultaneously by taking\\nqueries and keys of dimension dk, and values of dimension dv as inputs. The\\nkeys, queries and values are packed together into matricesK, Q and V. Finally,\\nthe output matrix is obtained as follows [19]:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 8, 'page_label': '9', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Attention Mechanism in Neural Networks: 9\\nFig. 3 The Transformer architecture and the attention mechanisms it uses in detail [19].\\n(Left) The Transformer with one encoder-decoder stack. (Center) Multi-head attention.\\n(Right) Scaled dot-product attention.\\nAttention(Q,K,V ) = softmax(QKT\\n√dk\\n)V (8)\\nThis calculation is performed by every word against the other words. This\\nleads to having values of each word relative to each other. For instance, if\\nthe word x2 is not relevant for the word x1, then the softmax score gives low\\nprobability scores. As a result, the corresponding value is decreased. This leads\\nto an increase in the value of relevant words, and those of others decrease. In\\nthe end, every word obtains a new value for itself.\\nAs seen from Fig. 3, the Transformer model does not directly use scaled\\ndot-product attention. But the attention mechanism it uses is based on these\\ncalculations. The second mechanism proposed, called the multi-head attention,\\nlinearly projects the queries, keys and values h times with diﬀerent, learned\\nlinear projections to dq, dk and dv dimensions, respectively [19]. The attention\\nfunction is performed in parallel on each of these projected versions of queries,\\nkeys and values, i.e., heads. By this way, dv-dimensional output values are\\nobtained. In order to get the ﬁnal values, they are concatenated and projected\\none last time as shown in the center of Fig. 3. By this way, the self-attention is\\ncalculated multiple times using diﬀerent sets of query, key and value vectors.\\nThus, the model can jointly attend to information at diﬀerent positions [19]:\\nMultiHead(Q,K,V ) = Concat(head1,...,head h)WO (9)\\nwherehead i = Attention(QWQ\\ni ,KW K\\ni ,VW V\\ni )\\nIn the decoder part of the Transformer, masked multi-head attention is\\napplied ﬁrst to ensure that only previous word embeddings are used when\\ntrying to predict the next word in the sentence. Therefore, the embeddings\\nthat shouldn’t be seen by the decoder are masked by multiplying with zero.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 9, 'page_label': '10', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='10 Derya Soydaner\\nAn interesting study examines the contribution made by individual atten-\\ntion heads in the encoder [76]. Also, there is an evaluation of the eﬀects of\\nself-attention on gradient propagation in recurrent networks [77]. For a deeper\\nanalysis of multi-head self-attention mechanism from a theoretical perspective\\nsee [78].\\nSelf-attention has been used successfully in a variety of tasks including\\nsentence embedding [79] and abstractive summarization [80]. It is shown that\\nself-attention can lead to improvements to discriminative constituency parser\\n[81], and speech recognition as well [82,83]. Also, the listen-attend-spell model\\n[84] has been improved with the self-attention for acoustic modeling [85].\\nAs soon as these self-attention mechanisms were proposed, they have been\\nincorporated with deep neural networks for a wide range of tasks. For instance,\\na deep learning model learned a number of large-scale tasks from multiple do-\\nmains with the aid of self-attention mechanism [86]. Novel self-attention neural\\nmodels are proposed for cross-target stance classiﬁcation [87] and NMT [88].\\nAnother study points out that a fully self-attentional model can reach com-\\npetitive predictive performance on ImageNet classiﬁcation and COCO object\\ndetection tasks [89]. Besides, developing novel attention mechanisms has been\\ncarried out such as area attention, a novel mechanism that can be used along\\nmulti-head attention [90]. It attends to areas in the memory by deﬁning the\\nkey of an area as the mean vector of the key of each item, and deﬁning the\\nvalue as the sum of all value vectors in the area.\\nWhen a novel mechanism is proposed, it is inevitable to incorporate it\\ninto the GAN framework [91]. Self-Attention Generative Adversarial Networks\\n(SAGANs) [92] introduce a self-attention mechanism into convolutional GANs.\\nDiﬀerent from the traditional convolutional GANs, SAGAN generates high-\\nresolution details using cues from all feature locations. Similarly, Attentional\\nGenerative Adversarial Network (AttnGAN) is presented for text to image\\ngeneration [93]. On the other side, a machine reading and question answering\\narchitecture called QANet [94] is proposed without any recurrent networks. It\\nuses self-attention to learn the global interaction between each pair of words\\nwhereas convolution captures the local structure of the text. In another study,\\nGated Attention Network (GaAN) controls the importance of each attention\\nhead’s output by introducing gates [95]. Another interesting study introduces\\nattentive group convolutions with a generalization of visual self-attention [96].\\nA deep transformer model is implemented for language modeling over long\\nsequences [97].\\n5.1 Self-attention variants\\nIn recent years, self-attention has become an important research direction\\nwithin the deep learning community. Self-attention idea has been examined\\nin diﬀerent aspects. For example, self-attention is handled in a multi-instance\\nlearning framework [98]. The idea of Sparse Adaptive Connection (SAC) is\\npresented for accelerating and structuring self-attention [99]. The research on'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 10, 'page_label': '11', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Attention Mechanism in Neural Networks: 11\\nTable 1 Summary of Notation\\nSymbol Deﬁnition\\na annotation\\nc context vector\\nα weight\\ne energy\\nf feedforward neural network\\nh hidden state\\nφ hard (stochastic) / soft (deterministic) attention\\ns location variable\\np source position\\nK, Q, V keys, queries and values matrices, respectively\\nWq, Wk, Wv weight matrices for queries, keys and values, respectively\\nimproving self-attention continues as well [100,101,102]. Besides, based on the\\nself-attention mechanisms proposed in the Transformer, important studies that\\nmodify the self-attention have been presented. Some of the most recent and\\nprominent studies are summarized below.\\nRelation-aware self-attention It extends the self-attention mechanism by\\nregarding representations of the relative positions, or distances between se-\\nquence elements [103]. Thus, it can consider the pairwise relationships between\\ninput elements. This type of attention mechanism deﬁnes vectors to represent\\nthe edge between two inputs. It provides learning two distinct edge represen-\\ntations that can be shared across attention heads without requiring additional\\nlinear transformations.\\nDirectional self-attention (DiSA) A novel neural network architecture\\nfor learning sentence embedding named Directional Self-Attention Network\\n(DiSAN) [104] uses directional self-attention followed by a multi-dimensional\\nattention mechanism. Instead of computing a single importance score for each\\nword based on the word embedding, multi-dimensional attention computes a\\nfeature-wise score vector for each token. To extend this mechanism to the self-\\nattention, two variants are presented: The ﬁrst one, called multi-dimensional\\n‘token2token’ self-attention generates context-aware coding for each element.\\nThe second one, called multi-dimensional ‘source2token’ self-attention com-\\npresses the sequence into a vector [104]. On the other side, directional self-\\nattention produces context-aware representations with temporal information\\nencoded by using positional masks. By this way, directional information is en-\\ncoded. First, the input sequence is transformed to a sequence of hidden states\\nby a fully connected layer. Then, multi-dimensional token2token self-attention\\nis applied to these hidden states. Hence, context-aware vector representations\\nare generated for all elements from the input sequence.\\nReinforced self-attention (ReSA)A sentence-encoding model named Re-\\ninforced Self-Attention Network (ReSAN)uses reinforced self-attention (ReSA)\\nthat integrates soft and hard attention mechanisms into a single model. ReSA'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 11, 'page_label': '12', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='12 Derya Soydaner\\nselects a subset of head tokens, and relates each head token to a small sub-\\nset of dependent tokens to generate their context-aware representations [105].\\nFor this purpose, a novel hard attention mechanism called reinforced sequence\\nsampling (RSS) , which selects tokens from an input sequence in parallel and\\ntrained via policy gradient, is proposed. Given an input sequence, RSS gener-\\nates an equal-length sequence of binary random variables that indicates both\\nthe selected and discarded ones. On the other side, the soft attention provides\\nreward signals back for training the hard attention. The proposed RSS pro-\\nvides a sparse mask to self-attention. ReSA uses two RSS modules to extract\\nthe sparse dependencies between each pair of selected tokens.\\nOuter product attention (OPA)Self-Attentive Associative Memory (SAM)\\nis a novel operator based upon outer product attention (OPA) [106]. This at-\\ntention mechanism is an extension of dot-product attention [19]. OPA diﬀers\\nusing element-wise multiplication, outer product, and tanh function instead of\\nsoftmax.\\nBidirectional block self-attention (Bi-BloSA)Another mechanism, bidi-\\nrectional block self-attention (Bi-BloSA) which is simply a masked block self-\\nattention (mBloSA) with forward and backward masks to encode the tempo-\\nral order information is presented [107]. Here, mBloSA is composed of three\\nparts from its bottom to top namely intra-block self-attention , inter-block\\nself-attention and the context fusion . It splits a sequence into several length-\\nequal blocks, and applies an intra-block self-attention to each block indepen-\\ndently. Then, inter-block self-attention processes the outputs for all blocks.\\nThis stacked self-attention model results a reduction in the amount of mem-\\nory compared to a single one applied to the whole sequence. Finally, a feature\\nfusion gate combines the outputs of intra-block and inter-block self-attention\\nwith the original input, to produce the ﬁnal context-aware representations of\\nall tokens.\\nFixed multi-head attentionThe ﬁxed multi-head attention proposes ﬁxing\\nthe head size of the Transformer in the aim of improving the representation\\npower [108]. This study emphasizes its importance by setting the head size of\\nattention units to input sequence length.\\nSparse sinkhorn attentionIt is based on the idea of diﬀerentiable sorting\\nof internal representations within the self-attention module [109]. Instead of\\nallowing tokens to only attend to tokens within the same block, it operates\\non block sorted sequences. Each token attends to tokens in the sorted block.\\nThus, tokens that may be far apart in the unsorted sequence can be considered.\\nAdditionally, a variant of this mechanism named SortCut sinkhorn attention\\napplies a post-sorting truncation of the input sequence.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 12, 'page_label': '13', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Attention Mechanism in Neural Networks: 13\\nAdaptive attention spanAdaptive attention span is proposed as an alter-\\nnative to self-attention [110]. It learns the attention span of each head inde-\\npendently. To this end, a masking function inspired by [111] is used to control\\nthe attention span for each head. The purpose of this novel mechanism is to\\nreduce the computational burden of the Transformer. Additionally, dynamic\\nattention span approach is presented to dynamically change the attention span\\nbased on the current input as an extension [51,112].\\n5.2 Transformer variants\\nDiﬀerent from developing novel self-attention mechanisms, several studies have\\nbeen published in the aim of improving the performance of the Transformer.\\nThese studies mostly modify the model architecture. For instance, an addi-\\ntional recurrence encoder is preferred to model recurrence for Transformer di-\\nrectly [113]. In another study, a new weight initialization scheme is applied to\\nimprove Transformer optimization [114]. A novel positional encoding scheme\\nis used to extend the Transformer to tree-structured data [115]. Investigating\\nmodel size by handling Transformer width and depth for eﬃcient training is\\nalso an active research area [116]. Transformer is used in reinforcement learn-\\ning settings [117,118,119] and for time series forecasting in adversarial training\\nsetting [120].\\nBesides, many Transformer variants have been presented in the recent past.\\nCOMmonsEnse Transformer (COMET) is introduced for automatic construc-\\ntion of commonsense knowledge bases [121]. Evolved Transformer applies neu-\\nral architecture search for a better Transformer model [122]. Transformer Au-\\ntoencoder is a sequential autoencoder for conditional music generation [123].\\nCrossTransformer takes a small number of labeled images and an unlabeled\\nquery, and computes distances between spatially-corresponding features to in-\\nfer class membership [124]. DEtection TRansformer (DETR) is a new design\\nfor object detection systems [125], and Deformable DETR is an improved ver-\\nsion that achieves better performance in less time [126]. FLOw-bAsed Trans-\\nformER (FLOATER) emphasizes the importance of position encoding in the\\nTransformer, and models the position information via a continuous dynamical\\nmodel [127]. Disentangled Context (DisCo) Transformer simultaneously gener-\\nates all tokens given diﬀerent contexts by predicting every word in a sentence\\nconditioned on an arbitrary subset of the rest of the words [128]. Genera-\\ntive Adversarial Transformer (GANsformer) is presented for visual generative\\nmodeling [129].\\nRecent work has demonstrated signiﬁcant performance on NLP tasks. In\\nOpenAI GPT, there is a left-to-right architecture, where every token can only\\nattend to previous tokens in the self-attention layers of the Transformer [130].\\nGPT-2 [131] and GPT-3 [18] models have improved the progress. In addition\\nto these variants, some prominent Transformer-based models are summarized\\nbelow.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 13, 'page_label': '14', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='14 Derya Soydaner\\nUniversal Transformer A generalization of the Transformer model named\\nthe Universal Transformer [132] iteratively computes representations Ht at\\nstep t for all positions in the sequence in parallel. To this end, it uses the\\nscaled dot-product attention in Eq. (8) where d is the number of columns\\nof Q, K and V. In the Universal Transformer, the multi-head self-attention\\nwith k heads is used. The representations Ht is mapped to queries, keys and\\nvalues with aﬃne projections using learned parameter matrices WQ ∈ℜd×d/k,\\nWK ∈ℜd×d/k, WV ∈ℜd×d/k and WO ∈ℜd×d [132]:\\nMultiHead(Ht) = Concat(head1,...,head k)WO (10)\\nwherehead i = Attention(HtWQ\\ni ,HtWK\\ni ,HtWV\\ni )\\nImage TransformerImage Transformer [133] demonstrates that self-attention\\nbased models can also be well-suited for images instead of text. This Trans-\\nformer type restricts the self-attention mechanism to attend to local neigh-\\nborhoods. Thus, the size of images that the model can process is increased.\\nIts larger receptive ﬁelds allow the Image Transformer to signiﬁcantly improve\\nthe model performance on image generation as well as image super-resolution.\\nTransformer-XL This study aims to improve the ﬁxed-length context of the\\nTransformer [19] for language modeling. Transformer-XL [134] makes model-\\ning very long-term dependency possible by reusing the hidden states obtained\\nin previous segments. Hence, information can be propagated through the recur-\\nrent connections. In order to reuse the hidden states without causing temporal\\nconfusion, Transformer-XL uses relative positional encodings. Based on this\\narchitecture, a modiﬁed version named the Gated Transformer-XL (GTrXL)\\nis presented in the reinforcement learning setting [135].\\nTensorized TransformerTensorized Transformer[136] compresses the multi-\\nhead attention in Transformer. To this end, it uses a novel self-attention model\\nmulti-linear attention with Block-Term Tensor Decomposition (BTD) [137]. It\\nbuilds a single-block attention based on the Tucker decomposition [138]. Then,\\nit uses a multi-linear attention constructed by a BTD to compress the multi-\\nhead attention mechanism. In Tensorized Transformer, the factor matrices are\\nshared across multiple blocks.\\nBERT The Bidirectional Encoder Representations fromTransformers (BERT)\\naims to pre-train deep bidirectional representations from unlabeled text [139].\\nBERT uses a multilayer bidirectional Transformer as the encoder. Besides,\\ninspired by the Cloze task [140], it has a masked language model pre-training\\nobjective. BERT randomly masks some of the tokens from the input, and pre-\\ndicts the original vocabulary id of the masked word based only on its context.\\nThis model can pre-train a deep bidirectional Transformer. In all layers, the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 14, 'page_label': '15', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Attention Mechanism in Neural Networks: 15\\npre-training is carried out by jointly conditioning on both left and right con-\\ntext. BERT diﬀers from the left-to-right language model pre-training from this\\naspect.\\nRecently, BERT model has been examined in detail. For instance, the be-\\nhaviour of attention heads are analysed [141]. Various methods have been\\ninvestigated for compressing [142,143], pruning [144], and quantization [145].\\nAlso, BERT model has been considered for diﬀerent tasks such as coreference\\nresolution [146]. A novel method is proposed in order to accelerate BERT\\ntraining [147].\\nFurthermore, various BERT variants have been presented. ALBERT aims\\nto increase the training speed of BERT, and presents two parameter reduction\\ntechniques [148]. Similarly, PoWER-BERT [149] is developed to improve the\\ninference time of BERT. This scheme is also used to accelerate ALBERT. Also,\\nTinyBERT is proposed to accelerate inference and reduce model size while\\nmaintaining accuracy [150]. In order to obtain better representations, Span-\\nBERT is proposed as a pre-training method [151]. As a robustly optimized\\nBERT approach, RoBERTa shows that BERT was signiﬁcantly undertrained\\n[152]. Also, DeBERTa improves RoBERTa using the disentangled attention\\nmechanism [153]. On the other side, DistilBERT shows that it is possible to\\nreach similar performances using much smaller language models pre-trained\\nwith knowledge distillation [154]. StructBERT proposes two novel lineariza-\\ntion strategies [155]. Q-BERT is introduced for quantizing BERT models [156],\\nBioBERT is for biomedical text mining [157], and RareBERT is for rare dis-\\nease diagnosis [158].\\nSince 2017 when the Transformer was presented, research directions have\\ngenerally focused on novel self-attention mechanisms, adapting the Trans-\\nformer for various tasks, or making them more understandable. In one of the\\nmost recent studies, NLP becomes possible in the mobile setting with Lite\\nTransformer. It applies long-short range attention where some heads specialize\\nin the local context modeling while the others specialize in the long-distance\\nrelationship modeling [159]. A deep and light-weight Transformer DeLighT\\n[160] and a hypernetwork-based model namely HyperGrid Transformers [161]\\nperform with fewer parameters. Graph Transformer Network is introduced\\nfor learning node representations on heterogeneous graphs [162] and diﬀerent\\napplications are performed for molecular data [163] or textual graph represen-\\ntation [164]. Also, Transformer-XH applies eXtra Hop attention for structured\\ntext data [165]. AttentionXML is a tree-based model for extreme multi-label\\ntext classiﬁcation [166]. Besides, attention mechanism is handled in a Bayesian\\nframework [167]. For a better understanding of Transformers, an identiﬁabil-\\nity analysis of self-attention weights is conducted in addition to presenting\\neﬀective attention to improve explanatory interpretations [168]. Lastly,Vision\\nTransformer (ViT) processes an image using a standard Transformer encoder\\nas used in NLP by interpreting it as a sequence of patches, and performs well\\non image classiﬁcation tasks [169].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 15, 'page_label': '16', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='16 Derya Soydaner\\n5.3 What about complexity?\\nAll these aforementioned studies undoubtedly demonstrate signiﬁcant success.\\nBut success not make one great. The Transformer also brings a very high\\ncomputational complexity and memory cost. The necessity of storing atten-\\ntion matrix to compute the gradients with respect to queries, keys and val-\\nues causes a non-negligible quadratic computation and memory requirements.\\nTraining the Transformer is a slow process for very long sequences because\\nof its quadratic complexity. There is also time complexity which is quadratic\\nwith respect to the sequence length. In order to improve the Transformer in\\nthis respect, recent studies have been conducted to improve this issue. One\\nof them is Linear Transformer which expresses the self-attention as a linear\\ndot-product of kernel feature maps [170]. Linear Transformer reduces both\\nmemory and time complexity by changing the self-attention from the softmax\\nfunction in Eq. (8) to a feature map based dot-product attention. Its per-\\nformance is competitive with the vanilla Transformer architecture on image\\ngeneration and automatic speech recognition tasks while being faster during\\ninference. On the other side, FMMformers which use the idea of thefast multi-\\npole method (FMM) [171] outperform the linear Transformer by decomposing\\nthe attention matrix into near-ﬁeld and far-ﬁeld attention with linear time and\\nmemory complexity [172].\\nAnother suggestion made in response to the Transformer’s quadratic na-\\nture is The Reformer that replaces dot-product attention by one that uses\\nlocality-sensitive hashing [173]. It reduces the complexity but one limitation\\nof the Reformer is its requirement for the queries and keys to be identical. Set\\nTransformer aims to reduce computation time of self-attention from quadratic\\nto linear by using an attention mechanism based on sparse Gaussian process\\nliterature [174]. Routing Transformer aims to reduce the overall complexity\\nof attention by learning dynamic sparse attention patterns by using routing\\nattention with clustering [175]. It applies k-means clustering to model sparse\\nattention matrices. At ﬁrst, queries and keys are assigned to clusters. The at-\\ntention scheme is determined by considering only queries and keys from the\\nsame cluster. Thus, queries are routed to keys belonging to the same cluster\\n[175].\\nSparse Transformer introduces sparse factorizations of the attention ma-\\ntrix by using factorized self-attention, and avoids the quadratic growth of com-\\nputational burden [176]. It also shows the possibility of modeling sequences\\nof length one million or more by using self-attention in theory. In the Trans-\\nformer, all the attention heads with the softmax attention assign a non-zero\\nweight to all context words. Adaptively Sparse Transformer replaces softmax\\nwith α-entmax which is a diﬀerentiable generalization of softmax allowing\\nlow-scoring words to receive precisely zero weight [177]. By means of context-\\ndependent sparsity patterns, the attention heads become ﬂexible in the Adap-\\ntively Sparse Transformer. Random feature attention approximates softmax\\nattention with random feature methods [178]. Skyformer replaces softmax\\nwith a Gaussian kernel and adapts Nystr¨ om method [179]. A sparse atten-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 16, 'page_label': '17', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Attention Mechanism in Neural Networks: 17\\ntion mechanism named BIGBIRD aims to reduce the quadratic dependency\\nof Transformer-based models to linear [180]. Diﬀerent from the similar stud-\\nies, BIGBIRD performs well for genomics data alongside NLP tasks such as\\nquestion answering.\\nMusic Transformer [181] shows that self-attention can also be useful for\\nmodeling music. This study emphasizes the infeasibility of the relative po-\\nsition representations introduced by [103] for long sequences because of the\\nquadratic intermediate relative information in the sequence length. Therefore,\\nthis study presents an extended version of relative attention named relative\\nlocal attention that improves the relative attention for longer musical com-\\npositions by reducing its intermediate memory requirement to linear in the\\nsequence length. A softmax-free Transformer (SOFT ) is presented to improve\\nthe computational eﬃciency of ViT. It uses Gaussian kernel function instead\\nof the dot-product similarity [182].\\nAdditionally, various approaches have been presented in Hierarchical Vi-\\nsual Transformer [183], Long-Short Transformer (Transformer-LS)[184], Per-\\nceiver [185], and Performer [186]. Image Transformer based on the cross-\\ncovariance matrix between keys and queries is applied [187], and a new vi-\\nsion Transformer is proposed [188]. Furthermore, a Bernoulli sampling atten-\\ntion mechanism decreases the quadratic complexity to linear [189]. A novel\\nlinearized attention mechanism performs well on object detection, instance\\nsegmentation, and stereo depth estimation [190]. A study shows that kernel-\\nized attention with relative positional encoding can be calculated using Fast\\nFourier Transform and it leads to get rid of the quadratic complexity for long\\nsequences [191]. A linear uniﬁed nested attention mechanism namely Luna\\nuses two nested attention functions to approximate the softmax attention in\\nTransformer to achieve linear time and space complexity [192].\\n6 Concluding Remarks: A New Hope\\nInspired by the human visual system, the attention mechanisms in neural net-\\nworks have been developing for a long time. In this study, we examine this\\nduration beginning with its roots up to the present time. Some mechanisms\\nhave been modiﬁed, or novel mechanisms have emerged in this period. Today,\\nthis journey has reached a very important stage. The idea of incorporating\\nattention mechanisms into deep neural networks has led to state-of-the-art re-\\nsults for a large variety of tasks. Self-attention mechanisms and GPT-n family\\nmodels have become a new hope for more advanced models. These promising\\nprogress bring the questions whether the attention could help further devel-\\nopment, replace the popular neural network layers, or could be a better idea\\nthan the existing attention mechanisms? It is still an active research area and\\nmuch to learn we still have, but it is obvious that more powerful systems are\\nawaiting when neural networks and attention mechanisms join forces.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 17, 'page_label': '18', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='18 Derya Soydaner\\nConﬂict of interest\\nThe author declares that she has no conﬂict of interest.\\nReferences\\n1. I. Goodfellow, Y. Bengio, A. Courville, The MIT Press (2016)\\n2. D. Noton, L. Stark, Scientiﬁc American 224(6), 34 (1971)\\n3. D. Noton, L. Stark, Vision Research 11, 929 (1971)\\n4. E. Alpaydın, Advances in Neural Information Processing Systems 8 pp. 771–777 (1995)\\n5. S. Ahmad, Advances in Neural Information Processing Systems 4 pp. 420–427 (1991)\\n6. M. Posner, S. Petersen, Annual Review of Neuroscience 13(1), 25 (1990)\\n7. C. Bundesen, Psychological Review 97(4), 523 (1990)\\n8. R. Desimone, J. Duncan, Annual Review of Neuroscience 18(1), 193 (1995)\\n9. M. Corbetta, G. Shulman, Nature Reviews Neuroscience 3(3), 201 (2002)\\n10. S. Petersen, M. Posner, Annual Review of Neuroscience 35, 73 (2012)\\n11. R. Rimey, C. Brown, Technical Report, University of Rochester (1990)\\n12. B. Sheliga, L. Riggio, G. Rizzolatti, Experimental Brain Research 98(3), 507 (1994)\\n13. B. Sheliga, L. Riggio, G. Rizzolatti, Experimental Brain Research 105(2), 261 (1995)\\n14. J. Hoﬀman, B. Subramaniam, Perception and Psychophysics 57(6), 787 (1995)\\n15. S. Chaudhari, et al., ACM Transactions on Intelligent Systems and Technology (TIST)\\npp. 1–32 (2021)\\n16. A. Galassi, et al., IEEE Transactions on Neural Networks and Learning Systems (2020)\\n17. J. Lee, et al., ACM Transactions on Knowledge Discovery from Data (TKDD) 13(6),\\n1 (2019)\\n18. T. Brown, et al., Advances in Neural Information Processing Systems 33 pp. 1877–1901\\n(2020)\\n19. A. Vaswani, et al., Advances in Neural Information Processing Systems 30 pp. 5998–\\n6008 (2017)\\n20. K. Fukushima, Biological Cybernetics 36, 193 (1980)\\n21. K. Fukushima, Applied Optics 26(23), 4985 (1987)\\n22. K. Fukushima, T. Imagawa, Neural Networks 6(1), 33 (1993)\\n23. E. Postma, H.V. den Herik, P. Hudson, Neural Networks 10(6), 993 (1997)\\n24. J. Schmidhuber, R. Huber, International Journal of Neural Systems pp. 125–134 (1991)\\n25. R. Milanese, et al., IEEE Computer Society Conference on Computer Vision and Pat-\\ntern Recoginition, Seattle, WA, USA pp. 781–785 (1994)\\n26. J. Tsotsos, et al., Artiﬁcial Intelligence 78(1-2), 507 (1995)\\n27. S. Culhane, J. Tsotsos, Proceedings of the 11th IAPR International Conference on\\nPattern Recognition, The Hague, Netherlands pp. 36–40 (1992)\\n28. D. Reisfeld, H. Wolfson, Y. Yeshurun, International Journal of Computer Vision14(2),\\n119 (1995)\\n29. I. Rybak, et al., Vision Research 38(15-16), 2387 (1998)\\n30. J. Keller, et al., Pattern Analysis and Applications 2(3) (1999)\\n31. F. Miau, L. Itti, Proceedings of the 23rd Annual International Conference of the IEEE\\nEngineering in Medicine and Biology Society, Istanbul, Turkey pp. 789–792 (2001)\\n32. W. Zhang, et al., Advances in Neural Information Processing Systems 19 pp. 1609–1616\\n(2006)\\n33. A. Salah, E. Alpaydın, L. Akarun, IEEE Transactions on Pattern Analysis and Machine\\nIntelligence 24(3), 420 (2002)\\n34. D. Walther, et al., International Workshop on Biologically Motivated Computer Vision,\\nSpringer, Berlin, Heidelberg pp. 472–479 (2002)\\n35. K. Schill, et al., Journal of Electronic Imaging 10(1), 152 (2001)\\n36. L. Paletta, G. Fritz, C. Seifert, International Conference on Machine Learning (2005)\\n37. O.L. Meur, et al., IEEE Transactions on Pattern Analysis and Machine Intelligence\\n28(5), 802– (2006)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 18, 'page_label': '19', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Attention Mechanism in Neural Networks: 19\\n38. S. Gould, et al., International Joint Conference on Artiﬁcial Intelligence (IJCAI) pp.\\n2115–2121 (2007)\\n39. H. Larochelle, G. Hinton, Advances in Neural Information Processing Systems 23 pp.\\n1243–1251 (2010)\\n40. L. Bazzani, et al., International Conference on Machine Learning (2011)\\n41. V. Mnih, et al., Advances in Neural Information Processing Systems 27 pp. 2204–2212\\n(2014)\\n42. M. Stollenga, et al., Advances in Neural Information Processing Systems 27 pp. 3545–\\n3553 (2014)\\n43. Y. Tang, N. Srivastava, R. Salakhutdinov, Advances in Neural Information Processing\\nSystems 27 (2014)\\n44. D. Bahdanau, K. Cho, Y. Bengio, International Conference on Learning Representa-\\ntions (2015)\\n45. I. Sutskever, O. Vinyals, Q. Le, Advances in Neural Information Processing Systems\\n27 pp. 3104–3112 (2014)\\n46. K. Cho, et al., Proceedings of the 2014 Conference on Empirical Methods in Natural\\nLanguage Processing (EMNLP) pp. 1724–1734 (2014)\\n47. M. Schuster, K. Paliwal, IEEE Transactions on Signal Processing 45(11), 2673 (1997)\\n48. K. Xu, et al., International Conference on Machine Learning pp. 2048–2057 (2015)\\n49. O. Vinyals, et al., In Proceedings of the IEEE Conference on Computer Vision and\\nPattern Recognition pp. 3156–3164 (2015)\\n50. R. Williams, Machine Learning 8(3-4), 229 (1992)\\n51. M.T. Luong, H.P..C. Manning, Proceedings of the 2015 Conference on Empirical Meth-\\nods in Natural Language Processing, Lisbon, Portugal pp. 1412–1421 (2015)\\n52. J. Lu, et al., Advances in Neural Information Processing Systems 29 (2016)\\n53. J. Weston, S. Chopra, A. Bordes, International Conference on Learning Representa-\\ntions (2014)\\n54. A. Graves, G. Wayne, I. Danihelka, arXiv preprint arXiv:1410.5401 (2014)\\n55. S. Sukhbaatar, et al., Advances in Neural Information Processing Systems 28 pp. 2440–\\n2448 (2015)\\n56. J. Cheng, L. Dong, M. Lapata, Proceedings of the 2016 Conference on Empirical\\nMethods in Natural Language Processing pp. 551–561 (2016)\\n57. A. Parikh, et al., Proceedings of the 2016 Conference on Empirical Methods in Natural\\nLanguage Processing, Austin, Texas pp. 2249–2255 (2016)\\n58. Q. You, et al., In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition (CVPR), Las Vegas, NV pp. 4651–4659 (2016)\\n59. A. Rush, S. Chopra, J. Weston, Proceedings of the 2015 Conference on Empirical\\nMethods in Natural Language Processing, Lisbon, Portugal pp. 379–389 (2015)\\n60. D. Yu, et al., Interspeech pp. 17–21 (2016)\\n61. J. Chorowski, et al., Advances in Neural Information Processing Systems 28 pp. 577–\\n585 (2015)\\n62. M. Zanﬁr, E. Marinoiu, C. Sminchisescu, In Asian Conference on Computer Vision,\\nSpringer, Cham pp. 104—-119 (2016)\\n63. Y. Cheng, et al., Proceedings of the 25th International Joint Conference on Artiﬁcial\\nIntelligence (2016)\\n64. T. Rockt International Conference on Learning Representations (2016)\\n65. Y. Zhu, et al., Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition pp. 4995–5004 (2016)\\n66. K. Chen, et al., arXiv preprint arXiv:1511.05960 (2015)\\n67. H. Xu, K. Saenko, In European Conference on Computer Vision pp. 451–466 (2016)\\n68. W. Yin, et al., Transactions of the Association for Computational Linguistics 4, 259\\n(2016)\\n69. S. Sharma, R. Kiros, R. Salakhutdinov, International Conference on Learning Repre-\\nsentations (2016)\\n70. Z. Yang, et al., In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition pp. 21–29 (2016)\\n71. I. Sorokin, et al., arXiv preprint arXiv:1512.01693 (2015)\\n72. J. Ba, et al., Advances in Neural Information Processing Systems 28 pp. 2593–2601\\n(2015)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 19, 'page_label': '20', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='20 Derya Soydaner\\n73. K. Gregor, et al., International Conference on Machine Learning pp. 1462–1471 (2015)\\n74. E. Mansimov, et al., International Conference on Learning Representations (2016)\\n75. S. Reed, et al., Advances in Neural Information Processing Systems 29 pp. 217–225\\n(2016)\\n76. E. Voita, et al., In Proceedings of the 57th Annual Meeting of the Association for\\nComputational Linguistics, Florence, Italy pp. 5797–5808 (2019)\\n77. G. Kerg, et al., Advances in Neural Information Processing Systems 33 (2020)\\n78. J.B. Cordonnier, A. Loukas, M. Jaggi, International Conference on Learning Repre-\\nsentations (2020)\\n79. Z. Lin, et al., International Conference on Learning Representations (2017)\\n80. R. Paulus, C. Xiong, R. Socher, International Conference on Learning Representations\\n(2018)\\n81. N. Kitaev, D. Klein, In Proceedings of the 56th Annual Meeting of the Association for\\nComputational Linguistics (Long papers) pp. 2676–2686 (2018)\\n82. D. Povey, et al., IEEE International Conference on Acoustics, Speech and Signal Pro-\\ncessing (ICASSP), IEEE pp. 5874–5878 (2018)\\n83. A. Vyas, et al., Advances in Neural Information Processing Systems 33 (2020)\\n84. W. Chan, et al., IEEE International Conference on Acoustics, Speech and Signal Pro-\\ncessing (ICASSP), Shanghai pp. 4960—-4964 (2016)\\n85. M. Sperber, et al., In proceedings of Annual Conference of the International Speech\\nCommunication Association (InterSpeech) pp. 3723–3727 (2018)\\n86. L. Kaiser, et al., arXiv preprint arXiv:1706.05137 (2017)\\n87. C. Xu, et al., Proceedings of the 56th Annual Meeting of the Association for Compu-\\ntational Linguistics (Short papers), Melbourne, Australia pp. 778–783 (2018)\\n88. S. Maruf, A. Martins, G. Haﬀari, Proceedings of NAACL-HLT, Minneapolis, Minnesota\\npp. 3092–3102 (2019)\\n89. P. Ramachandran, et al., Advances in Neural Information Processing Systems 32 pp.\\n68–80 (2019)\\n90. Y. Li, et al., International Conference on Machine Learning (2019)\\n91. I. Goodfellow, et al., Advances in Neural Information Processing Systems 27 pp. 2672–\\n2680 (2014)\\n92. H. Zhang, et al., International Conference on Machine Learning pp. 7354–7363 (2019)\\n93. T. Xu, et al., In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition (CVPR) pp. 1316–1324 (2018)\\n94. A. Yu, et al., International Conference on Learning Representations (2018)\\n95. J. Zhang, et al., Conference on Uncertainty in Artiﬁcial Intelligence (2018)\\n96. D. Romero, et al., International Conference on Machine Learning (2020)\\n97. R. Al-Rfou, et al., AAAI Conference on Artiﬁcial Intelligence 33, 3159 (2019)\\n98. J. Du, et al., Proceedings of the 2018 Conference on Empirical Methods in Natural\\nLanguage Processing pp. 2216–2225 (2018)\\n99. X. Li, et al., Advances in Neural Information Processing Systems 33 (2020)\\n100. B. Yang, et al., AAAI Conference on Artiﬁcial Intelligence 33, 387 (2019)\\n101. B. Yang, et al., Proceedings of the 2018 Conference on Empirical Methods in Natural\\nLanguage Processing, Brussels, Belgium pp. 4449–4458 (2018)\\n102. Proceedings of the IEEE International Conference on Computer Vision pp. 3286–3295\\n103. P. Shaw, J. Uszkoreit, A. Vaswani, Proceedings of NAACL-HLT, New Orleans,\\nLouisiana pp. 464–468 (2018)\\n104. T. Shen, et al., AAAI Conference on Artiﬁcial Intelligence pp. 5446–5455 (2018)\\n105. T. Shen, et al., In Proceedings of the 27th International Joint Conference on Artiﬁcial\\nIntelligence, (IJCAI-18) pp. 4345–4352 (2018)\\n106. H. Le, T. Tran, S. Venkatesh, International Conference on Machine Learning (2020)\\n107. T. Shen, et al., International Conference on Learning Representations (2018)\\n108. S. Bhojanapalli, et al., International Conference on Machine Learning (2020)\\n109. Y. Tay, et al., International Conference on Machine Learning (2020)\\n110. S. Sukhbaatar, et al., Proceedings of the 57th Annual Meeting of the Association for\\nComputational Linguistics, Florence, Italy pp. 331–335 (2019)\\n111. Y. Jernite, et al., International Conference on Learning Representations (2017)\\n112. R. Shu, H. Nakayama, In Proceedings of the First Workshop on Neural Machine Trans-\\nlation, Vancouver, Canada pp. 1–10 (2017)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 20, 'page_label': '21', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Attention Mechanism in Neural Networks: 21\\n113. J. Hao, et al., Proceedings of NAACL-HLT, Minneapolis, Minnesota pp. 1198–1207\\n(2019)\\n114. X. Huang, et al., International Conference on Machine Learning (2020)\\n115. V. Shiv, C. Quirk, Advances in Neural Information Processing Systems 32 pp. 12,081–\\n12,091 (2019)\\n116. Z. Li, et al., International Conference on Machine Learning (2020)\\n117. Y. Hoshen, Advances in Neural Information Processing Systems 30, Long Beach, CA,\\nUSA (2017)\\n118. S. Hu, et al., International Conference on Learning Representations (2021)\\n119. E. Parisotto, R. Salakhutdinov, International Conference on Learning Representations\\n(2021)\\n120. S. Wu, et al., Advances in Neural Information Processing Systems 33 (2020)\\n121. A. Bosselut, et al., Proceedings of the 57th Annual Meeting of the Association for\\nComputational Linguistics (2019)\\n122. D. So, C. Liang, Q. Le, International Conference on Machine Learning (2019)\\n123. K. Choi, et al., International Conference on Machine Learning (2020)\\n124. C. Doersch, A. Gupta, A. Zisserman, Advances in Neural Information Processing Sys-\\ntems 33 pp. 21,981–21,993 (2020)\\n125. N. Carion, et al., European Conference on Computer Vision pp. 213—-229 (2020)\\n126. X. Zhu, et al., International Conference on Learning Representations (2021)\\n127. X. Liu, et al., International Conference on Machine Learning pp. 6327–6335 (2020)\\n128. J. Kasai, et al., International Conference on Machine Learning (2020)\\n129. D. Hudson, L. Zitnick, International Conference on Machine Learning pp. 4487–4499\\n(2021)\\n130. A. Radford, et al., Technical Report, OpenAI (2018)\\n131. A. Radford, et al., OpenAI blog p. 9 (2019)\\n132. M. Dehghani, et al., International Conference on Learning Representations (2019)\\n133. N. Parmar, International Conference on Machine Learning (2018)\\n134. Z. Dai, et al., Proceedings of the 57th Annual Meeting of the Association for Compu-\\ntational Linguistics pp. 2978–2988 (2019)\\n135. E. Parisotto, International Conference on Machine Learning (2020)\\n136. X. Ma, et al., Advances in Neural Information Processing Systems 32 pp. 2232–2242\\n(2019)\\n137. L. Lathauwer, SIAM Journal on Matrix Analysis and Applications 30(3), 1033 (2008)\\n138. L. Tucker, Psychometrika 31(3), 279 (1966)\\n139. J. Devlin, et al., Proceedings of NAACL-HLT 2019 pp. 4171–4186 (2019)\\n140. W. Taylor, Journalism Bulletin 30(4), 415 (1953)\\n141. K. Clark, et al., arXiv preprint arXiv:1906.04341 (2019)\\n142. S. Sun, et al., Proceedings of the 2019 Conference on Empirical Methods in Natural\\nLanguage Processing and the 9th International Joint Conference on Natural Language\\nProcessing, Hong Kong, China pp. 4323–4332 (2019)\\n143. W. Wang, et al., Advances in Neural Information Processing Systems 33 (2020)\\n144. J. McCarley, R. Chakravarti, A. Sil, arXiv preprint arXiv:1910.06360 (2020)\\n145. O. Zafrir, et al., The 5th Workshop on Energy Eﬃcient Machine Learning and Cogni-\\ntive Computing - NeurIPS (2019)\\n146. M. Joshi, et al., In Proceedings of the 2019 Conference on Empirical Methods in\\nNatural Language Processing and the 9th International Joint Conference on Natural\\nLanguage Processing pp. 5803–5808 (2019)\\n147. L. Gong, et al., International Conference on Machine Learning pp. 2337–2346 (2019)\\n148. Z. Lan, et al., International Conference on Learning Representations (2020)\\n149. S. Goyal, et al., International Conference on Machine Learning (2020)\\n150. X. Jiao, et al., arXiv preprint arXiv:1909.10351 (2019)\\n151. M. Joshi, et al., Transactions of the Association for Computational Linguistics 8, 64\\n(2020)\\n152. Y. Liu, et al., arXiv preprint arXiv:1907.11692 (2019)\\n153. P. He, et al., International Conference on Learning Representations (2021)\\n154. V. Sanh, et al., the 5th Workshop on Energy Eﬃcient Machine Learning and Cognitive\\nComputing - NeurIPS (2019)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 21, 'page_label': '22', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='22 Derya Soydaner\\n155. W. Wang, et al., International Conference on Learning Representations (2020)\\n156. S. Shen, et al., AAAI Conference on Artiﬁcial Intelligence 34, 8815 (2020)\\n157. J. Lee, et al., Bioinformatics 36(4), 1234 (2020)\\n158. P. Prakash, et al., AAAI Conference on Artiﬁcial Intelligence 35, 453 (2021)\\n159. Z. Wu, et al., International Conference on Learning Representations (2020)\\n160. S. Mehta, et al., International Conference on Learning Representations (2021)\\n161. Y. Tay, et al., International Conference on Learning Representations (2021)\\n162. S. Yun, et al., International Conference on Learning Representations (2018)\\n163. Y. Rong, et al., Advances in Neural Information Processing Systems 33 (2020)\\n164. J. Yang, et al., Advances in Neural Information Processing Systems 34 (2021)\\n165. C. Zhao, et al., International Conference on Learning Representations (2020)\\n166. R. You, et al., Advances in Neural Information Processing Systems 32 (2019)\\n167. X. Fan, et al., Advances in Neural Information Processing Systems 33 (2020)\\n168. G. Brunner, et al., International Conference on Learning Representations (2020)\\n169. A. Dosovitskiy, et al., International Conference on Learning Representations (2021)\\n170. A. Katharopoulos, et al., International Conference on Machine Learning (2020)\\n171. L. Greengard, V. Rokhlin, Journal of Computational Physics 73(2), 325– (1987)\\n172. T. Nguyen, et al., Advances in Neural Information Processing Systems 34 (2021)\\n173. N. Kitaev, L. Kaiser, A. Levskaya, International Conference on Learning Representa-\\ntions (2020)\\n174. J. Lee, et al., International Conference on Machine Learning pp. 3744–3753 (2019)\\n175. A. Roy, et al., Transactions of the Association for Computational Linguistics pp. 53–68\\n(2020)\\n176. R. Child, et al., arXiv preprint arXiv:1904.10509 (2019)\\n177. G. Correia, V. Niculae, A. Martins, Proceedings of the 2019 Conference on Empirical\\nMethods in Natural Language Processing and the 9th International Joint Conference\\non Natural Language Processing pp. 2174–2184 (2019)\\n178. H. Peng, et al., International Conference on Learning Representations (2021)\\n179. Y. Chen, et al., Advances in Neural Information Processing Systems 34 (2021)\\n180. M. Zaheer, et al., Advances in Neural Information Processing Systems 33 (2020)\\n181. C.Z. Huang, et al., International Conference on Learning Representations (2019)\\n182. J. Lu, et al., Advances in Neural Information Processing Systems 34 (2021)\\n183. Z. Pan, et al., Proceedings of the IEEE/CVF International Conference on Computer\\nVision pp. 377–386 (2021)\\n184. C. Zhu, et al., Advances in Neural Information Processing Systems 34 (2021)\\n185. A. Jaegle, et al., International Conference on Machine Learning pp. 4651–4664 (2021)\\n186. K. Choromanski, et al., International Conference on Learning Representations (2021)\\n187. A. El-Nouby, et al., Advances in Neural Information Processing Systems 34 (2021)\\n188. Q. Yu, et al., Advances in Neural Information Processing Systems 34 (2021)\\n189. Z. Zeng, et al., International Conference on Machine Learning pp. 12,321–12,332 (2021)\\n190. Z. Shen, et al., Proceedings of the IEEE/CVF Winter Conference on Applications of\\nComputer Vision pp. 3531–3539 (2021)\\n191. S. Luo, et al., Advances in Neural Information Processing Systems 34 (2021)\\n192. X. Ma, et al., Advances in Neural Information Processing Systems 34 (2021)'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 0, 'page_label': '1', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nQZhou-Embedding Technical Report\\nPeng Yu, En Xu, Bin Chen, Haibiao Chen, Yinfei Xu\\nKingsoft AI ∗\\nAugust 2025\\nAbstract\\nWe present QZhou-Embedding, a general-purpose contextual text embed-\\nding model with exceptional text representation capabilit ies. Built upon the\\nQwen2.5-7B-Instruct foundation model, we designed a uniﬁe d multi-task frame-\\nwork comprising specialized data transformation and train ing strategies. The\\ndata transformation scheme enables the incorporation of mo re diverse textual\\ntraining datasets, while the task-speciﬁc training strate gies enhance model learn-\\ning eﬃciency. We developed a data synthesis pipeline levera ging LLM API, in-\\ncorporating techniques such as Paraphrasing, Augmentatio n, and Hard negative\\nexample generation to improve the semantic richness and sam ple diﬃculty of\\nthe training set. Additionally, we employ a two-stage train ing strategy, compris-\\ning initial retrieval-focused pretraining followed by ful l-task ﬁne-tuning, enabling\\nthe embedding model to extend its capabilities based on robu st retrieval perfor-\\nmance. Our model achieves state-of-the-art results on the M TEB and CMTEB\\nbenchmarks, ranking ﬁrst on both leaderboards(August 27, 2 025), simultaneously\\nachieves state-of-the-art performance on tasks including Reranking, Clustering,\\netc. Our ﬁndings demonstrate that higher-quality, more div erse data is crucial for\\nadvancing retrieval model performance, and that leveragin g LLMs’ generative ca-\\npabilities can further optimize data quality for embedding model breakthroughs.\\nOur model weights are released on HuggingFace 1 under Apache 2.0 license. For\\nreproducibility, we provide evaluation code and instructi ons on GitHub 2.\\n1 Introduction\\nText embedding models, which transform natural language text int o mathematical vec-\\ntor representations, play an indispensable role in text mining, quest ion-answering sys-\\ntems, recommendation systems, and retrieval-augmented gener ation. Recently, LLM-\\nbased agent technology has experienced rapid development and wid espread adoption,\\nembedding models, which transform textual or multimodal data into vector represen-\\ntations for knowledge base construction, have signiﬁcantly enhan ced agent systems\\n∗ https://kingsoft.com/\\n1https://huggingface.co/Kingsoft-LLM/QZhou-Embedding\\n2https://github.com/Kingsoft-LLM/QZhou-Embedding\\narXiv:2508.21632v1  [cs.CL]  29 Aug 2025'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 1, 'page_label': '2', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nin terms of real-time performance, long-term memory, data privac y preservation, and\\nknowledge integration capabilities. With the continuous advancemen t of neural net-\\nworks and deep learning, text embeddings have evolved from early s parse representa-\\ntions (e.g., BM25[ 1]) to dense representations based on ﬁne-tuned deep networks s uch\\nas BERT[2] and T5[ 3], leading to signiﬁcant performance improvements[ 4][5][6][7][8]. In\\n2022, the rise of large language models (LLMs), exempliﬁed by ChatG PT[9], ushered in\\na new era of text embeddings based on LLM representations, includ ing models like text-\\nembedding-3-large and RepLLaMA[ 10]. Recent research on optimizing text embedding\\nmodels has explored diverse perspectives and focal points. For ins tance, to address\\nthe limitation of decoder-only architectures—where causal atten tion mechanisms re-\\nstrict token embeddings to unidirectional semantic capture—seve ral approaches have\\nbeen proposed: Echo Embedding[ 11] employs input repetition and instruction design\\nto enable preceding tokens to capture subsequent token semant ics. LLM2Vec[ 12] modi-\\nﬁes attention to bi-directional mechanism to remove backward dep endency constraints.\\nConan-Embedding-v2[13] proposes a novel soft masking mechanism combined with dy-\\nnamic rank reduction. Another widely adopted approach is knowledg e distillation,\\nwhere text embeddings are treated as the ”signal states” repre senting textual seman-\\ntics. By distilling knowledge from high-performing teacher models to s tudent models,\\nthe objective is to optimize the embedding performance. For instan ce, Jasper[ 14] em-\\nploys a multi-stage knowledge distillation framework, combining with mu ltiple carefully\\ndesigned loss functions and ﬁnally achieving superior results. Debat er[16] proposes a\\nstep-by-step thinking mechanism for embedding generation, itera tively optimizing doc-\\nument representations through continuous COT. Distillation is applie d to constrain\\nthe ﬁnal token representation to learn the optimal semantic stat es from these thinking\\nsteps. Additionally, hard negative sampling has emerged as a crucial research direc-\\ntion in text embedding models, serving as a pivotal technique for mod el optimization.\\nANCE[18] identiﬁed that conventional dense retrieval training leads to dimin ishing gra-\\ndient norms during optimization. Thus they developed an asynchron ous Approximate\\nNearest Neighbor (ANN) indexing mechanism that periodically refres hes the negative\\nsample pool using the current model parameters, thereby ensur ing the maintenance\\nof up-to-date and optimally challenging negative samples. Both Cona n-Embedding[24]\\nand its v2 version incorporated similar dynamic hard negative sampling techniques to\\nenhance model performance. NV-Embed[ 19] implemented an alternative approach by\\nleveraging their previously developed NV-Retriever’s[ 20] positive-aware negative min-\\ning strategy, including TopK-MarginPos and TopKPercPos ﬁltering m echanisms.\\nIn this work, we present QZhou-Embedding, built upon the powerfu l Qwen2.5-7B-\\nInstruct[21] model, which pushes the boundaries of text embedding capabilities. To\\nenhance the model’s semantic understanding, we designed a uniﬁed m ulti-task learn-\\ning framework that not only accommodates more diverse training da ta but also bring\\neﬃcient learning across three key tasks: retrieval, natural langu age inference (NLI),\\nand classiﬁcation. Our framework comprises two core components : 1. Data Trans-\\nformation: We carefully adapt data formats to the speciﬁc require ments of retrieval,\\nNLI, and classiﬁcation tasks, enabling eﬀective feature extractio n from heterogeneous\\ndata sources, signiﬁcantly beneﬁting retrieval model training. 2. Training Strategy:\\nWe designed specialized loss functions based on each task’s charact eristics, optimizing\\nmodel training eﬃciency. To further improve the robustness and g eneralization of vec-\\n2'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 2, 'page_label': '3', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\ntor representation, we propose a data synthesis method by emplo ying three techniques\\nto address data scarcity: Paraphrasing & Data augmentation for limited datasets and\\nHard negative generation for negative sample enrichment. Building u pon prior work, we\\ndesigned a strategy named ”Data Grouping Strategy”, enabling ba tch sampling within\\nsingle datasets, inadvertently increasing training diﬃculty through in-batch negative\\nsampling from the same distribution. For model training, we used a tw o-phase train-\\ning approach, through the ﬁrst-stage retrieval training and sec ond-stage full-capability\\ntraining, our model acquires a solid foundation of retrieval capabilit ies, while eﬀectively\\nextending to multiple capability dimensions. Our model achieved state -of-the-art av-\\nerage scores on CMTEB[ 22] and MTEB[ 23] benchmarks, ranking ﬁrst overall on both\\nCMTEB and MTEB leaderboards, demonstrating the eﬀectiveness o f our approach.\\nThe contributions of our work are summarized as follows:\\n• We propose a uniﬁed multi-task learning framework that systematic ally coordi-\\nnates both data processing and training pipelines, enhancing divers ity in datasets\\nand eﬃciency in model training ;\\n• We develop advanced data synthesis techniques powered by LLM, in cluding Para-\\nphrasing, Data augmentation, and Hard negative generation. The se methods\\nsigniﬁcantly enhance the quality of training corpora, thereby impro ving model’s\\nrobustness and generalization capabilities;\\n• We emply a two-stage training paradigm: Stage 1 focuses exclusively on retrieval\\ncapability building, establishing strong foundational retrieval perf ormance; and\\nstage 2 implements balanced training with controled retrieval/non-r etrieval task\\nratios, achieving superior performance on classiﬁcation (CLS), pa ir classiﬁcation\\n(PairCLS), and semantic textual similarity (STS) tasks while maintain ing re-\\ntrieval eﬀectiveness;\\n• Our model achieves state-of-the-art performance on both MTE B and CMTEB\\nbenchmarks, which validates the eﬀectiveness of our proposed me thods.\\n2 Related Works\\n2.1 Text Embedding Models\\nText vector representation is a fundamental research area in na tural language processing\\n(NLP) and serves as the cornerstone for language understandin g. Early approaches re-\\nlied on sparse vector representations, such as TF-IDF[\\n25], BM25[26], and LSA[ 27]. With\\nthe advent of pretrained language models, dense contextualized r epresentations based\\non architectures like BERT[ 2] and T5[ 3] became widely studied and applied[ 4][5][6]. In\\nthe era of large language models (LLMs), major advancements hav e led to the devel-\\nopment of LLM-based embedding models, such as text-embedding- 3-small/large (Ope-\\nnAI), E5-Mistral-7B[28], SFR-Embedding-Mistral[29], SFR-Embedding-2R[ 30], GRITLM[31],\\nLLM2Vec[12], RepLLaMA[10], BGE-en-icl[32], NV-Embed[19], gte-Qwen2-7B-Instruct[33],\\nQwen3-Embedding[34], etc. These models beneﬁt from optimized LLM architectures—suc h\\nas RoPE positional encoding[ 35], RMSNorm[ 36], and GeGLU activation[ 37]—combined\\nwith their strong semantic contextualization capabilities acquired th rough large-scale\\n3'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 3, 'page_label': '4', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\npretraining. As a result, LLM-based embeddings achieve superior p erformance in re-\\ntrieval and related tasks.\\n2.2 Embedding Model Training\\nThe mainstream approaches currently involve contrastive learning pretraining on un-\\nsupervised/weakly supervised corpora and supervised contrast ive learning training on\\nhigh-quality labeled positive and negative samples. In unsupervised le arning, early\\nwork like SimCSE[\\n7] proposed feeding continuous inputs of both original and noise-\\naugmented texts while employing contrastive learning to enhance th e model’s dis-\\ncriminative representation capability. For weakly supervised learnin g, gte[ 33] utilized\\nlarge-scale structured data (web search data, title-article pairs , etc.) for pretraining,\\nfollowed by ﬁne-tuning on high-quality open-source retrieval train ing data, achieving\\nperformance comparable to OpenAI embeddings with signiﬁcantly fe wer parameters.\\nConan-Embedding[24] and v2 similarly adopted the weakly supervised pretraining &\\nsupervised ﬁne-tuning approach but incorporated techniques like cross-GPU batch loss\\nbalancing, dynamic hard negative mining, and soft masking (v2) to op timize the model.\\nSeed1.6-Embedding[38] employed a phased training strategy combining text and multi-\\nmodal pretraining followed by business-scenario-speciﬁc ﬁne-tun ing, achieving superior\\nrepresentation quality.\\nSubstantial research has also been conducted on modeling diﬀeren t tasks. Piccolo2[\\n39]\\nintroduced multi-task hybrid loss functions for diverse downstrea m tasks, an approach\\nwe also incorporate. SFR-Embedding[ 30] utilized multi-task learning techniques to\\nregularize embeddings, signiﬁcantly enhancing domain data discrimina tion. Xiaobu-\\nembedding uniﬁed the treatment of major CMTEB problem categorie s from the per-\\nspective of circle loss[ 40], fully leveraging multiple positive examples in original datasets\\nwhile carefully balancing diﬀerent loss weights.\\n2.3 Data Synthesis\\nData quantity and quality are the most critical factors in model opt imization, data\\nsynthesis methods have become a critical research direction due t o the high cost of\\nmanual annotation. Doc2Query[\\n41] and Query2Doc[ 42] employ question-answering\\nmodels to generate pseudo-queries and pseudo-documents resp ectively, enhancing data\\nfor improved RAG performance. Promptagator[ 43] addresses few-shot retrieval sce-\\nnarios by generating queries of diverse intents using few-shot dem onstrations and an-\\nnotations, eﬀectively improving retrieval capabilities across varyin g intents or distri-\\nbutions. GPL[ 44] utilizes existing T5 encoder-decoder models to generate queries,\\nretrieves similar passages as hard negatives using existing retrieva l models, and em-\\nploys cross-encoders to score each (query, passage) pair. Unn atural Instructions[ 45]\\nleverages prompt and in-context learning (ICL) techniques to gen erate synthetic ex-\\namples through controlled instructions, inputs, and constraints, producing 64k diverse\\ndata entries from several seed examples with promising experiment al results. Qwen3-\\nEmbedding[34] designs a diversiﬁed prompting strategy by assigning document-s peciﬁc\\nroles to simulate potential users querying that document, enabling LLMs to generate\\nstylistically authentic queries that enhance diversity and realism.\\n4'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 4, 'page_label': '5', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\n2.4 Hard Negative Mining Techniques\\nHard negatives serve as essential components in contrastive lear ning for retrieval model\\ntraining. Early work like ANCE[\\n46] proposed an asynchronous ANN indexing mech-\\nanism that periodically updates hard negatives using checkpoint sta tes to maintain\\noptimally challenging samples. Conan-Embedding[ 24] and its v2 version implemented\\na dynamic hard negative sampling strategy by excluding and refresh ing samples when\\ntheir scores fall below a threshold. NV-Retriever[ 47] proposed positive-aware negative\\nmining, introducing TopK-MarginPos and TopKPercPos ﬁltering crite ria to minimize\\nfalse negatives. LGAI-Embedding[ 17] built upon NV-Retriever’s strategy with adap-\\ntive margin-based mining strategies, employing ANNA IR as a teacher retrieval model\\nto identify high-quality hard negatives while using TopKPercPos ﬁlter ing to eliminate\\nfalse negatives.\\n3 Uniﬁed Multi-task Learning Framework\\nEmbedding models support numerous downstream tasks including re trieval, reranking,\\nSTS, and classiﬁcation. Given the diversity of these tasks and their associated data\\ncomplexity, we explore a uniﬁed strategy to eﬀectively handle them c ollectively while\\npromoting optimization of the embedding model. Existing research on uniﬁed task pro-\\ncessing includes circle loss[\\n40], which approaches sentence pair similarity from a global\\nperspective by categorizing tasks into class-level labels and pair-w ise labels, Xiaobu-\\nembedding demonstrated signiﬁcant improvements by adopting this approach. Other\\nmodels like Piccolo2[ 39], SFR-Embedding[ 30], NV-Embed[ 47], Conan-Embedding[ 24] ,\\nand Conan-Embedding-v2 have incorporated multi-task learning us ing diverse train-\\ning data with varying label processing methods, some employing task -speciﬁc losses\\n(InfoNCE[48], Cosent[ 49], etc.).\\nOur design principle aims to accommodate more tasks and data types , enabling cross-\\ndomain and cross-task data to eﬀectively enhance embedding capa bilities. We propose\\na uniﬁed multi-task learning framework that categorizes training da ta into three task\\ntypes: retrieval, NLI, and classiﬁcation, with customized data and training solutions\\nfor each, allowing most natural text data to be converted into emb edding training data\\nthrough this framework. The following sections detail the framewo rk’s components and\\nimplementation methods.\\n3.1 Model Architecture\\nEmbedding models based on BERT or T5 [\\n39][15][50][24] exhibit powerful contextual\\nrepresentation capabilities, primarily attributed to their bidirection al attention mech-\\nanisms. However, recent large language models predominantly adop t decoder-only ar-\\nchitectures with unidirectional attention, signiﬁcantly constrainin g tokens’ ability to\\ncapture contextual information. Several studies have address ed this limitation through\\narchitectural modiﬁcations or attention mechanism optimizations[ 12][31][47]. Our work\\nbuilds upon the Qwen2.5-7B-Instruct architecture and checkpoin t due to its exceptional\\nChinese language contextual capabilities. Consequently, we impleme nted the following\\nmodiﬁcations: (1) modifying the original causal attention to bi-dire ctional attention\\n5'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 5, 'page_label': '6', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nFigure 1: QZhou-Embedding Architecture\\nto enable comprehensive context capture, and (2) employing mean pooling with sub-\\nsequent normalization to produce ﬁnal embedding vectors. The mo del architecture is\\nshown in Figure 1\\n3.2 Data Transformation\\n3.2.1 Retrieval-oriented Process\\nWhile open-source datasets such as MS MARCO[\\n64] are readily accessible, they alone\\nare insuﬃcient for further advancing embedding model capabilities, thus we supplement\\nwith data from additional sources, such as news, academic paper a nd QA datasets.\\nGiven the heterogeneous nature of these datasets across doma ins and purposes, we\\ndesign a retrieval-oriented data transformation methodology to c onvert diverse sources\\nand formats into training data suitable for retrieval task. Below we outline selected\\ncategories of training data used for transformation and their pro cessing procedures:\\n• Title-Body/Abstract ”Title-Body/Abstract” type data primarily consists of\\ntitle-body/article pairs typically sourced from online news, articles, documents,\\narXiv publications and Wikipedia. For these data types, the transfo rmation pro-\\ncess involves using the title as the query and the body/abstract as the positive\\nsample. However, since the latter are documents, truncation is ap plied when they\\nexceed the maximum training length.\\n• Claim-Evidence This data type typically presents a claim or statement followed\\nby extracted evidence that either supports or refutes it, commo nly used for multi-\\nhop fact extraction and claim veriﬁcation tasks. Datasets genera lly contain claims\\nand corresponding evidence, with each evidence instance labeled as ”Supports”\\nor ”Refutes”. The transformation process involves: converting the claim portion\\n6'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 6, 'page_label': '7', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\ninto a query sample, for evidence labeled as ”Supports”, the text is treated as a\\npositive sample; for evidence labeled as ”Refutes”, it is converted in to a negative\\nsample.\\n• Question-Answer Question-answering data and conversational Q-A pairs pri-\\nmarily originate from chat platforms and forums. Within the current wave of\\nLLM and reinforcement learning research, such data exhibits rema rkable volume\\nand diversity. Virtually single-turn Q-A datasets(one question pair ed with one\\nanswer) represents the most suitable format for retrieval train ing. For transfor-\\nmation, the ”Question/Query/User” portion is converted into que ries, while the\\n”Answer/Response/Assistant” portion is processed as documen ts.\\n3.2.2 NLI-oriented Process\\nNatural Language Inference (NLI) represents a fundamental capability of NLP models,\\nencompassing tasks such as semantic similarity, textual entailment , and sentiment anal-\\nysis. This section describes the methodology for transforming and constructing training\\nsets from NLI-style data, using textual semantic similarity (STS) a nd textual entailment\\ntasks as illustrative examples. Our approach distinctively reformula tes NLI tasks into\\ntext\\npair-score formats compatible with Cosent loss[ 49] training strategy, where sample\\npairs are quantitatively scored based on their semantic relationship s. The processing\\nprocedures for each are detailed below:\\n• STS Semantic Textual Similarity (STS) is characterized by its symmetric s e-\\nmantic matching to determine whether two sentences share equiva lent meaning.\\nSTS datasets typically consist of sentence pairs with associated lab els, which may\\nbe binary classiﬁcations (yes/no, true/false) or numerical score s (e.g., 1.2, 3.1,\\n4.8). For binary labels, ”yes”/”true” are mapped to a numerical va lue of 1, while\\n”no”/”false” are converted to 0. The data is then structured int o (query, docu-\\nment, score) triplets. Due to the symmetric nature of STS, each s ingle original\\ndata sample can generate two training triplets by interchanging the query and\\npositive document roles.\\n• Textual Entailment Textual entailment further examines a model’s capabilities\\nin reasoning, typically featuring three-class labels: entailment, neu tral, contradic-\\ntion. Our processing method employs a three-tier scoring system: labels are\\nassigned values of 2, 1, and 0 for entailment, neutral, and contrad iction respec-\\ntively. We construct (query, document, score) triplets accordin gly, and similarly\\nleverage symmetry to double the dataset size.\\n3.2.3 CLS-oriented Process\\nClassiﬁcation tasks encompass text categorization and sentiment classiﬁcation scenar-\\nios, it typically follows a (text, label) format, where texts within the s ame category\\nexhibit semantic proximity while distinct boundaries separate diﬀeren t classes. NV-\\nEmbed[\\n47] compared label-based and example-based data construction met hods, with\\nexperimental results demonstrating the superiority of the latter . Adopting the example-\\nbased approach, we process classiﬁcation data (text, label) by us ing the text as query,\\n7'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 7, 'page_label': '8', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nFigure 2: CLS-oriented data transformation\\nsampling other texts sharing the same label as positive examples, an d selecting texts\\nfrom diﬀerent labels as negative examples. Figure 2 provides a detailed schematic\\nillustration of this process.\\n3.3 Training Strategy\\nEach task category—retrieval, NLI, and classiﬁcation—operates within a data construc-\\ntion process respectively, for which we have designed specialized tr aining objectives to\\nto enhance model training eﬃciency. This section elaborates on the design of loss\\nfunctions for retrieval, NLI, and classiﬁcation tasks.\\n3.3.1 Retrieval\\nFor the retrieval task, we adopt the widely used InfoNCE loss[\\n48], but incorporate an\\nimprovement inspired by gte[ 33] by augmenting the original query-negative loss with an\\nadditional query-query loss term. Speciﬁcally, each query within a b atch is treated as a\\nnegative sample for all other queries. The ﬁnal loss formulation is ex plicitly described\\nin Equation ( 1).\\nLRetrieval = − 1\\nn\\n∑\\ni\\nlog esim(qi,d +\\ni )/τ\\nesim(qi,d +\\ni )/τ + ∑\\nj esim(qi,d −\\nj )/τ + ∑\\nj̸=i esim(qi,q j )/τ\\n(1)\\n3.3.2 NLI\\nFor NLI tasks, the transformed labels are numerically comparable a nd exhibit ordinal\\nrelationships. We employ Cosent loss[\\n49] to optimize such data, which is designed\\nbased on the principles of Circle loss[ 40]. As a ranking-sensitive loss function, Cosent\\nloss requires only ordinal label information for optimization while demo nstrating faster\\nconvergence. Its mathematical formulation is presented in Equat ion ( 2).\\n8'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 8, 'page_label': '9', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nLNLI = log(1 +\\n∑\\nsim(i,j )>sim(k,l )\\nexp(sim(xk, x l) − sim(xi, x j)\\nτ )) (2)\\n3.3.3 CLS\\nThe classiﬁcation loss also adopts the InfoNCE objective. However , since CLS data is\\nprocessed in an example-based manner, directly applying in-batch n egative sampling\\non classiﬁcation datasets with limited categories may lead to false neg atives from items\\nof diﬀerent classes. Numerous studies have proposed diverse app roaches to address\\nthis issue[\\n51][52][47]. We propose a masking mechanism that appends class labels to\\neach positive and negative sample during preprocessing (recorded as separate variables\\nrather than modifying raw text). During in-batch negative sampling , for each negative\\nsample from other data instances, we check whether its label matc hes the current query’s\\nclass. If matched, the negative loss contribution is masked to zero to prevent erroneous\\npenalization; otherwise, it is normally computed. The core loss remain s InfoNCE, with\\nthe CLS loss formulation shown in Equation ( 3). Where Cti denotes the class label of\\nsample ti, and nrepresents the number of negative samples per data instance.\\nLCLS = − 1\\nn\\n∑\\ni\\nlog esim(ti,t +\\ni )/τ\\nZi\\n(3)\\nwhere Zi = esim(ti,t +\\ni )/τ +\\n∑\\nn\\nMASK(ti, t −\\ni,n ) ·esim(ti,t −\\ni,n )/τ +\\n∑\\nj̸=i\\nMASK(ti, t j ) ·esim(ti,t j )/τ +\\n∑\\nj̸=i\\n∑\\nn\\nMASK(ti, t −\\nj,n ) ·esim(ti,t −\\nj,n )/τ\\nand Cti = Ct+\\ni\\nand MASK( ti, t j ) =\\n{\\n0 if Cti = Ctj ,\\n1 otherwise\\n4 Data Synthesis\\nThe production of higher-quality data through data production ha s gained critical im-\\nportance in embedding training. Manual annotation incurs higher co sts and lower\\nproduction eﬃciency, thus developing eﬀective automated data sy nthesis methods has\\nemerged as a key research focus. Recent advancements in large la nguage models (LLMs)\\nhave signiﬁcantly improved their linguistic capabilities, enabling accura te interpretation\\nof human instructions and generation of high-quality outputs. Mult iple existing meth-\\nods have eﬀectively leveraged LLMs to generate high-quality data[\\n28][34], we similarly\\n9'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 9, 'page_label': '10', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nleverages LLM capabilities for data production across three dimens ions: structural di-\\nversity, semantic diversity, and diﬃculty, with dedicated synthesis strategies for each.\\nFor structural diversity, we propose Paraphrasing techniques; for semantic diversity,\\nwe introduce Augmentation methods; and to increase training diﬃcu lty and improve\\nsemantic discriminability, we employ LLMs to generate more challenging hard negative\\nexamples. The following sections detail these methodologies. The co nstraint compo-\\nnents for all data synthesis techniques are speciﬁed in Table 5 of Appendix A.1.\\n4.1 Structural Diversity Enhancement\\nLinguistic structures of text encompass lexical, syntactic, and gr ammatical features,\\nwhich represent relatively surface-level characteristics reﬂect ing word arrangements,\\ncombinations, tenses, voices, and other formal attributes. Emb edding models must\\naccurately capture underlying semantics despite variations in surf ace form, ensuring\\nrobustness to external structural changes. For example, the following two sentences,\\ndespite structural diﬀerences, should be recognized as semantic ally equivalent:\\n• The cat chased the mouse.\\n• The mouse was chased by the cat.\\nTo eﬀectively train an embedding model that remains invariant to str uctural variations\\nwhile accurately capturing semantic information, we propose a Para phrasing strategy.\\nFor each training sample containing a query and a positive document, we apply LLM-\\nbased paraphrasing to both contents, generating augmented ins tances that preserve\\nsemantic equivalence while introducing structural divergence. The prompt constraints\\nand workﬂow are illustrated in Figure\\n3.\\nFigure 3: LLM-based Paraphrasing Workﬂow\\n4.2 Semantic Diversity Enhancement\\nMerely augmenting data through superﬁcial structural modiﬁcat ions yields negligible\\nimprovements in model capabilities, as generalization relies not only on structural dis-\\nentanglement but also on diverse topics and content to ensure unif orm vector rep-\\nresentations in the spatial domain. Therefore, beyond paraphra sing, we propose an\\naugmentation method using LLM to diversify semantics. The core co ncept is: given a\\n10'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 10, 'page_label': '11', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\ncomplete (query, positive) pair, the model must comprehend the d omain and perspec-\\ntive discussed and learn to expand into diﬀerent topics, aspects, a nd viewpoints while\\nremaining contextually anchored. This process is governed via prom pt constraints. The\\nAugmentation framework is illustrated in Figure 4.\\nFigure 4: Semantic Augmentation Workﬂow\\nFigure 5: Hard Negative Synthesis Workﬂow\\n4.3 More challenging embeddings\\nHard negative examples are crucial for enhancing the performanc e of text embedding\\nmodels, often requiring substantial eﬀort to acquire. Leveraging the linguistic capabili-\\nties of large language models, we design an automated hard negative synthesis method\\ntailored for retrieval datasets. Our domain-speciﬁc experiments demonstrate that large\\nlanguage models can generate examples that are indistinguishable, t he framework is\\nillustrated in Figure\\n5.\\nDuring Data paraphrasing and Augmentation, we implement task-sp eciﬁc strategies:\\nfor retrieval tasks, we rewrite/expand (query, positive) pairs a nd add them to the orig-\\ninal dataset; for NLI tasks, we rewrite individual sentences by ra ndomly duplicating\\nexisting entries containing the original sentences and replacing the m with rewritten\\nversions to achieve data expansion—without applying augmentation to prevent ambi-\\nguity; for classiﬁcation tasks, we rewrite sentences while retaining their original labels,\\nexample-based processing was applied using the rewritten results, again without em-\\nploying augmentation. We provide several data synthesis examples in Appendix A.3\\nfor reference.\\n11'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 11, 'page_label': '12', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nFigure 6: Training pipeline\\n5 Training Optimization\\n5.1 Data Grouping Strategy\\nPrior works like Linq-Embedding[\\n52] and SFR-Embedding-Mistral[ 30] adopted task-\\nhomogeneous batching, partitioning data by task rather than mixin g them, and sam-\\npling tasks based on weighted randomness during training. Building on this, we propose\\na reﬁned Data Grouping Strategy, extending the granularity from task-level to dataset-\\nlevel partitioning. We posit that dataset-level grouping captures more domain-speciﬁc\\nclustering patterns—samples within the same dataset often exhibit inherent domain\\nsimilarities, while such consistency may not hold across datasets.\\nOur approach partitions training data into subsets by name. During training, only\\nsamples from a single dataset are sampled per batch, with ﬁle pointer s recorded to\\nenable sequential reading in subsequent iterations. For sampling we ights, we adopt\\nthe data sampling strategy from gte[\\n33] and mgte[ 50], scaling weights by dataset size\\nfollowed by normalization. For dataset i with size li, its sampling weight is computed\\nas Equation ( 4)\\npi = lα\\ni∑ m\\nj=1 lα\\nj\\n(4)\\n5.2 Two-Stage Training\\nInspired by NV-Embed’s[\\n47] two-stage contrastive learning instruction tuning tech-\\nnique, we adopt a similar training approach: the ﬁrst stage exclusive ly uses retrieval-\\noriented training data, while the second stage integrates both ret rieval and non-retrieval\\ntasks, the overall training framework is illustrated in the ﬁgure 6. Two key distinctions\\nare incorporated: ﬁrst, we integrate the previously described Da ta Grouping Strat-\\negy; second, we implement global control over the sampling ratio of retrieval training\\ndatasets, since our ﬁndings indicate that naively incorporating add itional data signiﬁ-\\ncantly degrades retrieval performance.\\nFor global control of sampling ratio, a hyperparameter η is introduced into the sampling\\n12'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 12, 'page_label': '13', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nfunction to control the proportion of retrieval training, ensurin g that throughout the\\nsecond training stage, the computational contribution of retriev al data accounts for η,\\nwhile non-retrieval data constitutes 1 − η. The following set of equations formalizes the\\ncomputational process from partitioned datasets to sampling rat io determination. Let\\nthe training data D = [ d1, d 2, ..., d N ] , where each di represents a distinct dataset (e.g.,\\nMSMARCO passage, SQUAD), with corresponding sizes L = [ l1, l 2, ..., l N ]. Following\\nthe aforementioned strategy, we ﬁrst apply an exponential scalin g factor α , a mask fac-\\ntor M is then applied to ﬁlter retrieval and non-retrieval training sets fo r summation.\\nThe equations are as follows:\\nSret =\\n∑\\ni\\nMi ·lα\\ni\\nSnon ret =\\n∑\\ni\\n(1 − Mi) ·lα\\ni\\nwhere M i =\\n{\\n0 if di ∈ RET,\\n1 else\\nwhere RET denotes the set of retrieval training datasets. The re trieval ratio is then\\nscaled using η to derive the ﬁnal normalized sampling ratios for the training sets:\\nLsamp = [ lsamp\\n1 , l samp\\n2 , ...l samp\\nN ]\\nwhere l samp\\ni =\\n{ ηRET ·lα\\ni\\nSret\\nif di ∈ RET,\\n(1−ηRET )·lα\\ni\\nSnon ret\\nelse\\n6 Experiments\\n6.1 Training Dataset\\nPrimary data sources include bge-en-icl, bge-m3-data, and bge-m ultilingual-gemma2-\\ndata\\n3 . The E5 dataset (approximately 1.5M samples) 4, utilized in E5-Mistral-7B[ 28],\\nEcho Embedding[ 11], and LLM2Vec[ 12], is also incorporated. The aforementioned\\ndatasets include commonly used retrieval training corpora such as MS MARCO (both\\npassage and document versions)[ 64], Natural Questions (NQ)[ 65], ELI5[66], HotpotQA[ 67],\\nMIRACL[68], SQuAD[ 69], FEVER[70], Quora Question Pairs(QQP), and DuReader[ 71],\\netc. Previous researchers have already systematically collected a nd organized these\\ndatasets, making them readily usable, we solely utilized the proposed method to update\\nharder negative samples. Stella’s[ 53] retrieval data llm 5 provides high-quality (query,\\npositive, negative) triplets, while zpoint leverages datasets such a s Huatuo medical QA 6,\\nall above data has been incorporated. Additional data from huggin gface’s sentence-\\ntransformers7 repository includes reddit, hover[ 72], mr-tydi[ 73], law-gpt, and s2orc[ 74].\\n3https://github.com/FlagOpen/FlagEmbedding/tree/master/dataset\\n4https://drive.google.com/ﬁle/d/1YqgaJIzmBIH37XBxpRPCVzV CLh6aOI4/view\\n5https://huggingface.co/datasets/infgrad/retrieval data llm\\n6https://huggingface.co/iampanda/zpoint large embedding zh\\n7https://huggingface.co/sentence-transformers\\n13'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 13, 'page_label': '14', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nOther sources encompass web questions, BioASQ[ 54], cmrc[ 55], CSL 8, nli for simcse\\n(used in SimCSE[ 7] and GTE[ 33]), MLDR 9, GLUE Benchmark[ 56], Yelp Reviews[ 57]\\nand Weibo Sentiment 10 training sets.\\nWe further integrate MTEB evaluation-related datasets like Imdb- Classiﬁcation[58],\\nMassiveIntent-Classiﬁcation[59], MassiveScenario-Classiﬁcation[59], STS12[60], LCQMC[61],\\nPAWSX[62], and STSB[ 63], we utilized the training split from these datasets with con-\\ntamination exclusion applied to remove samples highly similar to test set s.\\nFor data requiring format conversion, we apply the methodologies d escribed in Sen-\\ntion 3.2. Datasets with limited samples (e.g., subsets of bge and e5 series, Im db-\\nClassiﬁcation, STS12, LCQMC) are augmented via Paraphrasing and Augmentation\\n(typically applied to datasets with fewer than 60k samples), we ultima tely obtained ap-\\nproximately 5M high-quality training samples through API interfaces . We deduplicate\\nall training sets and ﬁlter out samples with low query-pos scores usin g GTE-Qwen2-7B-\\nInstruct 11. For retrieval data lacking hard negatives, we employ synthetic ha rd negative\\ngeneration. Due to API cost constraints, only 30% of hard negativ es are synthetically\\ngenerated; the remainder are produced using stella-large-zh-v3 -1792d[53], with top-10\\nto top-30 ranked results selected as hard negatives. The ﬁnal tr aining dataset contains\\n11M quadruples (query, pos, neg, instruction) in total.\\n6.2 Trainset Instructions\\nFor most training data containing instruction formats, we retain th eir original con-\\ntents. For the MTEB training set, we adopt instructions correspo nding to its evalu-\\nation(consistent with Qwen3-Embedding runtime). For external d ata lacking instruc-\\ntions (e.g., Huatuo, Reddit, Law-GPT, GLUE), we design task-spec iﬁc and domain-\\nadaptive instructions. Partial instruction templates are provided in Appendix\\nA.2.\\n6.3 Training Details\\nAs previously mentioned, we adopt a two-stage training approach. For the ﬁrst-stage\\nretrieval training, we train on all retrieval datasets, with a warm- up step of 300 and\\na learning rate of 3e-5, the total step of training is 32k. In the sec ond stage, we use\\nall training data, set the learning rate to 2e-5, and train for 8k ste ps, keeping all other\\nconﬁgurations the same as in the ﬁrst stage. We employ a batch size of 256 for all data\\nusing the InfoNCE loss (i.e., retrieval and classiﬁcation), considerin g data using the\\ncosent loss (i.e., NLI), due to lower memory consumption from the ab sence of forward\\ncomputation for negative samples, the batch size is set to 768. Acr oss all stages, we\\nemploy bﬂoat16 precision, with 4 hard negative samples and a cosine t emperature of\\n0.02, using Adam optimizer with a weight decay of 0.01. The Data Group ing Strategy\\nremains unchanged between the two stages, except that the sec ond stage incorporates\\nall data with a global retrieval ratio ηRET of 0.72. Unlike existing works that commonly\\n8https://github.com/ydli-ai/CSL?tab=readme-ov-ﬁle\\n9https://huggingface.co/datasets/Shitao/MLDR\\n10https://github.com/SophonPlus/ChineseNlpCorpus?tab=readme-ov-ﬁle\\n11https://huggingface.co/Alibaba-NLP/gte-Qwen2-7B-instruct\\n14'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 14, 'page_label': '15', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nuse LoRA ﬁne-tuning, we employ full-parameter ﬁne-tuning at all st ages to ensure\\nmaximum performance improvement. The query and passage length s are set to 256\\nand 1536 respectively. However, in practice, the model can handle sequences up to 8k\\nin length due to the strong length extrapolation capability of the RoP E[35] positional\\nencoding used in most LLMs. The hyperparameter conﬁgurations f or all training stages\\nare provided in the table 1.\\nTable 1: Training Hyperparameter Speciﬁcations\\nItem Stage1 Stage2\\nWarm-up 300\\nSteps 3e-5 2e-5\\nLR 32k 8k\\nBatch Size InfoNCE 256\\nBatch Size Cosent - 768\\nPrecision bﬂoat16\\nTemperature 0.02\\nOptimizer Adam\\nQuery Length 256\\nPassage Length 1536\\n6.4 Compared Methods\\nWe selected the top-10 ranked models(August 27, 2025) on the MT EB/CMTEB leader-\\nboards prior to the release of QZhou-Embedding as baselines. For M TEB, the compar-\\native models include LGAI-Embedding-Preview[\\n17], the Seed series (v1.5[ 75] , v1.6[ 38]),\\nQwen series (8B, 4B)[ 34], ritrieve zh v1, xiaobu-embedding-v2, gemini-embedding-001[ 76],\\njasper en vision language v1[14], Linq-Embed-Mistral[52], SFR-Embedding-Mistral[ 30],\\nand NV-Embed-v2[ 47]. For CMTEB, the baseline models comprise the Seed series (as\\nabove), Qwen series (as above), Conan series (v1[ 24], v2[13]), zpoint large embedding zh,\\nand piccolo-large-zh-v2[ 39].\\n6.5 Main Results\\nThis section presents the evaluation results of Qzhou-embedding o n MTEB/CMTEB\\nbenchmarks, alongside comparative scores from the top 10 ranke d models. As detailed\\nin Table\\n2, Table 3, Qzhou-embedding achieves state-of-the-art performance ac ross\\nboth task-level and task-type average metrics, demonstrating the eﬀectiveness of our\\napproach. Furthermore, under MTEB’s oﬃcial ranking protocol, Q zhou-embedding\\nsecured the top position on both leaderboards. ( Note: Highlighted maximum values\\nin certain columns may reﬂect the best performance among the liste d models rather\\nthan the overall leaderboard maximum, as exempliﬁed by the MTEB/c lassiﬁcation\\nbenchmark where the top score does not appear in the top 10 mode ls.)\\n15'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 15, 'page_label': '16', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nTable 2: Performance on MTEB(eng, v2)\\nModel Class. Clust. Pair Class. Rerank. STS Retr. Summ. Mean(Task) Mean(TaskType)\\nLGAI-Embedding-Preview 89.97 59.25 88.67 49.13 66.18 86.69 38.93 74.12 68.4\\nSeed1.5-Embedding 89.88 60.83 87.39 50.67 67.45 87.23 36.44 74.76 68.56\\nQwen3-Embedding-8B 90.43 58.57 87.52 51.56 69.44 88.58 34.83 75.22 68.71\\nQwen3-Embedding-4B 89.84 57.51 87.01 50.76 68.46 88.72 34.39 74.6 68.1\\nSeed1.6-embedding 92.42 59.22 85.07 50.28 64.9 86.87 37.1 74.07 67.98\\ngemini-embedding-001 90.05 59.39 87.7 48.59 64.35 85.29 38.28 73.3 67.67\\njasper en vision language v1 90.27 60.52 88.14 50 56.05 84.37 37.19 71.41 66.65\\nLinq-Embed-Mistral 83 54.07 88.44 49.44 60.14 84.69 37.26 69.8 65.29\\nSFR-Embedding-Mistral 80.47 54.93 88.59 50.15 59.33 84.77 36.32 69.31 64.94\\nNV-Embed-v2 87.19 47.66 88.69 49.61 62.84 83.82 35.21 69.81 65\\nQZhou-Embedding(Ours) 88.97 61.65 92.43 51.77 67.12 91.65 33.05 75.97 69.52\\nTable 3: Performance on CMTEB(cmn, v1)\\nModel Class. Clust. Pair Class. Rerank. STS Retr. Mean(Task) Mean(TaskType)\\nSeed1.6-embedding 77.98 73.11 88.71 71.65 79.69 68.94 75.63 76.68\\nSeed1.5-Embedding 79.37 71.11 89.57 70.14 79.33 66.56 74.87 76.01\\nritrieve zh v1 76.88 66.5 85.98 72.86 76.97 63.92 72.71 73.85\\nConan-embedding-v2 76.47 68.84 92.44 74.41 78.31 65.48 74.24 75.99\\nxiaobu-embedding-v2 76.53 65.17 85.94 72.58 76.49 64.18 72.36 73.48\\nQwen3-Embedding-8B 76.97 80.08 84.23 66.99 78.21 63.53 73.84 75\\nConan-embedding-v1 76.77 66.33 85.68 72.76 76.67 63.67 72.5 73.65\\nzpoint large embedding zh 76.4 62.23 85.75 72.33 76.36 63.86 71.81 72.82\\npiccolo-large-zh-v2 76.42 62.16 85.22 70 74.36 63.46 70.86 71.94\\nQwen3-Embedding-4B 75.46 77.89 83.34 66.05 77.03 61.26 72.27 73.51\\nQZhou-Embedding(Ours) 79.99 70.91 95.07 74.85 78.80 71.89 76.99 78.58\\n7 Conclusion\\nIn this technical report, we present QZhou-Embedding, a genera l-purpose contextual\\ntext embedding model with exceptional text representation capa bilities. We designed a\\nuniﬁed multi-task framework comprising specialized data transform ation and training\\nstrategies, eﬀectively enhanced the diversity of training data. To further improve the\\nquality of training data and the model’s generalization capabilities, we d eveloped a data\\nsynthesis pipeline leveraging LLM API, incorporating techniques suc h as Paraphrasing,\\nAugmentation, and Hard negative example generation. We employ a t wo-stage training\\nstrategy comprising initial retrieval-focused training followed by fu ll-task ﬁne-tuning,\\nenabling the embedding model to extend its capabilities based on robu st retrieval per-\\nformance. The model achieves state-of-the-art results on the MTEB and CMTEB\\nbenchmarks, ranking ﬁrst on both leaderboards. Our ﬁndings est ablish that data qual-\\nity and diversity are pivotal for improving embedding model capabilitie s. In the future,\\nwe will focus on developing multimodal and multilingual embedding models , as well\\nas exploring eﬀective applications of embedding models in agent syste ms, aiming to\\nintegrate cutting-edge technologies to optimize this classical modu le.\\nReferences\\n[1] Robertson, Stephen E., and Steve Walker. ”Some simple eﬀective approximations to\\nthe 2-poisson model for probabilistic weighted retrieval.” In SIGIR’9 4: Proceedings\\n16'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 16, 'page_label': '17', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nof the Seventeenth Annual International ACM-SIGIR Conferen ce on Research and\\nDevelopment in Information Retrieval, organised by Dublin City Univer sity, pp.\\n232-241. London: Springer London, 1994.\\n[2] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutano va. Bert: Pre-\\ntraining of deep bidirectional transformers for language underst anding. arXiv\\npreprint arXiv:1810.04805, 2018.\\n[3] Colin Raﬀel, Noam Shazeer, Adam Roberts, Katherine Lee, Shara n Narang, Michael\\nMatena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of tr ansfer learn-\\ning with a uniﬁed text-to-text transformer. Journal of machine le arning research,\\n21(140):1–67, 2020.\\n[4] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, D axin Jiang,\\nRangan Majumder, and Furu Wei. Text embeddings by weakly-super vised con-\\ntrastive pre-training. arXiv preprint arXiv:2212.03533, 2022.\\n[5] Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Ried el, Piotr Bo-\\njanowski, Armand Joulin, and Edouard Grave. Unsupervised dense information\\nretrieval with contrastive learning. arXiv preprint arXiv:2112.0911 8, 2021.\\n[6] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence em beddings using\\nsiamese bert-networks. arXiv preprint arXiv:1908.10084, 2019.\\n[7] Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple contrastive\\nlearning of sentence embeddings. In Proceedings of the 2021 Conf erence on Empir-\\nical Methods in Natural Language Processing, pages 6894–6910, Online and Punta\\nCana, Dominican Republic. Association for Computational Linguistics .\\n[8] Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hern´ andez ´Abrego, Ji Ma,\\nVincent Y Zhao, Yi Luan, Keith B Hall, Ming-Wei Chang, et al. Large du al encoders\\nare generalizable retrievers. arXiv preprint arXiv:2112.07899, 202 1.\\n[9] Brown, Tom, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D . Kaplan, Pra-\\nfulla Dhariwal, Arvind Neelakantan et al. ”Language models are few-s hot learners.”\\nAdvances in neural information processing systems 33 (2020): 18 77-1901.\\n[10] Ma, Xueguang, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin. ”F ine-tuning\\nllama for multi-stage text retrieval.” In Proceedings of the 47th Int ernational ACM\\nSIGIR Conference on Research and Development in Information Re trieval, pp. 2421-\\n2425. 2024.\\n[11] Springer, Jacob Mitchell, Suhas Kotha, Daniel Fried, Graham Ne ubig, and Aditi\\nRaghunathan. ”Repetition improves language model embeddings.” a rXiv preprint\\narXiv:2402.15449 (2024).\\n[12] BehnamGhader, Parishad, Vaibhav Adlakha, Marius Mosbach, D zmitry Bah-\\ndanau, Nicolas Chapados, and Siva Reddy. ”Llm2vec: Large languag e models are\\nsecretly powerful text encoders.” arXiv preprint arXiv:2404.0596 1 (2024).\\n[13] https://cloud.tencent.com/developer/news/2461911\\n17'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 17, 'page_label': '18', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\n[14] Zhang, Dun, Jiacheng Li, Ziyang Zeng, and Fulong Wang. ”Jaspe r and stella:\\ndistillation of sota embedding models.” arXiv preprint arXiv:2412.19048 (2024).\\n[15] Chen, Jianlv, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng\\nLiu. ”Bge m3-embedding: Multi-lingual, multi-functionality, multi-gran ularity text\\nembeddings through self-knowledge distillation.” arXiv preprint arXiv :2402.03216\\n(2024).\\n[16] Ji, Yifan, Zhipeng Xu, Zhenghao Liu, Yukun Yan, Shi Yu, Yishan L i, Zhiyuan\\nLiu, Yu Gu, Ge Yu, and Maosong Sun. ”Learning more eﬀective repre senta-\\ntions for dense retrieval through deliberate thinking before sear ch.” arXiv preprint\\narXiv:2502.12974 (2025).\\n[17] Choi J, Kim H, Jang H, et al. LG-ANNA-Embedding technical repo rt[J]. arXiv\\npreprint arXiv:2506.07438, 2025.\\n[18] Xiong, Lee, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Pau l Bennett,\\nJunaid Ahmed, and Arnold Overwijk. ”Approximate nearest neighbo r negative con-\\ntrastive learning for dense text retrieval.” arXiv preprint arXiv:200 7.00808 (2020).\\n[19] Lee, Chankyu, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad\\nShoeybi, Bryan Catanzaro, and Wei Ping. ”Nv-embed: Improved t echniques for\\ntraining llms as generalist embedding models.” arXiv preprint arXiv:2405 .17428\\n(2024).\\n[20] Moreira, Gabriel de Souza P., Radek Osmulski, Mengyao Xu, Rona y Ak, Benedikt\\nSchiﬀerer, and Even Oldridge. ”NV-Retriever: Improving text emb edding models\\nwith eﬀective hard-negative mining.” arXiv preprint arXiv:2407.15831 (2024).\\n[21] Team, Qwen. ”Qwen2 technical report.” arXiv preprint arXiv:24 07.10671 (2024).\\n[22] Xiao, Shitao, Zheng Liu, Peitian Zhang, Niklas Muennighoﬀ, Defu L ian, and Jian-\\nYun Nie. ”C-pack: Packed resources for general chinese embedd ings.” In Proceedings\\nof the 47th international ACM SIGIR conference on research and development in\\ninformation retrieval, pp. 641-649. 2024. Team, Qwen.\\n[23] Muennighoﬀ, Niklas, Nouamane Tazi, Lo¨ ıc Magne, and Nils Reimers . ”Mteb: Mas-\\nsive text embedding benchmark.” arXiv preprint arXiv:2210.07316 (2 022).\\n[24] Li, Shiyu, Yang Tang, Shizhe Chen, and Xi Chen. ”Conan-embed ding: Gen-\\neral text embedding with more and better negative samples.” arXiv p reprint\\narXiv:2408.15710 (2024).\\n[25] Aizawa, Akiko. ”An information-theoretic perspective of tf–id f measures.” Infor-\\nmation Processing & Management 39, no. 1 (2003): 45-65.\\n[26] Robertson, Stephen E., and Steve Walker. ”Some simple eﬀectiv e approximations\\nto the 2-poisson model for probabilistic weighted retrieval.” In SIGI R’94: Proceed-\\nings of the Seventeenth Annual International ACM-SIGIR Confe rence on Research\\nand Development in Information Retrieval, organised by Dublin City Un iversity,\\npp. 232-241. London: Springer London, 1994.\\n18'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 18, 'page_label': '19', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\n[27] Deerwester, Scott, Susan T. Dumais, George W. Furnas, Tho mas K. Landauer, and\\nRichard Harshman. ”Indexing by latent semantic analysis.” Journal of the American\\nsociety for information science 41, no. 6 (1990): 391-407.\\n[28] Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Maj umder, and\\nFuru Wei. Improving text embeddings with large language models. arX iv preprint\\narXiv:2401.00368, 2023b.\\n[29] Meng, Rui, Ye Liu, Shaﬁq Rayhan Joty, Caiming Xiong, Yingbo Zhou , and Semih\\nYavuz. ”Sfrembedding-mistral: enhance text retrieval with tran sfer learning.” Sales-\\nforce AI Research Blog 3 (2024): 6.\\n[30] Meng R, Liu Y, Joty S R, et al. Sfr-embedding-2: Advanced text embedding with\\nmulti-stage training, 2024[J].\\n[31] Muennighoﬀ, Niklas, S. U. Hongjin, Liang Wang, Nan Yang, Furu W ei, Tao Yu,\\nAmanpreet Singh, and Douwe Kiela. ”Generative representational instruction tun-\\ning.” In The Thirteenth International Conference on Learning Rep resentations.\\n2024.\\n[32] Chaofan Li, MingHao Qin, Shitao Xiao, Jianlyu Chen, Kun Luo, Yingx ia Shao,\\nDefu Lian, and Zheng Liu. Making text embedders few-shot learner s. arXiv preprint\\narXiv:2409.15700, 2024.\\n[33] Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie , and Meis-\\nhan Zhang. Towards general text embeddings with multi-stage con trastive learning,\\n2023. URL https://arxiv.org/abs/2308.03281.\\n[34] Zhang, Yanzhao, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, B aosong Yang,\\nPengjun Xie et al. ”Qwen3 Embedding: Advancing Text Embedding and Reranking\\nThrough Foundation Models.” arXiv preprint arXiv:2506.05176 (2025 ).\\n[35] Su, Jianlin, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, an d Yunfeng Liu.\\n”Roformer: Enhanced transformer with rotary position embeddin g.” Neurocomput-\\ning 568 (2024): 127063.\\n[36] Zhang, Biao, and Rico Sennrich. ”Root mean square layer norma lization.” Ad-\\nvances in neural information processing systems 32 (2019).\\n[37] Shazeer, Noam. ”Glu variants improve transformer.” arXiv pre print\\narXiv:2002.05202 (2020).\\n[38] https://seed1-6-embedding.github.io/\\n[39] Huang, Junqin, Zhongjie Hu, Zihao Jing, Mengya Gao, and Yichao Wu. ”Pic-\\ncolo2: General text embedding with multi-task hybrid loss training.” a rXiv preprint\\narXiv:2405.06932 (2024).\\n[40] Sun, Yifan, Changmao Cheng, Yuhan Zhang, Chi Zhang, Liang Z heng, Zhongdao\\nWang, and Yichen Wei. ”Circle loss: A uniﬁed perspective of pair similarit y op-\\ntimization.” In Proceedings of the IEEE/CVF conference on comput er vision and\\npattern recognition, pp. 6398-6407. 2020.\\n19'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 19, 'page_label': '20', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\n[41] Rodrigo Nogueira, Wei Yang, Jimmy Lin, and Kyunghyun Cho. 201 9. Document\\nexpansion by query prediction. ArXiv preprint, abs/1904.08375.\\n[42] Liang Wang, Nan Yang, and Furu Wei. 2023. Query2doc: Query e xpansion with\\nlarge language models. In Proceedings of the 2023 Conference on E mpirical Meth-\\nods in Natural Language Processing, pages 9414–9423, Singapor e. Association for\\nComputational Linguistics.\\n[43] Zhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, An ton Bakalov,\\nKelvin Guu, Keith Hall, and Ming-Wei Chang. 2022. Promptagator: Fe wshot dense\\nretrieval from 8 examples. In The Eleventh International Confer ence on Learning\\nRepresentations.\\n[44] Kexin Wang, Nandan Thakur, Nils Reimers, and Iryna Gurevych. 2022a. GPL:\\nGenerative pseudo labeling for unsupervised domain adaptation of d ense retrieval.\\nIn Proceedings of the 2022 Conference of the North American Cha pter of the\\nAssociation for Computational Linguistics: Human Language Techn ologies, pages\\n2345–2360, Seattle, United States. Association for Computation al Linguistics.\\n[45] Honovich, Or, Thomas Scialom, Omer Levy, and Timo Schick. ”Unn atural in-\\nstructions: Tuning language models with (almost) no human labor.” ar Xiv preprint\\narXiv:2212.09689 (2022).\\n[46] Xiong, Lee, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Pau l Bennett,\\nJunaid Ahmed, and Arnold Overwijk. ”Approximate nearest neighbo r negative con-\\ntrastive learning for dense text retrieval.” arXiv preprint arXiv:200 7.00808 (2020).\\n[47] Moreira, Gabriel de Souza P., Radek Osmulski, Mengyao Xu, Rona y Ak, Benedikt\\nSchiﬀerer, and Even Oldridge. ”NV-Retriever: Improving text emb edding models\\nwith eﬀective hard-negative mining.” arXiv preprint arXiv:2407.15831 (2024).\\n[48] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representatio n learning with\\ncontrastive predictive coding. arXiv preprint arXiv:1807.03748, 20 18.\\n[49] https://www.kexue.fm/archives/8847\\n[50] Xin Zhang, Yanzhao Zhang, Dingkun Long, Wen Xie, Ziqi Dai, Jialon g Tang, Huan\\nLin, Baosong Yang, Pengjun Xie, Fei Huang, Meishan Zhang, Wenjie Li, and Min\\nZhang. mgte: Generalized long-context text representation and reranking models\\nfor multilingual text retrieval, 2024.\\n[51] Lee, Jinhyuk, Zhuyun Dai, Xiaoqi Ren, Blair Chen, Daniel Cer, Je remy R. Cole,\\nKai Hui et al. ”Gecko: Versatile text embeddings distilled from large la nguage\\nmodels, 2024.” URL https://arxiv. org/abs/2403.20327.\\n[52] Junseong Kim, Seolhwa Lee, Jihoon Kwon, Sangmo Gu, Yejin Kim, M inkyung\\nCho, Jy yong Sohn, and Chanyeol Choi. Linq-embed-mistral: Elevat ing text re-\\ntrieval with improved gpt data through task-speciﬁc control and quality reﬁnement.\\nlinq ai research blog, 2024.\\n[53] https://huggingface.co/dunzhang/stella-large-zh-v3-1792d\\n20'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 20, 'page_label': '21', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\n[54] Tsatsaronis G, Balikas G, Malakasiotis P, et al. An overview of the BIOASQ large-\\nscale biomedical semantic indexing and question answering competitio n[J]. BMC\\nbioinformatics, 2015, 16(1): 138.\\n[55] Cui Y, Liu T, Che W, et al. A span-extraction dataset for Chines e machine reading\\ncomprehension[J]. arXiv preprint arXiv:1810.07366, 2018.\\n[56] Wang A, Singh A, Michael J, et al. GLUE: A multi-task benchmark a nd analysis\\nplatform for natural language understanding[J]. arXiv preprint ar Xiv:1804.07461,\\n2018.\\n[57] Yelp Dataset. Yelp Inc., [Year]. Available: https://www.yelp.com/dataset\\n[58] Maas A, Daly R E, Pham P T, et al. Learning word vectors for sent iment analy-\\nsis[C]//Proceedings of the 49th annual meeting of the association f or computational\\nlinguistics: Human language technologies. 2011: 142-150.\\n[59] Jack FitzGerald, Christopher Hench, Charith Peris, Scott Mac kie, Kay Rottmann,\\nAna Sanchez, Aaron Nash, Liam Urbach, Vishesh Kakarala, Richa Sin gh, Swetha\\nRanganath, Laurie Crist, Misha Britan, Wouter Leeuwis, Gokhan Tu r, and Prem\\nNatarajan. 2022. Massive: A 1m-example multilingual natural langu age understand-\\ning dataset with 51 typologically-diverse languages.\\n[60] Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre . 2012. Semeval-\\n2012 task 6: A pilot on semantic textual similarity. In * SEM 2012: The First\\nJoint Conference on Lexical and Computational Semantics–Volume 1: Proceedings\\nof the main conference and the shared task, and Volume 2: Procee dings of the Sixth\\nInternational Workshop on Semantic Evaluation (SemEval 2012), pages 385–393.\\n[61] Liu, Xin, Qingcai Chen, Chong Deng, Huajun Zeng, Jing Chen, Do ngfang Li,\\nand Buzhou Tang. ”Lcqmc: A large-scale chinese question matching corpus.” In\\nProceedings of the 27th international conference on computatio nal linguistics, pp.\\n1952-1962. 2018.\\n[62] Yang, Yinfei, Yuan Zhang, Chris Tar, and Jason Baldridge. ”PAW S-X: A\\ncross-lingual adversarial dataset for paraphrase identiﬁcation .” arXiv preprint\\narXiv:1908.11828 (2019).\\n[63] Cer, Daniel, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and L ucia Specia.\\n”Semeval-2017 task 1: Semantic textual similarity-multilingual and c ross-lingual\\nfocused evaluation.” arXiv preprint arXiv:1708.00055 (2017).\\n[64] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh T iwary, Rangan\\nMajumder, and Li Deng. 2016. MS MARCO: A human generated mach ine read-\\ning comprehension dataset. In Proceedings of the Workshop on Co gnitive Com-\\nputation: Integrating neural and symbolic approaches 2016 co-lo cated with the\\n30th Annual Conference on Neural Information Processing Syst ems (NIPS 2016),\\nBarcelona, Spain, December 9, 2016, volume 1773 of CEUR Worksho p Proceedings.\\nCEUR-WS.org.\\n21'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 21, 'page_label': '22', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\n[65] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redﬁeld, Michael Collins , Ankur\\nParikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Ke nton Lee,\\net al. Natural questions: a benchmark for question answering res earch. Transactions\\nof the Association for Computational Linguistics, 7:453–466, 2019 .\\n[66] Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jaso n Weston, and\\nMichael Auli. 2019. ELI5: Long Form Question Answering. In Procee dings of\\nthe 57th Annual Meeting of the Association for Computational Ling uistics, pages\\n3558–3567, Florence, Italy. Association for Computational Lingu istics.\\n[67] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan\\nSalakhutdinov, and Christopher D. Manning. HotpotQA: A dataset for diverse,\\nexplainable multi-hop question answering. In Proceedings of the 201 8 Conference\\non Empirical Methods in Natural Language Processing, pp. 2369–2 380, Brussels,\\nBelgium, October-November 2018. Association for Computational Linguistics. doi:\\n10.18653/v1/D18-1259. URL https://aclanthology.org/D18-125 9.\\n[68] Xinyu Zhang, Nandan Thakur, Odunayo Ogundepo, Ehsan Kama lloo, David\\nAlfonso-Hermelo, Xiaoguang Li, Qun Liu, Mehdi Rezagholizadeh, and Jimmy Lin.\\nMiracl: A multilingual retrieval dataset covering 18 diverse language s. Transactions\\nof the Association for Computational Linguistics, 11:1114–1131, 2 023.\\n[69] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Per cy Liang.\\nSquad: 100,000+ questions for machine comprehension of text. ar Xiv preprint\\narXiv:1606.05250, 2016.\\n[70] James Thorne, Andreas Vlachos, Christos Christodoulopoulos , and Arpit Mit-\\ntal. Fever: a large-scale dataset for fact extraction and veriﬁca tion. arXiv preprint\\narXiv:1803.05355, 2018.\\n[71] Wei He, Kai Liu, Jing Liu, Yajuan Lyu, Shiqi Zhao, Xinyan Xiao, Yu an Liu,\\nYizhong Wang, Hua Wu, Qiaoqiao She, Xuan Liu, Tian Wu, and Haifeng Wa ng.\\n2018. DuReader: a Chinese Machine Reading Comprehension Datase t from Real-\\nworld Applications. In Proceedings of the Workshop on Machine Read ing for Ques-\\ntion Answering, pages 37–46, Melbourne, Australia. Association fo r Computational\\nLinguistics.\\n[72] Yichen Jiang, Shikha Bordia, Zheng Zhong, Charles Dognin, Mane esh Singh, and\\nMohit Bansal. 2020. HoVer: A Dataset for Many-Hop Fact Extract ion And Claim\\nVeriﬁcation. In Findings of the Association for Computational Lingu istics: EMNLP\\n2020, pages 3441–3460, Online. Association for Computational Lin guistics.\\n[73] Zhang X, Ma X, Shi P, et al. Mr. TyDi: A multi-lingual benchmark fo r dense\\nretrieval[J]. arXiv preprint arXiv:2108.08787, 2021.\\n[74] Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Danie l Weld. 2020.\\nS2ORC: The Semantic Scholar Open Research Corpus. In Proceedin gs of the 58th\\nAnnual Meeting of the Association for Computational Linguistics, p ages 4969–4983,\\nOnline. Association for Computational Linguistics.\\n22'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 22, 'page_label': '23', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\n[75] https://huggingface.co/spaces/mteb/leaderboard\\n[76] Jinhyuk Lee, Feiyang Chen, Sahil Dua, Daniel Cer, Madhuri Sha nbhogue, Iftekhar\\nNaim, Gustavo Hernandez /acute.ts1Abrego, Zhe Li, Kaifeng Chen, Henrique Schechter\\nVera, et al. Gemini embedding: Generalizable embeddings from gemini. arXiv\\npreprint arXiv:2503.07891, 2025b.\\nA Appendix\\nA.1 Framework Constraints\\nTable 4: Speciﬁcations of framework constraints\\nItem Explanation\\nKeep core semantics Preserving the core semantic content, which is the\\nmost critical requirement.\\nDiversity in morphology,\\nsyntax, grammar, tense,\\nrhetoric, etc\\nVariations in lexical composition, syntactic struc-\\nture, grammatical rules, and tense usage are per-\\nmitted.\\nLength within ±15% The length deviation from the original sentence\\nshould not exceed 15%.\\nKeep language The language used must be consistent with the\\noriginal sentence.\\nClose in ﬁeld The content must remain strictly aligned with the\\ndomain of the given sentence.\\nTopic transfer, expansion,\\nextension, prohibiting pure\\nrewriting\\nTopic shifting, extension, or elaboration is permit-\\nted, but purely paraphrased content (identical to\\nthe original topic) is prohibited.\\nPOS is the perfect\\nanswer(necessary &\\nsuﬃcient)\\nPositive examples must be unambiguous and pre-\\ncisely address the query (necessity condition) while\\ncontaining exclusively relevant content without ex-\\ntraneous information (suﬃciency condition).\\nHard NEG: Worse than\\nPOS:\\n- Semantic deviation\\n(inadequate)\\n- Including irrelevant\\ninformation(unnecessary)\\n- Diﬀerent aspects of the\\nsame topic\\nHard negative examples must exhibit inferior qual-\\nity compared to positive instances, with noise in-\\ntroduced through three strategies: 1) semantic de-\\nviation (failing to accurately address the query),\\n2) incorporation of irrelevant information, or 3)\\nmaintaining the same topic but diverging in as-\\npects.\\nImitation: syntax, sentence\\nstructure, structural\\nGenerating hard negative examples by emulating\\nthe structural and syntactic patterns of the given\\npositive instance is a critical step to maximize dis-\\ncriminative challenge for the model.\\n23'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 23, 'page_label': '24', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nA.2 Instruction Examples\\nTable 5: Instruction for partial training data\\nDataset Instruction\\nHuatuo Given a medical question, retrieve user replies that\\nbest answer the question\\nReddit Retrieve the paragraph most semantically similar\\nto the given statement\\nLaw-GPT Retrieve relevant legal provisions or interpreta-\\ntions for the given case\\nMNLI/SNLI Retrieve semantically similar text\\nYelp Classify the customer review of businesses\\nWeibo Classify the sentiment of Weibo comments\\nA.3 Data Synthesis Examples\\nNote: The text highlighted in yellow represents the original sentence, fo llowed by the\\nsynthetically generated sentence.\\nTable 6: Paraphrasing Example (1)\\nquery pos\\nWhat is the best credit\\ncard for someone with no\\ncredit history?\\nIf you’ve never had a credit card before a likely\\nreason can be due to lack of credit history. You\\ncan apply for a department store card.\\nWhat’s the ideal credit\\ncard for a person without\\nany credit history?\\nIf you’ve never had a credit card, it’s probably\\nbecause you don’t have a credit history. A depart-\\nment store card could be a good option to apply\\nfor.\\nWhat’s the top credit card\\nchoice for someone who has\\nno credit history?\\nIf you’ve never owned a credit card, it’s probably\\nbecause you don’t have a credit history. A depart-\\nment store card might be a good option to con-\\nsider.\\n24'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 24, 'page_label': '25', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nTable 7: Paraphrasing Example (2)\\nquery pos\\nWhich English Poet\\nLaureate wrote ’The Faerie\\nQueene’?\\nEnglish Renaissance to begin, shakily, in the 1520s,\\nand it continued until perhaps 1620. England had\\na strong tradition of literature in the English ver-\\nnacular, which gradually increased as English use\\nof the printing press became common during the\\nmid 16th century. By the time of Elizabethan liter-\\nature a vigorous literary culture in both drama and\\npoetry included poets such as Edmund Spenser,\\nwhose verse epic ’The Faerie Queene’ had a strong\\ninﬂuence on English literature but was eventu-\\nally overshadowed by the lyrics of William Shake-\\nspeare, Thomas Wyatt and others. Typically, the\\nworks of these playwrights and poets circulated in\\nmanuscript form.\\nWho was the English Poet\\nLaureate that penned ’The\\nFaerie Queene’?\\nThe English Renaissance began, albeit slowly, in\\nthe 1520s and likely lasted until around 1620. Eng-\\nland boasted a robust literary tradition in the En-\\nglish language, which expanded as the printing\\npress became more widely adopted during the mid-\\n16th century. By the time of Elizabethan litera-\\nture, a dynamic literary scene in both drama and\\npoetry emerged, featuring writers like Edmund\\nSpenser, whose epic poem The Faerie Queene sig-\\nniﬁcantly inﬂuenced English literature, although\\nit was eventually overshadowed by the works of\\nWilliam Shakespeare, Thomas Wyatt, and others.\\nGenerally, the writings of these poets and play-\\nwrights circulated in manuscript form.\\nWhich English Poet\\nLaureate authored ’The\\nFaerie Queene’?\\nThe English Renaissance began somewhat uncer-\\ntainly in the 1520s, continuing through to around\\n1620. England had a rich literary heritage in its\\nnative tongue, which expanded as the printing\\npress became more widely used in the mid-16th\\ncentury. By the Elizabethan era, a thriving lit-\\nerary culture in both drama and poetry emerged,\\nwith poets like Edmund Spenser, whose epic The\\nFaerie Queene profoundly impacted English liter-\\nature, though it was eventually eclipsed by the\\nworks of William Shakespeare, Thomas Wyatt,\\nand others. Generally, the writings of these play-\\nwrights and poets were circulated in manuscript\\nform.\\n25'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 25, 'page_label': '26', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nTable 8: Augmentation Example\\nquery pos neg\\nWhat is the best\\ncredit card for\\nsomeone with no\\ncredit history?\\nIf you’ve never had a credit\\ncard before a likely reason\\ncan be due to lack of credit\\nhistory. You can apply for a\\ndepartment store card.\\n-\\nWhich credit card is\\neasiest to get\\napproved for with bad\\ncredit?\\nFor those with poor credit\\nscores, secured credit cards\\nfrom Capital One or Dis-\\ncover are often the most ac-\\ncessible options since they\\nrequire a refundable de-\\nposit.\\nSome premium travel cards\\nlike Chase Sapphire have\\ngreat rewards but usually\\nneed excellent credit. Store\\ncards may accept lower\\nscores but have high inter-\\nest rates.\\nWhat credit cards\\ncan I get as a college\\nstudent with no\\ncredit?\\nStudents without credit his-\\ntory often qualify for starter\\ncards like Discover it Stu-\\ndent or Capital One Jour-\\nney, which are designed to\\nhelp build credit.\\nPremium cards like Amer-\\nican Express Platinum re-\\nquire good credit history,\\nthough some banks oﬀer\\nstudent accounts with debit\\ncards.\\nWhich English Poet\\nLaureate wrote ’The\\nFaerie Queene’?\\n...By the time of Eliz-\\nabethan literature a vig-\\norous literary culture in\\nboth drama and poetry in-\\ncluded poets such as Ed-\\nmund Spenser, whose verse\\nepic ’The Faerie Queene’\\nhad a strong inﬂuence on\\nEnglish literature but was\\neventually overshadowed by\\nthe lyrics of William ...\\n-\\nWhat major epic\\npoem did Edmund\\nSpenser write during\\nQueen Elizabeth’s\\nreign?\\nEdmund Spenser composed\\n’The Faerie Queene’, an\\nallegorical epic poem that\\nbecame one of the most\\nsigniﬁcant works of Eliz-\\nabethan literature though\\nlater eclipsed by Shake-\\nspeare’s popularity.\\nChristopher Marlowe’s\\n’Hero and Leander’ was an-\\nother notable Elizabethan\\npoem, but unlike Spenser’s\\nwork it wasn’t an epic\\nallegory.\\nWhich poet created\\n’Paradise Lost’ during\\nthe English\\nRenaissance?\\nJohn Milton authored the\\nepic poem ’Paradise Lost’\\nin the 17th century, a mon-\\numental work that explored\\nbiblical themes through\\nblank verse and became\\na cornerstone of English\\nliterature.\\nWilliam Blake’s ’The Mar-\\nriage of Heaven and Hell’\\nalso dealt with religious\\nthemes, though it was more\\nprophetic than epic in style\\ncompared to Milton’s mas-\\nterpiece.\\n26'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 26, 'page_label': '27', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nTable 9: Hard-Negative Generation Example\\nquery pos neg\\nWhat territory was\\nKing Hussein afraid\\nIsrael would obtain?\\n...Hussein was nonetheless\\nwary that an Egyptian-\\nIsraeli war would risk the\\nWest Bank’s occupation by\\nIsrael...\\n-\\nWhat territory was\\nKing Hussein afraid\\nIsrael would obtain?\\n...Hussein was nonetheless\\nwary that an Egyptian-\\nIsraeli war would risk the\\nWest Bank’s occupation by\\nIsrael...\\nKing Hussein expressed\\nconcerns about potential\\nIsraeli expansion during\\nthe Arab-Israeli conﬂicts,\\nthough his warnings to\\nNasser were delayed and\\ninitially dismissed, while\\nother Arab leaders focused\\nmore on direct military\\npreparations against Israel.\\nWhat territory was\\nKing Hussein afraid\\nIsrael would obtain?\\n...Hussein was nonetheless\\nwary that an Egyptian-\\nIsraeli war would risk the\\nWest Bank’s occupation by\\nIsrael...\\nKing Hussein expressed\\nconcerns about potential\\nIsraeli territorial expansion\\nduring the 1967 tensions,\\nthough his warnings were\\ndelayed in reaching Nasser\\nand mixed with broader\\nregional tensions, while\\nEgyptian military move-\\nments in Sinai were already\\nunderway under Amer’s\\norders.\\n27'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Title: A Survey of Object Detection: From Region Proposals to End-to-End \\nTransformers \\nAuthor: [Your Name/Institution] Date: October 2025 \\n \\nAbstract \\nObject detection, a fundamental task in computer vision, involves identifying and localizing \\ninstances of objects within an image or video. It goes beyond simple image classification by not only \\ndetermining the class of an object but also providing a bounding box that precisely outlines its \\nlocation. This paper provides a comprehensive survey of the evolution of object detection \\nmethodologies, primarily focusing on the deep learning era. We begin by contextualizing the \\nproblem with a brief overview of traditional computer vision techniques. The core of the review is \\ndedicated to the two dominant paradigms in deep learning-based detectors: two-stage detectors, \\nexemplified by the R-CNN family (R-CNN, Fast R-CNN, Faster R-CNN), which prioritize accuracy \\nthrough a region proposal mechanism; and single-stage detectors, such as YOLO and SSD, which \\noptimize for speed by performing detection in a single pass. We then explore key architectural \\ncomponents like backbone networks, anchor boxes, and non-maximum suppression. The survey \\nculminates with a discussion of modern architectures, including the paradigm-shifting DETR \\n(DEtection TRansformer), which reframes object detection as an end-to-end set prediction problem. \\nFinally, we cover standard evaluation metrics, common datasets, real-world applications, and the \\nongoing challenges and future directions that are shaping the field. \\nKeywords: Object Detection, Computer Vision, Deep Learning, R-CNN, YOLO, SSD, Transformer, \\nDETR, Bounding Box, mAP.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Table of Contents \\n1. Introduction 1.1. Defining Object Detection: Classification and Localization 1.2. Distinction \\nfrom Other Vision Tasks 1.3. The Importance of Object Detection 1.4. Paper Structure \\n2. Background and Foundational Concepts 2.1. Traditional Computer Vision Approaches \\n(Viola-Jones, HOG) 2.2. The Sliding Window Method 2.3. The Deep Learning Revolution \\n3. Two-Stage Object Detectors: A Focus on Accuracy 3.1. The \"Propose, then Classify\" \\nParadigm 3.2. R-CNN: Regions with CNN Features 3.3. Fast R-CNN: Sharing Computation \\n3.4. Faster R-CNN: The Region Proposal Network (RPN) \\n4. Single-Stage Object Detectors: A Focus on Speed 4.1. The \"Single Pass\" Paradigm 4.2. \\nYOLO: You Only Look Once 4.3. SSD: Single Shot MultiBox Detector \\n5. Key Architectural Components and Innovations 5.1. Backbone Networks: The Feature \\nExtractors 5.2. Anchor Boxes: The Priors for Prediction 5.3. Non-Maximum Suppression \\n(NMS): Pruning Redundant Detections \\n6. Modern Architectures and the Rise of Transformers 6.1. Balancing Speed and Accuracy: \\nEfficientDet 6.2. DETR: End-to-End Object Detection with Transformers \\n7. Evaluation Metrics and Datasets 7.1. Intersection over Union (IoU) 7.2. Average Precision \\n(AP) and mean Average Precision (mAP) 7.3. Landmark Datasets (PASCAL VOC, COCO) \\n8. Applications and Real-World Use Cases 8.1. Autonomous Vehicles 8.2. Medical Imaging \\n8.3. Retail and Inventory Management 8.4. Security and Surveillance \\n9. Challenges and Future Directions 9.1. Detecting Small and Occluded Objects 9.2. The \\nSpeed vs. Accuracy Trade-off 9.3. Domain Adaptation and Generalization 9.4. Few-Shot and \\nZero-Shot Detection \\n10. Conclusion \\n11. References'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='1. Introduction \\n1.1. Defining Object Detection: Classification and Localization \\nObject detection is a core computer vision task concerned with answering two fundamental questions \\nabout an image: \"What objects are in this image?\" and \"Where are they located?\". The first \\nquestion is a classification task, assigning a class label (e.g., \"cat,\" \"car,\" \"person\") to an object. The \\nsecond is a localization task, providing a tight-fitting bounding box (typically defined by x/y \\ncoordinates and width/height) around each identified object. \\n1.2. Distinction from Other Vision Tasks \\nIt is crucial to distinguish object detection from related tasks: \\n• Image Classification: Simply assigns one label to an entire image (e.g., \"this is a picture of a \\ncat\"). \\n• Semantic Segmentation: Assigns a class label to every pixel in the image but does not \\ndistinguish between different instances of the same object (e.g., all pixels belonging to any \\nperson are labeled \"person\"). \\n• Instance Segmentation: Assigns a class label to every pixel and differentiates between \\nobject instances (e.g., \"person 1,\" \"person 2,\" \"person 3\"). Object detection can be seen as a \\nprecursor to this more complex task. \\n1.3. The Importance of Object Detection \\nThe ability to detect and locate objects is foundational to how machines perceive and interact with \\nthe physical world. It is the technology that enables self-driving cars to see pedestrians and other \\nvehicles, allows doctors to identify tumors in medical scans, and helps robots navigate complex \\nenvironments. Its broad applicability has made it one of the most actively researched areas in \\nartificial intelligence. \\n1.4. Paper Structure \\nThis paper will trace the evolution of object detection methods, beginning with a brief look at pre-\\ndeep learning techniques. We will then delve into the two primary families of deep learning'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='detectors: two-stage and single-stage. We will discuss their core components, modern architectures \\nincluding Transformers, and conclude with evaluation metrics, applications, and future challenges. \\n \\n2. Background and Foundational Concepts \\n2.1. Traditional Computer Vision Approaches \\nBefore deep learning, object detection relied on hand-crafted features. Methods like the Viola-Jones \\nframework (famous for real-time face detection) used simple Haar-like features and a cascade of \\nclassifiers. Other approaches used more complex feature descriptors like HOG (Histogram of \\nOriented Gradients), often paired with a classifier like a Support Vector Machine (SVM), to \\nidentify objects. These methods were effective for specific tasks but were brittle and did not \\ngeneralize well. \\n2.2. The Sliding Window Method \\nA common technique was the sliding window approach. A window of a fixed size would be slid \\nacross all possible locations and scales of an image. For each window, a feature descriptor would be \\ncomputed and fed to a classifier. This method was computationally exhaustive and prone to errors. \\n2.3. The Deep Learning Revolution \\nThe success of AlexNet in the 2012 ImageNet classification challenge marked a turning point. \\nResearchers quickly realized that the rich, hierarchical features learned automatically by \\nConvolutional Neural Networks (CNNs) were far more powerful than any hand-crafted features. \\nThis discovery paved the way for the modern era of object detection. \\n \\n3. Two-Stage Object Detectors: A Focus on Accuracy \\nTwo-stage detectors break the object detection problem into two distinct steps, a paradigm that \\ngenerally leads to higher accuracy at the cost of speed. \\n3.1. The \"Propose, then Classify\" Paradigm'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content=\"The core idea is to first generate a sparse set of region proposals—areas of the image that are likely \\nto contain an object. In the second stage, a classifier is run only on these proposed regions to \\ndetermine the object's class and refine the bounding box. \\n3.2. R-CNN: Regions with CNN Features \\nR-CNN was the first major breakthrough in applying deep learning to this paradigm. However, its \\nprocess was slow and cumbersome: \\n1. Generate ~2000 region proposals using an external algorithm like Selective Search. \\n2. Warp/resize each proposed region to a fixed size. \\n3. Pass each warped region independently through a pre-trained CNN to extract features. \\n4. Use a set of SVMs to classify the object in each region. \\n3.3. Fast R-CNN: Sharing Computation \\nFast R-CNN made a significant improvement. Instead of running the CNN 2000 times, it passes the \\nentire image through the CNN just once to generate a feature map. The region proposals are then \\nprojected onto this feature map. A novel RoI (Region of Interest) Pooling layer extracts a fixed-size \\nfeature vector from each proposed region, which is then fed into a classifier. This shared computation \\nmade the process much faster. \\n3.4. Faster R-CNN: The Region Proposal Network (RPN) \\nThe bottleneck in Fast R-CNN was the external Selective Search algorithm for proposing regions. \\nFaster R-CNN introduced the Region Proposal Network (RPN), a small neural network that learns \\nto generate high-quality region proposals directly from the CNN features. By integrating the RPN, \\nFaster R-CNN became the first truly end-to-end, unified deep learning object detector, setting a new \\nstandard for accuracy. \\n \\n4. Single-Stage Object Detectors: A Focus on Speed \\nSingle-stage detectors remove the region proposal step and instead perform localization and \\nclassification in a single forward pass of the network, making them extremely fast and suitable for \\nreal-time applications.\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='4.1. The \"Single Pass\" Paradigm \\nThese models treat object detection as a regression problem. They look at the image once and \\ndirectly predict a set of bounding boxes and their corresponding class probabilities. \\n4.2. YOLO: You Only Look Once \\nThe YOLO family of models is renowned for its speed. YOLO divides the input image into a grid. \\nFor each grid cell, the model simultaneously predicts: \\n• Several bounding boxes. \\n• A \"confidence\" score for each box, indicating how likely it is to contain an object. \\n• Class probabilities for the object within the box. This unified architecture allows for end-to-\\nend training and blazingly fast inference speeds, making it ideal for video processing. \\n4.3. SSD: Single Shot MultiBox Detector \\nSSD aimed to find a middle ground between the speed of YOLO and the accuracy of Faster R-CNN. \\nIts key innovation is using feature maps from multiple layers of the backbone network to make \\npredictions. By making predictions at different scales, SSD is much better at detecting objects of \\nvarious sizes, particularly small ones, compared to the original YOLO. \\n \\n5. Key Architectural Components and Innovations \\nModern detectors, whether two-stage or single-stage, share several common components. \\n5.1. Backbone Networks: The Feature Extractors \\nThe backbone is a deep CNN (like ResNet, VGG, or MobileNet) pre-trained on a large image \\nclassification dataset (e.g., ImageNet). Its role is to act as a powerful feature extractor, converting the \\nraw pixel data of an image into rich, hierarchical feature maps that can be used for detection. \\n5.2. Anchor Boxes: The Priors for Prediction \\nInstead of predicting bounding boxes from scratch, most detectors predict offsets relative to a set of \\npre-defined default boxes called anchor boxes. These anchors have various sizes and aspect ratios'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='and are tiled across the image at different locations. Using anchors reframes the problem from \\npredicting absolute coordinates to refining a well-placed prior, which makes learning easier for the \\nnetwork. \\n5.3. Non-Maximum Suppression (NMS): Pruning Redundant Detections \\nA detector will often output multiple, highly overlapping bounding boxes for the same object. NMS \\nis a crucial post-processing step that cleans up these redundant detections. It sorts all boxes by their \\nconfidence scores, keeps the box with the highest score, and suppresses (discards) any other boxes \\nthat have a high overlap with it. \\n \\n6. Modern Architectures and the Rise of Transformers \\n6.1. Balancing Speed and Accuracy: EfficientDet \\nThe EfficientDet family of models introduced a systematic way to scale detectors for different \\nresource constraints. It uses a highly efficient backbone (EfficientNet) and a novel feature fusion \\nmechanism (BiFPN) to achieve state-of-the-art efficiency, balancing high accuracy with low \\ncomputational cost. \\n6.2. DETR: End-to-End Object Detection with Transformers \\nDETR (DEtection TRansformer) represents a major paradigm shift. It completely eliminates the need \\nfor hand-crafted components like anchor boxes and NMS. DETR uses a standard Transformer \\nencoder-decoder architecture, similar to those used in NLP. It treats object detection as a direct set \\nprediction problem: the model ingests image features and directly outputs the final set of unique \\nobject detections. This simplifies the detection pipeline significantly and has opened up a new and \\nexciting research direction. \\n \\n7. Evaluation Metrics and Datasets \\n7.1. Intersection over Union (IoU)'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='IoU is the fundamental metric used to measure the \"correctness\" of a predicted bounding box. It is \\ncalculated as the area of overlap between the predicted box and the ground-truth box, divided by the \\narea of their union. A detection is typically considered a \"true positive\" if its IoU with a ground-truth \\nbox is above a certain threshold (e.g., 0.5). \\n7.2. Average Precision (AP) and mean Average Precision (mAP) \\nAverage Precision (AP) is the primary metric for evaluating the performance of a detector on a \\nsingle object class. It is calculated from the precision-recall curve and effectively measures the \\ndetector\\'s accuracy across all confidence levels. Mean Average Precision (mAP) is the average of \\nthe AP values across all object classes and is the standard metric for comparing different object \\ndetection models. \\n7.3. Landmark Datasets \\nThe field has been driven by large-scale, publicly available datasets, most notably PASCAL VOC \\nand Microsoft COCO (Common Objects in Context). The COCO dataset, with its large number of \\nobject categories and instances per image, is the current benchmark for modern object detectors. \\n \\n8. Applications and Real-World Use Cases \\nObject detection is a deployed and impactful technology across numerous industries. \\n• Autonomous Vehicles: Detecting cars, pedestrians, cyclists, and traffic signals is essential \\nfor safe navigation. \\n• Medical Imaging: Assisting radiologists by automatically locating tumors, lesions, or other \\nanomalies in X-rays, CT scans, and MRIs. \\n• Retail: Powering cashier-less stores, monitoring shelf inventory, and analyzing customer foot \\ntraffic. \\n• Security and Surveillance: Automatically detecting intruders, unattended baggage, or \\nmonitoring crowd density. \\n \\n9. Challenges and Future Directions'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Despite immense progress, several challenges remain. \\n• Detecting Small and Occluded Objects: Models still struggle to reliably detect objects that \\nare very small, far away, or partially hidden. \\n• The Speed vs. Accuracy Trade-off: While models are becoming more efficient, the \\nfundamental trade-off between real-time speed and maximum accuracy remains a key design \\nconsideration. \\n• Domain Adaptation and Generalization: A model trained on daytime, sunny weather data \\nmay fail when deployed at night or in the rain. Improving robustness to new environments is \\na major challenge. \\n• Few-Shot and Zero-Shot Detection: Training models to detect new object categories with \\nvery few (or zero) labeled examples is an active and important area of research.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content=\"10. Conclusion \\nObject detection has undergone a remarkable transformation, moving from slow, brittle systems \\nbased on hand-crafted features to highly accurate and efficient end-to-end deep learning models. The \\nevolution from the methodical two-stage R-CNN family to the rapid single-stage YOLO and SSD \\ndetectors, and now to the elegant, anchor-free Transformer-based models like DETR, showcases the \\nfield's rapid pace of innovation. As a core enabling technology for machine perception, object \\ndetection continues to solve critical real-world problems and will undoubtedly remain a central focus \\nof AI research for years to come. \\n \\n11. References \\n[Viola & Jones, 2001] Viola, P., & Jones, M. (2001). Rapid object detection using a boosted cascade \\nof simple features. Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision \\nand Pattern Recognition. \\n[Girshick et al., 2014] Girshick, R., Donahue, J., Darrell, T., & Malik, J. (2014). Rich feature \\nhierarchies for accurate object detection and semantic segmentation. Proceedings of the IEEE \\nconference on computer vision and pattern recognition. \\n[Ren et al., 2015] Ren, S., He, K., Girshick, R., & Sun, J. (2015). Faster R-CNN: Towards real-time \\nobject detection with region proposal networks. Advances in neural information processing systems. \\n[Redmon et al., 2016] Redmon, J., Divvala, S., Girshick, R., & Farhadi, A. (2016). You only look \\nonce: Unified, real-time object detection. Proceedings of the IEEE conference on computer vision \\nand pattern recognition. \\n[Liu et al., 2016] Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C. Y., & Berg, A. C. \\n(2016). SSD: Single shot multibox detector. European conference on computer vision. \\n[Carion et al., 2020] Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., & Zagoruyko, S. \\n(2020). End-to-end object detection with transformers. European conference on computer vision.\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'page': 10, 'page_label': '11', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content=''),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:54:40+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:54:40+01:00', 'source': '..\\\\data\\\\pdf\\\\proposal.pdf', 'total_pages': 8, 'page': 0, 'page_label': '1', 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='Title: Self-Training with Uncertainty-Aware Style Transfer for Cross-Domain \\nObject Detection \\nPrincipal Investigator: [Your Name] Affiliation: [Your Institution/Research Group] Date: October \\n15, 2025 \\n \\nAbstract \\nModern object detection models achieve remarkable performance but suffer a significant drop in \\naccuracy when deployed in environments (target domains) that differ from their training data (source \\ndomain). This problem of domain shift is a major obstacle to the real-world application of \\ntechnologies like autonomous driving, where a vehicle must operate reliably in diverse weather, \\nlighting, and geographic conditions. This proposal outlines a research project to develop a novel \\nframework for unsupervised domain adaptation in object detection. We propose a method that \\ncombines generative style transfer with a robust self-training mechanism. Specifically, we will use a \\nCycle-Consistent Generative Adversarial Network (CycleGAN) to translate images between \\ndomains, artificially augmenting the training data. More importantly, we will enhance a self-training \\npipeline by incorporating uncertainty estimation. By using techniques like Monte Carlo Dropout, \\nour model will only leverage pseudo-labels from the target domain in which it has high confidence, \\npreventing the accumulation of errors from incorrect labels. We hypothesize that this uncertainty-\\naware approach will make the self-training process more stable and effective, leading to a significant \\nimprovement in object detection performance in unseen target domains. The proposed research will \\nbe evaluated on benchmark datasets like Cityscapes and Foggy Cityscapes, with the goal of creating \\nmore robust and reliable perception systems. \\nKeywords: Research Proposal, Object Detection, Domain Adaptation, Self-Training, Uncertainty \\nEstimation, Style Transfer, Autonomous Vehicles.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:54:40+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:54:40+01:00', 'source': '..\\\\data\\\\pdf\\\\proposal.pdf', 'total_pages': 8, 'page': 1, 'page_label': '2', 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='Table of Contents \\n1. Introduction and Problem Statement 1.1. The Success and Brittleness of Modern Detectors \\n1.2. The Challenge of Domain Shift 1.3. Research Questions and Objectives 1.4. Proposed \\nContribution \\n2. Literature Review 2.1. State-of-the-Art Object Detection Models 2.2. Unsupervised Domain \\nAdaptation (UDA) 2.3. UDA Techniques in Object Detection 2.3.1. Adversarial Training \\nMethods 2.3.2. Style Transfer and Image-to-Image Translation 2.3.3. Self-Training and \\nPseudo-Labeling \\n3. Proposed Methodology 3.1. Overall Framework Architecture 3.2. Module 1: Cross-Domain \\nStyle Transfer 3.3. Module 2: Self-Training with Pseudo-Labeling 3.4. The Core Innovation: \\nUncertainty-Aware Label Filtering \\n4. Experimental Setup and Evaluation 4.1. Datasets and Benchmarks 4.2. Baseline Models \\nfor Comparison 4.3. Evaluation Metrics 4.4. Implementation Details \\n5. Expected Results and Broader Impact 5.1. Hypothesized Performance Gains 5.2. Impact \\non Autonomous Systems and Robotics 5.3. Contribution to the Field of AI \\n6. Plan of Work and Timeline 6.1. Phase 1: Literature Review and Environment Setup \\n(Months 1-2) 6.2. Phase 2: Implementation of Core Modules (Months 3-6) 6.3. Phase 3: \\nExperimentation and Analysis (Months 7-10) 6.4. Phase 4: Dissemination of Results (Months \\n11-12) \\n7. Ethical Considerations \\n8. Conclusion \\n9. References'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:54:40+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:54:40+01:00', 'source': '..\\\\data\\\\pdf\\\\proposal.pdf', 'total_pages': 8, 'page': 2, 'page_label': '3', 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='1. Introduction and Problem Statement \\n1.1. The Success and Brittleness of Modern Detectors \\nDeep learning-based object detectors like Faster R-CNN and YOLO have become incredibly \\naccurate, forming the perception backbone for many emerging technologies. However, their success \\nis predicated on the assumption that the training and testing data are drawn from the same statistical \\ndistribution. \\n1.2. The Challenge of Domain Shift \\nIn the real world, this assumption is frequently violated. A model trained exclusively on clear, sunny \\nday driving data may fail catastrophically when deployed at night, in the rain, or in a city with \\ndifferent architecture. This phenomenon is known as domain shift. Manually annotating data for \\nevery possible domain is prohibitively expensive and unscalable. Therefore, Unsupervised Domain \\nAdaptation (UDA), which aims to adapt a model trained on a labeled source domain to an unlabeled \\ntarget domain, is a critical area of research. \\n1.3. Research Questions and Objectives \\nThis research aims to answer the following question: How can we make an object detector robust to \\ndomain shift without requiring any labeled data from the new domain? Our primary objectives are: \\n1. To design a framework that leverages both image-level style translation and model-level self-\\ntraining. \\n2. To develop a novel mechanism to filter noisy pseudo-labels generated during self-training by \\nestimating model uncertainty. \\n3. To empirically validate the proposed framework and demonstrate its superiority over existing \\nUDA methods. \\n1.4. Proposed Contribution \\nThe main contribution of this work will be an uncertainty-aware self-training framework. While \\nself-training is a common technique, it is often unstable because the model can reinforce its own \\nmistakes by trusting incorrect \"pseudo-labels.\" By introducing a principled mechanism for the model'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:54:40+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:54:40+01:00', 'source': '..\\\\data\\\\pdf\\\\proposal.pdf', 'total_pages': 8, 'page': 3, 'page_label': '4', 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='to gauge its own uncertainty, we can select only the most reliable pseudo-labels, leading to more \\nstable and effective adaptation. \\n \\n2. Literature Review \\n2.1. State-of-the-Art Object Detection Models \\nOur work will build upon established object detection architectures. We will consider both a two-\\nstage detector (e.g., Faster R-CNN) and a Transformer-based detector (e.g., DETR) as the base \\nmodels for our adaptation framework. \\n2.2. Unsupervised Domain Adaptation (UDA) \\nUDA is a well-established field in machine learning. The central goal is to leverage a label-rich \\nsource domain to learn a task in a label-scarce target domain. \\n2.3. UDA Techniques in Object Detection \\n• Adversarial Training: These methods use a \"domain discriminator\" network that tries to \\ndistinguish between features from the source and target domains. The main network is then \\ntrained to produce features that can \"fool\" this discriminator, thereby learning domain-\\ninvariant features. \\n• Style Transfer: Generative models like GANs are used to translate images from the source \\nstyle to the target style (e.g., making a sunny image look foggy). This creates a synthetic \\nlabeled dataset in the target style. \\n• Self-Training: This involves using an initial model to make predictions on the unlabeled \\ntarget data. The most confident predictions are then treated as \"pseudo-labels\" and are used to \\nretrain the model. This approach is powerful but risks error accumulation if the pseudo-labels \\nare noisy. Our proposed work directly addresses this key limitation. \\n \\n3. Proposed Methodology \\n3.1. Overall Framework Architecture'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:54:40+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:54:40+01:00', 'source': '..\\\\data\\\\pdf\\\\proposal.pdf', 'total_pages': 8, 'page': 4, 'page_label': '5', 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='The proposed system will consist of three interconnected modules operating on a base object \\ndetector. The model will be trained on labeled source data (e.g., sunny images) and unlabeled target \\ndata (e.g., rainy images). \\n3.2. Module 1: Cross-Domain Style Transfer \\nWe will first train a CycleGAN model to learn the mappings between the source and target domains. \\nThis will allow us to translate a source image into a \"fake\" target image (e.g., sunny -> rainy) and \\nvice-versa. This provides a basic form of data augmentation, allowing the detector to see labeled \\nimages that look like they are from the target domain. \\n3.3. Module 2: Self-Training with Pseudo-Labeling \\nIn parallel, we will use the model trained on the source data to generate predictions (bounding boxes \\nand classes) for the unlabeled target domain images. These predictions will serve as initial pseudo-\\nlabels. \\n3.4. The Core Innovation: Uncertainty-Aware Label Filtering \\nThis is the central component of our proposal. Instead of naively trusting all pseudo-labels above a \\nsimple confidence threshold, we will estimate the model\\'s uncertainty for each prediction. We will \\nuse Monte Carlo Dropout, a technique where dropout is applied at inference time over multiple \\nforward passes. The variance in the resulting predictions serves as a strong indicator of model \\nuncertainty. We will then filter the pseudo-labels using a combined score of confidence and low \\nuncertainty. Only the most certain and confident pseudo-labels will be added to a replay buffer used \\nto fine-tune the detector, making the adaptation process robust to noise. \\n \\n4. Experimental Setup and Evaluation \\n4.1. Datasets and Benchmarks \\nWe will focus on autonomous driving scenarios. The primary experiment will be adapting from the \\nCityscapes dataset (clear weather) to the Foggy Cityscapes dataset. We will also evaluate on other \\ncommon shifts, such as adapting from synthetic data (Sim10k) to real-world data (KITTI).'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:54:40+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:54:40+01:00', 'source': '..\\\\data\\\\pdf\\\\proposal.pdf', 'total_pages': 8, 'page': 5, 'page_label': '6', 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='4.2. Baseline Models for Comparison \\nWe will compare our method against three baselines: \\n1. A \"Lower Bound\" model trained only on source data. \\n2. A state-of-the-art adversarial training method for UDA. \\n3. A standard self-training method without uncertainty awareness. \\n4.3. Evaluation Metrics \\nPerformance will be measured using the standard object detection metric, mean Average Precision \\n(mAP), calculated on the labeled validation set of the target domain. \\n \\n5. Expected Results and Broader Impact \\n5.1. Hypothesized Performance Gains \\nWe expect our uncertainty-aware framework to significantly outperform the baselines. We \\nhypothesize that by reducing the noise in the pseudo-labeling process, our model will adapt more \\neffectively, resulting in a 5-10% absolute improvement in mAP on the target domain compared to \\nstandard self-training methods. \\n5.2. Impact on Autonomous Systems and Robotics \\nA more robust perception system directly translates to increased safety and reliability for \\nautonomous vehicles, drones, and industrial robots. This research could help bridge the gap between \\ndevelopment and real-world deployment of these technologies. \\n \\n6. Plan of Work and Timeline \\nThe project is planned for a 12-month period. \\n• Phase 1 (Months 1-2): In-depth literature review; setting up the computational environment \\nand baseline models.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:54:40+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:54:40+01:00', 'source': '..\\\\data\\\\pdf\\\\proposal.pdf', 'total_pages': 8, 'page': 6, 'page_label': '7', 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='• Phase 2 (Months 3-6): Implementation of the style transfer module and the uncertainty-\\naware self-training loop. \\n• Phase 3 (Months 7-10): Conducting extensive experiments on benchmark datasets, \\nanalyzing results, and performing ablation studies. \\n• Phase 4 (Months 11-12): Writing a research paper for submission to a top-tier computer \\nvision conference (e.g., CVPR, ICCV) and finalizing the project report. \\n \\n7. Ethical Considerations \\nThe primary application of this research is to enhance safety in autonomous systems. However, \\nobject detection technology can also be used for surveillance. Our research will be conducted \\ntransparently, and we will focus our evaluation on publicly available datasets related to driving. We \\nwill not use private or personally identifiable data. The code and models will be made publicly \\navailable to ensure reproducibility and encourage positive use. \\n \\n8. Conclusion \\nThis research proposal addresses the critical problem of domain shift in object detection. By \\nproposing a novel framework that integrates style transfer with a more robust, uncertainty-aware \\nself-training mechanism, we aim to significantly advance the state of the art in unsupervised domain \\nadaptation. The successful completion of this project will produce more reliable perception models, \\nthereby accelerating the safe and responsible deployment of AI in real-world applications. \\n \\n9. References \\n[Hoffman et al., 2018] Hoffman, J., Tzeng, E., Park, T., Zhu, J. Y., Isola, P., Saenko, K., ... & \\nDarrell, T. (2018). CyCADA: Cycle-consistent adversarial domain adaptation. International \\nconference on machine learning.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:54:40+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:54:40+01:00', 'source': '..\\\\data\\\\pdf\\\\proposal.pdf', 'total_pages': 8, 'page': 7, 'page_label': '8', 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='[Saito et al., 2017] Saito, K., Watanabe, K., Ushiku, Y., & Harada, T. (2017). Asymmetric tri-\\ntraining for unsupervised domain adaptation. International conference on machine learning. \\n[Gal & Ghahramani, 2016] Gal, Y., & Ghahramani, Z. (2016). Dropout as a Bayesian approximation: \\nRepresenting model uncertainty in deep learning. International conference on machine learning. \\n[Zou et al., 2018] Zou, Y., Yu, Z., Kumar, B., & Wang, J. (2018). Unsupervised domain adaptation \\nfor semantic segmentation via class-balanced self-training. European conference on computer vision.')]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pdf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "277a53f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Text splitting get into chunks\n",
    "\n",
    "def split_documents(documents,chunk_size=1000,chunk_overlap=200):\n",
    "    \"\"\"Split documents into smaller chunks for better RAG performance\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    split_docs = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(split_docs)} chunks\")\n",
    "    \n",
    "    # Show example of a chunk\n",
    "    if split_docs:\n",
    "        print(f\"\\nExample chunk:\")\n",
    "        print(f\"Content: {split_docs[0].page_content[:200]}...\")\n",
    "        print(f\"Metadata: {split_docs[0].metadata}\")\n",
    "    \n",
    "    return split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5f64bde5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 68 documents into 218 chunks\n",
      "\n",
      "Example chunk:\n",
      "Content: Attention Mechanism in Neural Networks:\n",
      "Where it Comes and Where it Goes\n",
      "Derya Soydaner\n",
      "Received: 22 July 2021 / Accepted: 27 April 2022\n",
      "Abstract A long time ago in the machine learning literature, th...\n",
      "Metadata: {'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 0, 'page_label': '1', 'source_file': 'attention.pdf', 'file_type': 'pdf'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 0, 'page_label': '1', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Attention Mechanism in Neural Networks:\\nWhere it Comes and Where it Goes\\nDerya Soydaner\\nReceived: 22 July 2021 / Accepted: 27 April 2022\\nAbstract A long time ago in the machine learning literature, the idea of\\nincorporating a mechanism inspired by the human visual system into neural\\nnetworks was introduced. This idea is named the attention mechanism, and it\\nhas gone through a long development period. Today, many works have been\\ndevoted to this idea in a variety of tasks. Remarkable performance has re-\\ncently been demonstrated. The goal of this paper is to provide an overview\\nfrom the early work on searching for ways to implement attention idea with\\nneural networks until the recent trends. This review emphasizes the impor-\\ntant milestones during this progress regarding diﬀerent tasks. By this way,\\nthis study aims to provide a road map for researchers to explore the current\\ndevelopment and get inspired for novel approaches beyond the attention.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 0, 'page_label': '1', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='this study aims to provide a road map for researchers to explore the current\\ndevelopment and get inspired for novel approaches beyond the attention.\\nKeywords Attention mechanism ·Neural networks ·Deep learning ·Survey\\n1 Introduction\\nHuman eye sees the world in an interesting way. We suppose as if we see the\\nentire scene at once, but this is an illusion created by the subconscious part\\nof our brain [1]. According to the Scanpath theory [2,3], when the human eye\\nlooks at an image, it can see only a small patch in high resolution. This small\\npatch is called thefovea. It can see the rest of the image in low resolution which\\nis called the periphery. To recognize the entire scene, the eye performs feature\\nextraction based on the fovea. The eye is moved to diﬀerent parts of the image\\nuntil the information obtained from the fovea is suﬃcient for recognition [4].\\nThese eye movements are called saccades. The eye makes successive ﬁxations\\nDerya Soydaner'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 0, 'page_label': '1', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='until the information obtained from the fovea is suﬃcient for recognition [4].\\nThese eye movements are called saccades. The eye makes successive ﬁxations\\nDerya Soydaner\\nDepartment of Brain and Cognition, University of Leuven (KU Leuven), Leuven, Belgium\\nTel.: +32-16710471\\nE-mail: derya.soydaner@kuleuven.be\\narXiv:2204.13154v1  [cs.LG]  27 Apr 2022'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='2 Derya Soydaner\\nuntil the recognition task is complete. This sequential process happens so\\nquickly that we feel as if it happens all at once.\\nBiologically, this is called visual attention system . Visual attention is de-\\nﬁned as the ability to dynamically restrict processing to a subset of the visual\\nﬁeld [5]. It seeks answers for two main questions: What and where to look?\\nVisual attention has been extensively studied in psychology and neuroscience;\\nfor reviews see [6,7,8,9,10]. Besides, there is a large amount of literature on\\nmodeling eye movements [11,12,13,14]. These studies have been a source of\\ninspiration for many artiﬁcial intelligence tasks. It has been discovered that\\nthe attention idea is useful from image recognition to machine translation.\\nTherefore, diﬀerent types of attention mechanisms inspired from the human\\nvisual system have been developed for years. Since the success of deep neural\\nnetworks has been at the forefront for these artiﬁcial intelligence tasks, these'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='visual system have been developed for years. Since the success of deep neural\\nnetworks has been at the forefront for these artiﬁcial intelligence tasks, these\\nmechanisms have been integrated into neural networks for a long time.\\nThis survey is about the journey of attention mechanisms used with neu-\\nral networks. Researchers have been investigating ways to strengthen neural\\nnetwork architectures with attention mechanisms for many years. The pri-\\nmary aim of these studies is to reduce computational burden and to improve\\nthe model performance as well. Previous work reviewed the attention mecha-\\nnisms from diﬀerent perspectives [15], or examined them in context of natural\\nlanguage processing (NLP) [16,17]. However, in this study, we examine the\\ndevelopment of attention mechanisms over the years, and recent trends. We\\nbegin with the ﬁrst attempts to integrate the visual attention idea to neural\\nnetworks, and continue until the most modern neural networks armed with at-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='begin with the ﬁrst attempts to integrate the visual attention idea to neural\\nnetworks, and continue until the most modern neural networks armed with at-\\ntention mechanisms. One of them is the Transformer, which is used for many\\nstudies including the GPT-3 language model [18], goes beyond convolutions\\nand recurrence by replacing them with only attention layers [19]. Finally, we\\ndiscuss how much more can we move forward, and what’s next?\\n2 From the Late 1980s to Early 2010s: The Attention Awakens\\nThe ﬁrst attempts at adapting attention mechanisms to neural networks go\\nback to the late 1980s. One of the early studies is the improved version of\\nthe Neocognitron [20] with selective attention [21]. This study is then mod-\\niﬁed to recognize and segment connected characters in cursive handwriting\\n[22]. Another study describes VISIT, a novel model that concentrates on its\\nrelationship to a number of visual areas of the brain [5]. Also, a novel archi-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='[22]. Another study describes VISIT, a novel model that concentrates on its\\nrelationship to a number of visual areas of the brain [5]. Also, a novel archi-\\ntecture named Signal Channelling Attentional Network (SCAN) is presented\\nfor attentional scanning [23].\\nEarly work on improving the attention idea for neural networks includes\\na variety of tasks such as target detection [24]. In another study, a visual at-\\ntention system extracts regions of interest by combining the bottom-up and\\ntop-down information from the image [25]. A recognition model based on se-\\nlective attention which analyses only a small part of the image at each step,\\nand combines results in time is described [4]. Besides, a model based on the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 2, 'page_label': '3', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Attention Mechanism in Neural Networks: 3\\nconcept of selective tuning is proposed [26]. As the years go by, several studies\\nthat use the attention idea in diﬀerent ways have been presented for visual\\nperception and recognition [27,28,29,30].\\nBy the 2000s, the studies on making attention mechanisms more useful for\\nneural networks continued. In the early years, a model that integrates an at-\\ntentional orienting where pathway and an object recognition what pathway is\\npresented [31]. A computational model of human eye movements is proposed\\nfor an object class detection task [32]. A serial model is presented for visual pat-\\ntern recognition gathering Markov models and neural networks with selective\\nattention on the handwritten digit recognition and face recognition problems\\n[33]. In that study, a neural network analyses image parts and generates pos-\\nterior probabilities as observations to the Markov model. Also, attention idea'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 2, 'page_label': '3', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='[33]. In that study, a neural network analyses image parts and generates pos-\\nterior probabilities as observations to the Markov model. Also, attention idea\\nis used for object recognition [34], and the analysis of a scene [35]. An inter-\\nesting study proposes to learn sequential attention in real-world visual object\\nrecognition using a Q-learner [36]. Besides, a computational model of visual\\nselective attention is described to automatically detect the most relevant parts\\nof a color picture displayed on a television screen [37]. The attention idea is\\nalso used for identifying and tracking objects in multi-resolution digital video\\nof partially cluttered environments [38].\\nIn 2010, the ﬁrst implemented system inspired by the fovea of human retina\\nwas presented for image classiﬁcation [39]. This system jointly trains a re-\\nstricted Boltzmann machine (RBM) and an attentional component called the\\nﬁxation controller. Similarly, a novel attentional model is implemented for si-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 2, 'page_label': '3', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='stricted Boltzmann machine (RBM) and an attentional component called the\\nﬁxation controller. Similarly, a novel attentional model is implemented for si-\\nmultaneous object tracking and recognition that is driven by gaze data [40].\\nBy taking advantage of reinforcement learning, a novel recurrent neural net-\\nwork (RNN) is described for image classiﬁcation [41]. Deep Attention Selective\\nNetwork (DasNet), a deep neural network with feedback connections that are\\nlearned through reinforcement learning to direct selective attention to certain\\nfeatures extracted from images, is presented [42]. Additionally, a deep learning\\nbased framework using attention has been proposed for generative modeling\\n[43].\\n3 2015: The Rise of Attention\\nIt can be said that 2015 is the golden year of attention mechanisms. Because\\nthe number of attention studies has grown like an avalanche after three main\\nstudies presented in that year. The ﬁrst one proposed a novel approach for'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 2, 'page_label': '3', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='the number of attention studies has grown like an avalanche after three main\\nstudies presented in that year. The ﬁrst one proposed a novel approach for\\nneural machine translation (NMT) [44]. As it is known, most of the NMT\\nmodels belong to a family of encoder-decoders [45,46], with an encoder and a\\ndecoder for each language. However, compressing all the necessary information\\nof a source sentence into a ﬁxed-length vector is an important disadvantage of\\nthis encoder-decoder approach. This usually makes it diﬃcult for the neural\\nnetwork to capture all the semantic details of a very long sentence [1].\\nThe idea that [44] introduced is an extension to the conventional NMT\\nmodels. This extension is composed of an encoder and decoder as shown in'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 3, 'page_label': '4', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='4 Derya Soydaner\\nFig. 1 The extension to the conventional NMT models that is proposed by [44]. It generates\\nthe t-th target word yt given a source sentence ( x1,x2,...,x T ).\\nFig 1. The ﬁrst part, encoder, is a bidirectional RNN (BiRNN) [47] that takes\\nword vectors as input. The forward and backward states of BiRNN are com-\\nputed. Then, an annotation aj for each word xj is obtained by concatenating\\nthese forward and backward hidden states. Thus, the encoder maps the input\\nsentence to a sequence of annotations ( a1,...,a Tx ). By using a BiRNN rather\\nthan conventional RNN, the annotation of each word can summarize both\\nthe preceding words and the following words. Besides, the annotation aj can\\nfocus on the words around xj because of the inherent nature of RNNs that\\nrepresenting recent inputs better.\\nIn decoder, a weight αij of each annotation aj is obtained by using its\\nassociated energy eij that is computed by a feedforward neural network f as'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 3, 'page_label': '4', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='representing recent inputs better.\\nIn decoder, a weight αij of each annotation aj is obtained by using its\\nassociated energy eij that is computed by a feedforward neural network f as\\nin Eq. (1). This neural network f is deﬁned as an alignment model that can\\nbe jointly trained with the proposed architecture. In order to reduce compu-\\ntational burden, a multilayer perceptron (MLP) with a single hidden layer is\\nproposed as f. This alignment model tells us about the relation between the\\ninputs around position j and the output at position i. By this way, the decoder\\napplies an attention mechanism. As it is seen in Eq. (2), the αij is the output\\nof softmax function:\\neij = f(hi−1,aj) (1)\\nαij = exp(eij)∑Tx\\nk=1 exp(eik)\\n(2)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 4, 'page_label': '5', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Attention Mechanism in Neural Networks: 5\\nHere, the probability αij determines the importance of annotation aj with\\nrespect to the previous hidden state hi−1. Finally, the context vector ci is\\ncomputed as a weighted sum of these annotations as follows [44]:\\nci =\\nTx∑\\nj=1\\nαijaj (3)\\nBased on the decoder state, the context and the last generated word, the\\ntarget word yt is predicted. In order to generate a word in a translation, the\\nmodel searches for the most relevant information in the source sentence to\\nconcentrate. When it ﬁnds the appropriate source positions, it makes the pre-\\ndiction. By this way, the input sentence is encoded into a sequence of vectors\\nand a subset of these vectors is selected adaptively by the decoder that is rel-\\nevant to predicting the target [44]. Thus, it is no longer necessary to compress\\nall the information of a source sentence into a ﬁxed-length vector.\\nThe second study is the ﬁrst visual attention model in image captioning'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 4, 'page_label': '5', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='all the information of a source sentence into a ﬁxed-length vector.\\nThe second study is the ﬁrst visual attention model in image captioning\\n[48]. Diﬀerent from the previous study [44], it uses a deep convolutional neural\\nnetwork (CNN) as an encoder. This architecture is an extension of the neural\\nnetwork [49] that encodes an image into a compact representation, followed by\\nan RNN that generates a corresponding sentence. Here, the annotation vectors\\nai ∈RD are extracted from a lower convolutional layer, each of which is a D-\\ndimensional representation corresponding to a part of the image. Thus, the\\ndecoder selectively focuses on certain parts of an image by weighting a subset\\nof all the feature vectors [48]. This extended architecture uses attention for\\nsalient features to dynamically come to the forefront instead of compressing\\nthe entire image into a static representation.\\nThe context vector ct represents the relevant part of the input image at'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 4, 'page_label': '5', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='salient features to dynamically come to the forefront instead of compressing\\nthe entire image into a static representation.\\nThe context vector ct represents the relevant part of the input image at\\ntime t. The weight αi of each annotation vector is computed similar to Eq. (2),\\nwhereas its associated energy is computed similar to Eq. (1) by using an MLP\\nconditioned on the previous hidden state ht−1. The remarkable point of this\\nstudy is a new mechanism φ that computes ct from the annotation vectors ai\\ncorresponding to the features extracted at diﬀerent image locations:\\nct = φ(\\n{\\nai\\n}\\n,\\n{\\nαi\\n}\\n) (4)\\nThe deﬁnition of the φ function causes two variants of attention mecha-\\nnisms: The hard (stochastic) attention mechanism is trainable by maximizing\\nan approximate variational lower bound, i.e., by REINFORCE [50]. On the\\nother side, the soft (deterministic) attention mechanism is trainable by stan-\\ndard backpropagation methods. The hard attention deﬁnes a location variable'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 4, 'page_label': '5', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='other side, the soft (deterministic) attention mechanism is trainable by stan-\\ndard backpropagation methods. The hard attention deﬁnes a location variable\\nst, and uses it to decide where to focus attention when generating the t-th\\nword. When the hard attention is applied, the attention locations are con-\\nsidered as intermediate latent variables. It assigns a multinoulli distribution\\nparametrized by αi, and ct becomes a random variable. Here, st,i is deﬁned\\nas a one-hot variable which is set to 1 if the i-th location is used to extract\\nvisual features [48]:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 5, 'page_label': '6', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='6 Derya Soydaner\\np(st,i = 1|sj<t,a) = αt,i (5)\\nct =\\n∑\\ni\\nst,iai (6)\\nWhereas learning hard attention requires sampling the attention location\\nst each time, the soft attention mechanism computes a weighted annotation\\nvector similar to [44] and takes the expectation of the context vectorct directly:\\nEp(st|α)[ct] =\\nL∑\\ni=1\\nαt,iai (7)\\nFurthermore, in training the deterministic version of the model, an alterna-\\ntive method namely doubly stochastic attention, is proposed with an additional\\nconstraint added to the training objective to encourage the model to pay equal\\nattention to all parts of the image.\\nThe third study should be emphasized presents two classes of attention\\nmechanisms for NMT: the global attention that always attends to all source\\nwords, and the local attention that only looks at a subset of source words at\\na time [51]. These mechanisms derive the context vector ct in diﬀerent ways:\\nWhereas the global attention considers all the hidden states of the encoder, the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 5, 'page_label': '6', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='a time [51]. These mechanisms derive the context vector ct in diﬀerent ways:\\nWhereas the global attention considers all the hidden states of the encoder, the\\nlocal one selectively focuses on a small window of context. In global attention, a\\nvariable-length alignment vector is derived similar to Eq. (2). Here, the current\\ntarget hidden state ht is compared with each source hidden state ¯hs by using a\\nscore function instead of the associated energy eij. Thus, the alignment vector\\nwhose size equals the number of time steps on the source side is derived.\\nGiven the alignment vector as weights, the context vector ct is computed as\\nthe weighted average over all the source hidden states. Here, score is referred\\nas a content-based function, and three diﬀerent alternatives are considered [51].\\nOn the other side, the local attention is diﬀerentiable. Firstly, an aligned\\nposition pt is generated for each target word at a time t. Then, a window'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 5, 'page_label': '6', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='On the other side, the local attention is diﬀerentiable. Firstly, an aligned\\nposition pt is generated for each target word at a time t. Then, a window\\ncentered around the source position pt is used to compute the context vector\\nas a weighted average of the source hidden states within the window. The\\nlocal attention selectively focuses on a small window of context, and obtains\\nthe alignment vector from the current target state ht and the source states ¯hs\\nin the window [51].\\nThe introduction of these novel mechanisms in 2015 triggered the rise of\\nattention for neural networks. Based on the proposed attention mechanisms,\\nsigniﬁcant research has been conducted in a variety of tasks. In order to imag-\\nine the attention idea in neural networks better, two visual examples are shown\\nin Fig. 2. A neural image caption generation task is seen in the top row that\\nimplements an attention mechanism [48]. Then, the second example shows'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 5, 'page_label': '6', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='in Fig. 2. A neural image caption generation task is seen in the top row that\\nimplements an attention mechanism [48]. Then, the second example shows\\nhow the attention mechanisms can be used for visual question answering [52].\\nBoth examples demonstrate how attention mechanisms focus on parts of input\\nimages.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 6, 'page_label': '7', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Attention Mechanism in Neural Networks: 7\\nFig. 2 Examples of the attention mechanism in visual. (Top) Attending to the correct\\nobject in neural image caption generation [48]. (Bottom) Visualization of original image\\nand question pairs, and co-attention maps namely word-level, phrase-level and question-\\nlevel, respectively [52].\\n4 2015-2016: Attack of the Attention\\nDuring two years from 2015, the attention mechanisms were used for diﬀerent\\ntasks, and novel neural network architectures were presented applying these\\nmechanisms. After the memory networks [53] that require a supervision signal\\ninstructing them how to use their memory cells, the introduction of the neural\\nTuring machine [54] allows end-to-end training without this supervision signal,\\nvia the use of a content-based soft attention mechanism [1]. Then, end-to-end\\nmemory network [55] that is a form of memory network based on a recurrent\\nattention mechanism is proposed.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 6, 'page_label': '7', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='via the use of a content-based soft attention mechanism [1]. Then, end-to-end\\nmemory network [55] that is a form of memory network based on a recurrent\\nattention mechanism is proposed.\\nIn these years, an attention mechanism called self-attention, sometimes\\ncalled intra-attention, was successfully implemented within a neural network\\narchitecture namely Long Short-Term Memory-Networks (LSTMN) [56]. It\\nmodiﬁes the standard LSTM structure by replacing the memory cell with a\\nmemory network [53]. This is because memory networks have a set of key\\nvectors and a set of value vectors, whereas LSTMs maintain a hidden vector\\nand a memory vector [56]. In contrast to attention idea in [44], memory and\\nattention are added within a sequence encoder in LSTMN. In order to compute\\na representation of a sequence, self-attention is described as relating diﬀerent\\npositions of it [19]. One of the ﬁrst approaches of self-attention is applied for\\nnatural language inference [57].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 6, 'page_label': '7', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='a representation of a sequence, self-attention is described as relating diﬀerent\\npositions of it [19]. One of the ﬁrst approaches of self-attention is applied for\\nnatural language inference [57].\\nMany attention-based models have been proposed for neural image cap-\\ntioning [58], abstractive sentence summarization [59], speech recognition [60,\\n61], automatic video captioning [62], neural machine translation [63], and rec-\\nognizing textual entailment [64]. Diﬀerent attention-based models perform vi-\\nsual question answering [65,66,67]. An attention-based CNN is presented for\\nmodeling sentence pairs [68]. A recurrent soft attention based model learns to\\nfocus selectively on parts of the video frames and classiﬁes videos [69].\\nOn the other side, several neural network architectures have been pre-\\nsented in a variety of tasks. For instance, Stacked Attention Network (SAN)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 7, 'page_label': '8', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='8 Derya Soydaner\\nis described for image question answering [70]. Deep Attention Recurrent Q-\\nNetwork (DARQN) integrates soft and hard attention mechanisms into the\\nstructure of Deep Q-Network (DQN) [71]. Wake-Sleep Recurrent Attention\\nModel (WS-RAM) speeds up the training time for image classiﬁcation and\\ncaption generation tasks [72]. alignDRAW model, an extension of the Deep\\nRecurrent Attention Writer (DRAW) [73], is a generative model of images\\nfrom captions using a soft attention mechanism [74]. Generative Adversarial\\nWhat-Where Network (GAWWN) synthesizes images given instructions de-\\nscribing what content to draw in which location [75].\\n5 The Transformer: Return of the Attention\\nAfter the proposed attention mechanisms in 2015, researchers published stud-\\nies that mostly modifying or implementing them to diﬀerent tasks. However,\\nin 2017, a novel neural network architecture, namely the Transformer, based\\nentirely on self-attention was presented [19]. The Transformer achieved great'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 7, 'page_label': '8', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='in 2017, a novel neural network architecture, namely the Transformer, based\\nentirely on self-attention was presented [19]. The Transformer achieved great\\nresults on two machine translation tasks in addition to English constituency\\nparsing. The most impressive point about this architecture is that it contains\\nneither recurrence nor convolution. The Transformer performs well by replac-\\ning the conventional recurrent layers in encoder-decoder architecture used for\\nNMT with self-attention.\\nThe Transformer is composed of encoder-decoder stacks each of which has\\nsix identical layers within itself. In Fig. 3, one encoder-decoder stack is shown\\nto illustrate the model [19]. Each stack includes only attention mechanisms\\nand feedforward neural networks. As this architecture does not include any\\nrecurrent or convolutional layer, information about the relative or absolute\\npositions in the input sequence is given at the beginning of both encoder and\\ndecoder using positional encodings.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 7, 'page_label': '8', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='recurrent or convolutional layer, information about the relative or absolute\\npositions in the input sequence is given at the beginning of both encoder and\\ndecoder using positional encodings.\\nThe calculations of self-attention are slightly diﬀerent from the mechanisms\\ndescribed so far in this paper. It uses three vectors namely query, key and\\nvalue for each word. These vectors are computed by multiplying the input with\\nweight matrices Wq, Wk and Wv which are learned during training. In general,\\neach value is weighted by a function of the query with the corresponding key.\\nThe output is computed as a weighted sum of the values. Based on this idea,\\ntwo attention mechanisms are proposed: In the ﬁrst one, called scaled dot-\\nproduct attention, the dot products of the query with all keys are computed\\nas given in the right side of Fig. 3. Each result is divided to the square root of\\nthe dimension of the keys to have more stable gradients. They pass into the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 7, 'page_label': '8', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='as given in the right side of Fig. 3. Each result is divided to the square root of\\nthe dimension of the keys to have more stable gradients. They pass into the\\nsoftmax function, thus the weights for the values are obtained. Finally each\\nsoftmax score is multiplied with the value as given in Eq. (8). The authors\\npropose computing the attention on a set of queries simultaneously by taking\\nqueries and keys of dimension dk, and values of dimension dv as inputs. The\\nkeys, queries and values are packed together into matricesK, Q and V. Finally,\\nthe output matrix is obtained as follows [19]:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 8, 'page_label': '9', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Attention Mechanism in Neural Networks: 9\\nFig. 3 The Transformer architecture and the attention mechanisms it uses in detail [19].\\n(Left) The Transformer with one encoder-decoder stack. (Center) Multi-head attention.\\n(Right) Scaled dot-product attention.\\nAttention(Q,K,V ) = softmax(QKT\\n√dk\\n)V (8)\\nThis calculation is performed by every word against the other words. This\\nleads to having values of each word relative to each other. For instance, if\\nthe word x2 is not relevant for the word x1, then the softmax score gives low\\nprobability scores. As a result, the corresponding value is decreased. This leads\\nto an increase in the value of relevant words, and those of others decrease. In\\nthe end, every word obtains a new value for itself.\\nAs seen from Fig. 3, the Transformer model does not directly use scaled\\ndot-product attention. But the attention mechanism it uses is based on these\\ncalculations. The second mechanism proposed, called the multi-head attention,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 8, 'page_label': '9', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='dot-product attention. But the attention mechanism it uses is based on these\\ncalculations. The second mechanism proposed, called the multi-head attention,\\nlinearly projects the queries, keys and values h times with diﬀerent, learned\\nlinear projections to dq, dk and dv dimensions, respectively [19]. The attention\\nfunction is performed in parallel on each of these projected versions of queries,\\nkeys and values, i.e., heads. By this way, dv-dimensional output values are\\nobtained. In order to get the ﬁnal values, they are concatenated and projected\\none last time as shown in the center of Fig. 3. By this way, the self-attention is\\ncalculated multiple times using diﬀerent sets of query, key and value vectors.\\nThus, the model can jointly attend to information at diﬀerent positions [19]:\\nMultiHead(Q,K,V ) = Concat(head1,...,head h)WO (9)\\nwherehead i = Attention(QWQ\\ni ,KW K\\ni ,VW V\\ni )\\nIn the decoder part of the Transformer, masked multi-head attention is'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 8, 'page_label': '9', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='MultiHead(Q,K,V ) = Concat(head1,...,head h)WO (9)\\nwherehead i = Attention(QWQ\\ni ,KW K\\ni ,VW V\\ni )\\nIn the decoder part of the Transformer, masked multi-head attention is\\napplied ﬁrst to ensure that only previous word embeddings are used when\\ntrying to predict the next word in the sentence. Therefore, the embeddings\\nthat shouldn’t be seen by the decoder are masked by multiplying with zero.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 9, 'page_label': '10', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='10 Derya Soydaner\\nAn interesting study examines the contribution made by individual atten-\\ntion heads in the encoder [76]. Also, there is an evaluation of the eﬀects of\\nself-attention on gradient propagation in recurrent networks [77]. For a deeper\\nanalysis of multi-head self-attention mechanism from a theoretical perspective\\nsee [78].\\nSelf-attention has been used successfully in a variety of tasks including\\nsentence embedding [79] and abstractive summarization [80]. It is shown that\\nself-attention can lead to improvements to discriminative constituency parser\\n[81], and speech recognition as well [82,83]. Also, the listen-attend-spell model\\n[84] has been improved with the self-attention for acoustic modeling [85].\\nAs soon as these self-attention mechanisms were proposed, they have been\\nincorporated with deep neural networks for a wide range of tasks. For instance,\\na deep learning model learned a number of large-scale tasks from multiple do-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 9, 'page_label': '10', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='incorporated with deep neural networks for a wide range of tasks. For instance,\\na deep learning model learned a number of large-scale tasks from multiple do-\\nmains with the aid of self-attention mechanism [86]. Novel self-attention neural\\nmodels are proposed for cross-target stance classiﬁcation [87] and NMT [88].\\nAnother study points out that a fully self-attentional model can reach com-\\npetitive predictive performance on ImageNet classiﬁcation and COCO object\\ndetection tasks [89]. Besides, developing novel attention mechanisms has been\\ncarried out such as area attention, a novel mechanism that can be used along\\nmulti-head attention [90]. It attends to areas in the memory by deﬁning the\\nkey of an area as the mean vector of the key of each item, and deﬁning the\\nvalue as the sum of all value vectors in the area.\\nWhen a novel mechanism is proposed, it is inevitable to incorporate it\\ninto the GAN framework [91]. Self-Attention Generative Adversarial Networks'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 9, 'page_label': '10', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='value as the sum of all value vectors in the area.\\nWhen a novel mechanism is proposed, it is inevitable to incorporate it\\ninto the GAN framework [91]. Self-Attention Generative Adversarial Networks\\n(SAGANs) [92] introduce a self-attention mechanism into convolutional GANs.\\nDiﬀerent from the traditional convolutional GANs, SAGAN generates high-\\nresolution details using cues from all feature locations. Similarly, Attentional\\nGenerative Adversarial Network (AttnGAN) is presented for text to image\\ngeneration [93]. On the other side, a machine reading and question answering\\narchitecture called QANet [94] is proposed without any recurrent networks. It\\nuses self-attention to learn the global interaction between each pair of words\\nwhereas convolution captures the local structure of the text. In another study,\\nGated Attention Network (GaAN) controls the importance of each attention\\nhead’s output by introducing gates [95]. Another interesting study introduces'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 9, 'page_label': '10', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Gated Attention Network (GaAN) controls the importance of each attention\\nhead’s output by introducing gates [95]. Another interesting study introduces\\nattentive group convolutions with a generalization of visual self-attention [96].\\nA deep transformer model is implemented for language modeling over long\\nsequences [97].\\n5.1 Self-attention variants\\nIn recent years, self-attention has become an important research direction\\nwithin the deep learning community. Self-attention idea has been examined\\nin diﬀerent aspects. For example, self-attention is handled in a multi-instance\\nlearning framework [98]. The idea of Sparse Adaptive Connection (SAC) is\\npresented for accelerating and structuring self-attention [99]. The research on'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 10, 'page_label': '11', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Attention Mechanism in Neural Networks: 11\\nTable 1 Summary of Notation\\nSymbol Deﬁnition\\na annotation\\nc context vector\\nα weight\\ne energy\\nf feedforward neural network\\nh hidden state\\nφ hard (stochastic) / soft (deterministic) attention\\ns location variable\\np source position\\nK, Q, V keys, queries and values matrices, respectively\\nWq, Wk, Wv weight matrices for queries, keys and values, respectively\\nimproving self-attention continues as well [100,101,102]. Besides, based on the\\nself-attention mechanisms proposed in the Transformer, important studies that\\nmodify the self-attention have been presented. Some of the most recent and\\nprominent studies are summarized below.\\nRelation-aware self-attention It extends the self-attention mechanism by\\nregarding representations of the relative positions, or distances between se-\\nquence elements [103]. Thus, it can consider the pairwise relationships between\\ninput elements. This type of attention mechanism deﬁnes vectors to represent'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 10, 'page_label': '11', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='quence elements [103]. Thus, it can consider the pairwise relationships between\\ninput elements. This type of attention mechanism deﬁnes vectors to represent\\nthe edge between two inputs. It provides learning two distinct edge represen-\\ntations that can be shared across attention heads without requiring additional\\nlinear transformations.\\nDirectional self-attention (DiSA) A novel neural network architecture\\nfor learning sentence embedding named Directional Self-Attention Network\\n(DiSAN) [104] uses directional self-attention followed by a multi-dimensional\\nattention mechanism. Instead of computing a single importance score for each\\nword based on the word embedding, multi-dimensional attention computes a\\nfeature-wise score vector for each token. To extend this mechanism to the self-\\nattention, two variants are presented: The ﬁrst one, called multi-dimensional\\n‘token2token’ self-attention generates context-aware coding for each element.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 10, 'page_label': '11', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='attention, two variants are presented: The ﬁrst one, called multi-dimensional\\n‘token2token’ self-attention generates context-aware coding for each element.\\nThe second one, called multi-dimensional ‘source2token’ self-attention com-\\npresses the sequence into a vector [104]. On the other side, directional self-\\nattention produces context-aware representations with temporal information\\nencoded by using positional masks. By this way, directional information is en-\\ncoded. First, the input sequence is transformed to a sequence of hidden states\\nby a fully connected layer. Then, multi-dimensional token2token self-attention\\nis applied to these hidden states. Hence, context-aware vector representations\\nare generated for all elements from the input sequence.\\nReinforced self-attention (ReSA)A sentence-encoding model named Re-\\ninforced Self-Attention Network (ReSAN)uses reinforced self-attention (ReSA)\\nthat integrates soft and hard attention mechanisms into a single model. ReSA'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 11, 'page_label': '12', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='12 Derya Soydaner\\nselects a subset of head tokens, and relates each head token to a small sub-\\nset of dependent tokens to generate their context-aware representations [105].\\nFor this purpose, a novel hard attention mechanism called reinforced sequence\\nsampling (RSS) , which selects tokens from an input sequence in parallel and\\ntrained via policy gradient, is proposed. Given an input sequence, RSS gener-\\nates an equal-length sequence of binary random variables that indicates both\\nthe selected and discarded ones. On the other side, the soft attention provides\\nreward signals back for training the hard attention. The proposed RSS pro-\\nvides a sparse mask to self-attention. ReSA uses two RSS modules to extract\\nthe sparse dependencies between each pair of selected tokens.\\nOuter product attention (OPA)Self-Attentive Associative Memory (SAM)\\nis a novel operator based upon outer product attention (OPA) [106]. This at-\\ntention mechanism is an extension of dot-product attention [19]. OPA diﬀers'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 11, 'page_label': '12', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='is a novel operator based upon outer product attention (OPA) [106]. This at-\\ntention mechanism is an extension of dot-product attention [19]. OPA diﬀers\\nusing element-wise multiplication, outer product, and tanh function instead of\\nsoftmax.\\nBidirectional block self-attention (Bi-BloSA)Another mechanism, bidi-\\nrectional block self-attention (Bi-BloSA) which is simply a masked block self-\\nattention (mBloSA) with forward and backward masks to encode the tempo-\\nral order information is presented [107]. Here, mBloSA is composed of three\\nparts from its bottom to top namely intra-block self-attention , inter-block\\nself-attention and the context fusion . It splits a sequence into several length-\\nequal blocks, and applies an intra-block self-attention to each block indepen-\\ndently. Then, inter-block self-attention processes the outputs for all blocks.\\nThis stacked self-attention model results a reduction in the amount of mem-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 11, 'page_label': '12', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='dently. Then, inter-block self-attention processes the outputs for all blocks.\\nThis stacked self-attention model results a reduction in the amount of mem-\\nory compared to a single one applied to the whole sequence. Finally, a feature\\nfusion gate combines the outputs of intra-block and inter-block self-attention\\nwith the original input, to produce the ﬁnal context-aware representations of\\nall tokens.\\nFixed multi-head attentionThe ﬁxed multi-head attention proposes ﬁxing\\nthe head size of the Transformer in the aim of improving the representation\\npower [108]. This study emphasizes its importance by setting the head size of\\nattention units to input sequence length.\\nSparse sinkhorn attentionIt is based on the idea of diﬀerentiable sorting\\nof internal representations within the self-attention module [109]. Instead of\\nallowing tokens to only attend to tokens within the same block, it operates\\non block sorted sequences. Each token attends to tokens in the sorted block.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 11, 'page_label': '12', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='allowing tokens to only attend to tokens within the same block, it operates\\non block sorted sequences. Each token attends to tokens in the sorted block.\\nThus, tokens that may be far apart in the unsorted sequence can be considered.\\nAdditionally, a variant of this mechanism named SortCut sinkhorn attention\\napplies a post-sorting truncation of the input sequence.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 12, 'page_label': '13', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Attention Mechanism in Neural Networks: 13\\nAdaptive attention spanAdaptive attention span is proposed as an alter-\\nnative to self-attention [110]. It learns the attention span of each head inde-\\npendently. To this end, a masking function inspired by [111] is used to control\\nthe attention span for each head. The purpose of this novel mechanism is to\\nreduce the computational burden of the Transformer. Additionally, dynamic\\nattention span approach is presented to dynamically change the attention span\\nbased on the current input as an extension [51,112].\\n5.2 Transformer variants\\nDiﬀerent from developing novel self-attention mechanisms, several studies have\\nbeen published in the aim of improving the performance of the Transformer.\\nThese studies mostly modify the model architecture. For instance, an addi-\\ntional recurrence encoder is preferred to model recurrence for Transformer di-\\nrectly [113]. In another study, a new weight initialization scheme is applied to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 12, 'page_label': '13', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='tional recurrence encoder is preferred to model recurrence for Transformer di-\\nrectly [113]. In another study, a new weight initialization scheme is applied to\\nimprove Transformer optimization [114]. A novel positional encoding scheme\\nis used to extend the Transformer to tree-structured data [115]. Investigating\\nmodel size by handling Transformer width and depth for eﬃcient training is\\nalso an active research area [116]. Transformer is used in reinforcement learn-\\ning settings [117,118,119] and for time series forecasting in adversarial training\\nsetting [120].\\nBesides, many Transformer variants have been presented in the recent past.\\nCOMmonsEnse Transformer (COMET) is introduced for automatic construc-\\ntion of commonsense knowledge bases [121]. Evolved Transformer applies neu-\\nral architecture search for a better Transformer model [122]. Transformer Au-\\ntoencoder is a sequential autoencoder for conditional music generation [123].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 12, 'page_label': '13', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='ral architecture search for a better Transformer model [122]. Transformer Au-\\ntoencoder is a sequential autoencoder for conditional music generation [123].\\nCrossTransformer takes a small number of labeled images and an unlabeled\\nquery, and computes distances between spatially-corresponding features to in-\\nfer class membership [124]. DEtection TRansformer (DETR) is a new design\\nfor object detection systems [125], and Deformable DETR is an improved ver-\\nsion that achieves better performance in less time [126]. FLOw-bAsed Trans-\\nformER (FLOATER) emphasizes the importance of position encoding in the\\nTransformer, and models the position information via a continuous dynamical\\nmodel [127]. Disentangled Context (DisCo) Transformer simultaneously gener-\\nates all tokens given diﬀerent contexts by predicting every word in a sentence\\nconditioned on an arbitrary subset of the rest of the words [128]. Genera-\\ntive Adversarial Transformer (GANsformer) is presented for visual generative'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 12, 'page_label': '13', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='conditioned on an arbitrary subset of the rest of the words [128]. Genera-\\ntive Adversarial Transformer (GANsformer) is presented for visual generative\\nmodeling [129].\\nRecent work has demonstrated signiﬁcant performance on NLP tasks. In\\nOpenAI GPT, there is a left-to-right architecture, where every token can only\\nattend to previous tokens in the self-attention layers of the Transformer [130].\\nGPT-2 [131] and GPT-3 [18] models have improved the progress. In addition\\nto these variants, some prominent Transformer-based models are summarized\\nbelow.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 13, 'page_label': '14', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='14 Derya Soydaner\\nUniversal Transformer A generalization of the Transformer model named\\nthe Universal Transformer [132] iteratively computes representations Ht at\\nstep t for all positions in the sequence in parallel. To this end, it uses the\\nscaled dot-product attention in Eq. (8) where d is the number of columns\\nof Q, K and V. In the Universal Transformer, the multi-head self-attention\\nwith k heads is used. The representations Ht is mapped to queries, keys and\\nvalues with aﬃne projections using learned parameter matrices WQ ∈ℜd×d/k,\\nWK ∈ℜd×d/k, WV ∈ℜd×d/k and WO ∈ℜd×d [132]:\\nMultiHead(Ht) = Concat(head1,...,head k)WO (10)\\nwherehead i = Attention(HtWQ\\ni ,HtWK\\ni ,HtWV\\ni )\\nImage TransformerImage Transformer [133] demonstrates that self-attention\\nbased models can also be well-suited for images instead of text. This Trans-\\nformer type restricts the self-attention mechanism to attend to local neigh-\\nborhoods. Thus, the size of images that the model can process is increased.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 13, 'page_label': '14', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='former type restricts the self-attention mechanism to attend to local neigh-\\nborhoods. Thus, the size of images that the model can process is increased.\\nIts larger receptive ﬁelds allow the Image Transformer to signiﬁcantly improve\\nthe model performance on image generation as well as image super-resolution.\\nTransformer-XL This study aims to improve the ﬁxed-length context of the\\nTransformer [19] for language modeling. Transformer-XL [134] makes model-\\ning very long-term dependency possible by reusing the hidden states obtained\\nin previous segments. Hence, information can be propagated through the recur-\\nrent connections. In order to reuse the hidden states without causing temporal\\nconfusion, Transformer-XL uses relative positional encodings. Based on this\\narchitecture, a modiﬁed version named the Gated Transformer-XL (GTrXL)\\nis presented in the reinforcement learning setting [135].\\nTensorized TransformerTensorized Transformer[136] compresses the multi-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 13, 'page_label': '14', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='is presented in the reinforcement learning setting [135].\\nTensorized TransformerTensorized Transformer[136] compresses the multi-\\nhead attention in Transformer. To this end, it uses a novel self-attention model\\nmulti-linear attention with Block-Term Tensor Decomposition (BTD) [137]. It\\nbuilds a single-block attention based on the Tucker decomposition [138]. Then,\\nit uses a multi-linear attention constructed by a BTD to compress the multi-\\nhead attention mechanism. In Tensorized Transformer, the factor matrices are\\nshared across multiple blocks.\\nBERT The Bidirectional Encoder Representations fromTransformers (BERT)\\naims to pre-train deep bidirectional representations from unlabeled text [139].\\nBERT uses a multilayer bidirectional Transformer as the encoder. Besides,\\ninspired by the Cloze task [140], it has a masked language model pre-training\\nobjective. BERT randomly masks some of the tokens from the input, and pre-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 13, 'page_label': '14', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='inspired by the Cloze task [140], it has a masked language model pre-training\\nobjective. BERT randomly masks some of the tokens from the input, and pre-\\ndicts the original vocabulary id of the masked word based only on its context.\\nThis model can pre-train a deep bidirectional Transformer. In all layers, the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 14, 'page_label': '15', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Attention Mechanism in Neural Networks: 15\\npre-training is carried out by jointly conditioning on both left and right con-\\ntext. BERT diﬀers from the left-to-right language model pre-training from this\\naspect.\\nRecently, BERT model has been examined in detail. For instance, the be-\\nhaviour of attention heads are analysed [141]. Various methods have been\\ninvestigated for compressing [142,143], pruning [144], and quantization [145].\\nAlso, BERT model has been considered for diﬀerent tasks such as coreference\\nresolution [146]. A novel method is proposed in order to accelerate BERT\\ntraining [147].\\nFurthermore, various BERT variants have been presented. ALBERT aims\\nto increase the training speed of BERT, and presents two parameter reduction\\ntechniques [148]. Similarly, PoWER-BERT [149] is developed to improve the\\ninference time of BERT. This scheme is also used to accelerate ALBERT. Also,\\nTinyBERT is proposed to accelerate inference and reduce model size while'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 14, 'page_label': '15', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='inference time of BERT. This scheme is also used to accelerate ALBERT. Also,\\nTinyBERT is proposed to accelerate inference and reduce model size while\\nmaintaining accuracy [150]. In order to obtain better representations, Span-\\nBERT is proposed as a pre-training method [151]. As a robustly optimized\\nBERT approach, RoBERTa shows that BERT was signiﬁcantly undertrained\\n[152]. Also, DeBERTa improves RoBERTa using the disentangled attention\\nmechanism [153]. On the other side, DistilBERT shows that it is possible to\\nreach similar performances using much smaller language models pre-trained\\nwith knowledge distillation [154]. StructBERT proposes two novel lineariza-\\ntion strategies [155]. Q-BERT is introduced for quantizing BERT models [156],\\nBioBERT is for biomedical text mining [157], and RareBERT is for rare dis-\\nease diagnosis [158].\\nSince 2017 when the Transformer was presented, research directions have\\ngenerally focused on novel self-attention mechanisms, adapting the Trans-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 14, 'page_label': '15', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='ease diagnosis [158].\\nSince 2017 when the Transformer was presented, research directions have\\ngenerally focused on novel self-attention mechanisms, adapting the Trans-\\nformer for various tasks, or making them more understandable. In one of the\\nmost recent studies, NLP becomes possible in the mobile setting with Lite\\nTransformer. It applies long-short range attention where some heads specialize\\nin the local context modeling while the others specialize in the long-distance\\nrelationship modeling [159]. A deep and light-weight Transformer DeLighT\\n[160] and a hypernetwork-based model namely HyperGrid Transformers [161]\\nperform with fewer parameters. Graph Transformer Network is introduced\\nfor learning node representations on heterogeneous graphs [162] and diﬀerent\\napplications are performed for molecular data [163] or textual graph represen-\\ntation [164]. Also, Transformer-XH applies eXtra Hop attention for structured'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 14, 'page_label': '15', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='applications are performed for molecular data [163] or textual graph represen-\\ntation [164]. Also, Transformer-XH applies eXtra Hop attention for structured\\ntext data [165]. AttentionXML is a tree-based model for extreme multi-label\\ntext classiﬁcation [166]. Besides, attention mechanism is handled in a Bayesian\\nframework [167]. For a better understanding of Transformers, an identiﬁabil-\\nity analysis of self-attention weights is conducted in addition to presenting\\neﬀective attention to improve explanatory interpretations [168]. Lastly,Vision\\nTransformer (ViT) processes an image using a standard Transformer encoder\\nas used in NLP by interpreting it as a sequence of patches, and performs well\\non image classiﬁcation tasks [169].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 15, 'page_label': '16', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='16 Derya Soydaner\\n5.3 What about complexity?\\nAll these aforementioned studies undoubtedly demonstrate signiﬁcant success.\\nBut success not make one great. The Transformer also brings a very high\\ncomputational complexity and memory cost. The necessity of storing atten-\\ntion matrix to compute the gradients with respect to queries, keys and val-\\nues causes a non-negligible quadratic computation and memory requirements.\\nTraining the Transformer is a slow process for very long sequences because\\nof its quadratic complexity. There is also time complexity which is quadratic\\nwith respect to the sequence length. In order to improve the Transformer in\\nthis respect, recent studies have been conducted to improve this issue. One\\nof them is Linear Transformer which expresses the self-attention as a linear\\ndot-product of kernel feature maps [170]. Linear Transformer reduces both\\nmemory and time complexity by changing the self-attention from the softmax'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 15, 'page_label': '16', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='dot-product of kernel feature maps [170]. Linear Transformer reduces both\\nmemory and time complexity by changing the self-attention from the softmax\\nfunction in Eq. (8) to a feature map based dot-product attention. Its per-\\nformance is competitive with the vanilla Transformer architecture on image\\ngeneration and automatic speech recognition tasks while being faster during\\ninference. On the other side, FMMformers which use the idea of thefast multi-\\npole method (FMM) [171] outperform the linear Transformer by decomposing\\nthe attention matrix into near-ﬁeld and far-ﬁeld attention with linear time and\\nmemory complexity [172].\\nAnother suggestion made in response to the Transformer’s quadratic na-\\nture is The Reformer that replaces dot-product attention by one that uses\\nlocality-sensitive hashing [173]. It reduces the complexity but one limitation\\nof the Reformer is its requirement for the queries and keys to be identical. Set'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 15, 'page_label': '16', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='locality-sensitive hashing [173]. It reduces the complexity but one limitation\\nof the Reformer is its requirement for the queries and keys to be identical. Set\\nTransformer aims to reduce computation time of self-attention from quadratic\\nto linear by using an attention mechanism based on sparse Gaussian process\\nliterature [174]. Routing Transformer aims to reduce the overall complexity\\nof attention by learning dynamic sparse attention patterns by using routing\\nattention with clustering [175]. It applies k-means clustering to model sparse\\nattention matrices. At ﬁrst, queries and keys are assigned to clusters. The at-\\ntention scheme is determined by considering only queries and keys from the\\nsame cluster. Thus, queries are routed to keys belonging to the same cluster\\n[175].\\nSparse Transformer introduces sparse factorizations of the attention ma-\\ntrix by using factorized self-attention, and avoids the quadratic growth of com-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 15, 'page_label': '16', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='[175].\\nSparse Transformer introduces sparse factorizations of the attention ma-\\ntrix by using factorized self-attention, and avoids the quadratic growth of com-\\nputational burden [176]. It also shows the possibility of modeling sequences\\nof length one million or more by using self-attention in theory. In the Trans-\\nformer, all the attention heads with the softmax attention assign a non-zero\\nweight to all context words. Adaptively Sparse Transformer replaces softmax\\nwith α-entmax which is a diﬀerentiable generalization of softmax allowing\\nlow-scoring words to receive precisely zero weight [177]. By means of context-\\ndependent sparsity patterns, the attention heads become ﬂexible in the Adap-\\ntively Sparse Transformer. Random feature attention approximates softmax\\nattention with random feature methods [178]. Skyformer replaces softmax\\nwith a Gaussian kernel and adapts Nystr¨ om method [179]. A sparse atten-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 16, 'page_label': '17', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Attention Mechanism in Neural Networks: 17\\ntion mechanism named BIGBIRD aims to reduce the quadratic dependency\\nof Transformer-based models to linear [180]. Diﬀerent from the similar stud-\\nies, BIGBIRD performs well for genomics data alongside NLP tasks such as\\nquestion answering.\\nMusic Transformer [181] shows that self-attention can also be useful for\\nmodeling music. This study emphasizes the infeasibility of the relative po-\\nsition representations introduced by [103] for long sequences because of the\\nquadratic intermediate relative information in the sequence length. Therefore,\\nthis study presents an extended version of relative attention named relative\\nlocal attention that improves the relative attention for longer musical com-\\npositions by reducing its intermediate memory requirement to linear in the\\nsequence length. A softmax-free Transformer (SOFT ) is presented to improve\\nthe computational eﬃciency of ViT. It uses Gaussian kernel function instead'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 16, 'page_label': '17', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='sequence length. A softmax-free Transformer (SOFT ) is presented to improve\\nthe computational eﬃciency of ViT. It uses Gaussian kernel function instead\\nof the dot-product similarity [182].\\nAdditionally, various approaches have been presented in Hierarchical Vi-\\nsual Transformer [183], Long-Short Transformer (Transformer-LS)[184], Per-\\nceiver [185], and Performer [186]. Image Transformer based on the cross-\\ncovariance matrix between keys and queries is applied [187], and a new vi-\\nsion Transformer is proposed [188]. Furthermore, a Bernoulli sampling atten-\\ntion mechanism decreases the quadratic complexity to linear [189]. A novel\\nlinearized attention mechanism performs well on object detection, instance\\nsegmentation, and stereo depth estimation [190]. A study shows that kernel-\\nized attention with relative positional encoding can be calculated using Fast\\nFourier Transform and it leads to get rid of the quadratic complexity for long'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 16, 'page_label': '17', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='ized attention with relative positional encoding can be calculated using Fast\\nFourier Transform and it leads to get rid of the quadratic complexity for long\\nsequences [191]. A linear uniﬁed nested attention mechanism namely Luna\\nuses two nested attention functions to approximate the softmax attention in\\nTransformer to achieve linear time and space complexity [192].\\n6 Concluding Remarks: A New Hope\\nInspired by the human visual system, the attention mechanisms in neural net-\\nworks have been developing for a long time. In this study, we examine this\\nduration beginning with its roots up to the present time. Some mechanisms\\nhave been modiﬁed, or novel mechanisms have emerged in this period. Today,\\nthis journey has reached a very important stage. The idea of incorporating\\nattention mechanisms into deep neural networks has led to state-of-the-art re-\\nsults for a large variety of tasks. Self-attention mechanisms and GPT-n family'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 16, 'page_label': '17', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='attention mechanisms into deep neural networks has led to state-of-the-art re-\\nsults for a large variety of tasks. Self-attention mechanisms and GPT-n family\\nmodels have become a new hope for more advanced models. These promising\\nprogress bring the questions whether the attention could help further devel-\\nopment, replace the popular neural network layers, or could be a better idea\\nthan the existing attention mechanisms? It is still an active research area and\\nmuch to learn we still have, but it is obvious that more powerful systems are\\nawaiting when neural networks and attention mechanisms join forces.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 17, 'page_label': '18', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='18 Derya Soydaner\\nConﬂict of interest\\nThe author declares that she has no conﬂict of interest.\\nReferences\\n1. I. Goodfellow, Y. Bengio, A. Courville, The MIT Press (2016)\\n2. D. Noton, L. Stark, Scientiﬁc American 224(6), 34 (1971)\\n3. D. Noton, L. Stark, Vision Research 11, 929 (1971)\\n4. E. Alpaydın, Advances in Neural Information Processing Systems 8 pp. 771–777 (1995)\\n5. S. Ahmad, Advances in Neural Information Processing Systems 4 pp. 420–427 (1991)\\n6. M. Posner, S. Petersen, Annual Review of Neuroscience 13(1), 25 (1990)\\n7. C. Bundesen, Psychological Review 97(4), 523 (1990)\\n8. R. Desimone, J. Duncan, Annual Review of Neuroscience 18(1), 193 (1995)\\n9. M. Corbetta, G. Shulman, Nature Reviews Neuroscience 3(3), 201 (2002)\\n10. S. Petersen, M. Posner, Annual Review of Neuroscience 35, 73 (2012)\\n11. R. Rimey, C. Brown, Technical Report, University of Rochester (1990)\\n12. B. Sheliga, L. Riggio, G. Rizzolatti, Experimental Brain Research 98(3), 507 (1994)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 17, 'page_label': '18', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='11. R. Rimey, C. Brown, Technical Report, University of Rochester (1990)\\n12. B. Sheliga, L. Riggio, G. Rizzolatti, Experimental Brain Research 98(3), 507 (1994)\\n13. B. Sheliga, L. Riggio, G. Rizzolatti, Experimental Brain Research 105(2), 261 (1995)\\n14. J. Hoﬀman, B. Subramaniam, Perception and Psychophysics 57(6), 787 (1995)\\n15. S. Chaudhari, et al., ACM Transactions on Intelligent Systems and Technology (TIST)\\npp. 1–32 (2021)\\n16. A. Galassi, et al., IEEE Transactions on Neural Networks and Learning Systems (2020)\\n17. J. Lee, et al., ACM Transactions on Knowledge Discovery from Data (TKDD) 13(6),\\n1 (2019)\\n18. T. Brown, et al., Advances in Neural Information Processing Systems 33 pp. 1877–1901\\n(2020)\\n19. A. Vaswani, et al., Advances in Neural Information Processing Systems 30 pp. 5998–\\n6008 (2017)\\n20. K. Fukushima, Biological Cybernetics 36, 193 (1980)\\n21. K. Fukushima, Applied Optics 26(23), 4985 (1987)\\n22. K. Fukushima, T. Imagawa, Neural Networks 6(1), 33 (1993)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 17, 'page_label': '18', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='6008 (2017)\\n20. K. Fukushima, Biological Cybernetics 36, 193 (1980)\\n21. K. Fukushima, Applied Optics 26(23), 4985 (1987)\\n22. K. Fukushima, T. Imagawa, Neural Networks 6(1), 33 (1993)\\n23. E. Postma, H.V. den Herik, P. Hudson, Neural Networks 10(6), 993 (1997)\\n24. J. Schmidhuber, R. Huber, International Journal of Neural Systems pp. 125–134 (1991)\\n25. R. Milanese, et al., IEEE Computer Society Conference on Computer Vision and Pat-\\ntern Recoginition, Seattle, WA, USA pp. 781–785 (1994)\\n26. J. Tsotsos, et al., Artiﬁcial Intelligence 78(1-2), 507 (1995)\\n27. S. Culhane, J. Tsotsos, Proceedings of the 11th IAPR International Conference on\\nPattern Recognition, The Hague, Netherlands pp. 36–40 (1992)\\n28. D. Reisfeld, H. Wolfson, Y. Yeshurun, International Journal of Computer Vision14(2),\\n119 (1995)\\n29. I. Rybak, et al., Vision Research 38(15-16), 2387 (1998)\\n30. J. Keller, et al., Pattern Analysis and Applications 2(3) (1999)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 17, 'page_label': '18', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='119 (1995)\\n29. I. Rybak, et al., Vision Research 38(15-16), 2387 (1998)\\n30. J. Keller, et al., Pattern Analysis and Applications 2(3) (1999)\\n31. F. Miau, L. Itti, Proceedings of the 23rd Annual International Conference of the IEEE\\nEngineering in Medicine and Biology Society, Istanbul, Turkey pp. 789–792 (2001)\\n32. W. Zhang, et al., Advances in Neural Information Processing Systems 19 pp. 1609–1616\\n(2006)\\n33. A. Salah, E. Alpaydın, L. Akarun, IEEE Transactions on Pattern Analysis and Machine\\nIntelligence 24(3), 420 (2002)\\n34. D. Walther, et al., International Workshop on Biologically Motivated Computer Vision,\\nSpringer, Berlin, Heidelberg pp. 472–479 (2002)\\n35. K. Schill, et al., Journal of Electronic Imaging 10(1), 152 (2001)\\n36. L. Paletta, G. Fritz, C. Seifert, International Conference on Machine Learning (2005)\\n37. O.L. Meur, et al., IEEE Transactions on Pattern Analysis and Machine Intelligence\\n28(5), 802– (2006)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 18, 'page_label': '19', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Attention Mechanism in Neural Networks: 19\\n38. S. Gould, et al., International Joint Conference on Artiﬁcial Intelligence (IJCAI) pp.\\n2115–2121 (2007)\\n39. H. Larochelle, G. Hinton, Advances in Neural Information Processing Systems 23 pp.\\n1243–1251 (2010)\\n40. L. Bazzani, et al., International Conference on Machine Learning (2011)\\n41. V. Mnih, et al., Advances in Neural Information Processing Systems 27 pp. 2204–2212\\n(2014)\\n42. M. Stollenga, et al., Advances in Neural Information Processing Systems 27 pp. 3545–\\n3553 (2014)\\n43. Y. Tang, N. Srivastava, R. Salakhutdinov, Advances in Neural Information Processing\\nSystems 27 (2014)\\n44. D. Bahdanau, K. Cho, Y. Bengio, International Conference on Learning Representa-\\ntions (2015)\\n45. I. Sutskever, O. Vinyals, Q. Le, Advances in Neural Information Processing Systems\\n27 pp. 3104–3112 (2014)\\n46. K. Cho, et al., Proceedings of the 2014 Conference on Empirical Methods in Natural\\nLanguage Processing (EMNLP) pp. 1724–1734 (2014)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 18, 'page_label': '19', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='27 pp. 3104–3112 (2014)\\n46. K. Cho, et al., Proceedings of the 2014 Conference on Empirical Methods in Natural\\nLanguage Processing (EMNLP) pp. 1724–1734 (2014)\\n47. M. Schuster, K. Paliwal, IEEE Transactions on Signal Processing 45(11), 2673 (1997)\\n48. K. Xu, et al., International Conference on Machine Learning pp. 2048–2057 (2015)\\n49. O. Vinyals, et al., In Proceedings of the IEEE Conference on Computer Vision and\\nPattern Recognition pp. 3156–3164 (2015)\\n50. R. Williams, Machine Learning 8(3-4), 229 (1992)\\n51. M.T. Luong, H.P..C. Manning, Proceedings of the 2015 Conference on Empirical Meth-\\nods in Natural Language Processing, Lisbon, Portugal pp. 1412–1421 (2015)\\n52. J. Lu, et al., Advances in Neural Information Processing Systems 29 (2016)\\n53. J. Weston, S. Chopra, A. Bordes, International Conference on Learning Representa-\\ntions (2014)\\n54. A. Graves, G. Wayne, I. Danihelka, arXiv preprint arXiv:1410.5401 (2014)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 18, 'page_label': '19', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='53. J. Weston, S. Chopra, A. Bordes, International Conference on Learning Representa-\\ntions (2014)\\n54. A. Graves, G. Wayne, I. Danihelka, arXiv preprint arXiv:1410.5401 (2014)\\n55. S. Sukhbaatar, et al., Advances in Neural Information Processing Systems 28 pp. 2440–\\n2448 (2015)\\n56. J. Cheng, L. Dong, M. Lapata, Proceedings of the 2016 Conference on Empirical\\nMethods in Natural Language Processing pp. 551–561 (2016)\\n57. A. Parikh, et al., Proceedings of the 2016 Conference on Empirical Methods in Natural\\nLanguage Processing, Austin, Texas pp. 2249–2255 (2016)\\n58. Q. You, et al., In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition (CVPR), Las Vegas, NV pp. 4651–4659 (2016)\\n59. A. Rush, S. Chopra, J. Weston, Proceedings of the 2015 Conference on Empirical\\nMethods in Natural Language Processing, Lisbon, Portugal pp. 379–389 (2015)\\n60. D. Yu, et al., Interspeech pp. 17–21 (2016)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 18, 'page_label': '19', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Methods in Natural Language Processing, Lisbon, Portugal pp. 379–389 (2015)\\n60. D. Yu, et al., Interspeech pp. 17–21 (2016)\\n61. J. Chorowski, et al., Advances in Neural Information Processing Systems 28 pp. 577–\\n585 (2015)\\n62. M. Zanﬁr, E. Marinoiu, C. Sminchisescu, In Asian Conference on Computer Vision,\\nSpringer, Cham pp. 104—-119 (2016)\\n63. Y. Cheng, et al., Proceedings of the 25th International Joint Conference on Artiﬁcial\\nIntelligence (2016)\\n64. T. Rockt International Conference on Learning Representations (2016)\\n65. Y. Zhu, et al., Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition pp. 4995–5004 (2016)\\n66. K. Chen, et al., arXiv preprint arXiv:1511.05960 (2015)\\n67. H. Xu, K. Saenko, In European Conference on Computer Vision pp. 451–466 (2016)\\n68. W. Yin, et al., Transactions of the Association for Computational Linguistics 4, 259\\n(2016)\\n69. S. Sharma, R. Kiros, R. Salakhutdinov, International Conference on Learning Repre-\\nsentations (2016)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 18, 'page_label': '19', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='(2016)\\n69. S. Sharma, R. Kiros, R. Salakhutdinov, International Conference on Learning Repre-\\nsentations (2016)\\n70. Z. Yang, et al., In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition pp. 21–29 (2016)\\n71. I. Sorokin, et al., arXiv preprint arXiv:1512.01693 (2015)\\n72. J. Ba, et al., Advances in Neural Information Processing Systems 28 pp. 2593–2601\\n(2015)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 19, 'page_label': '20', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='20 Derya Soydaner\\n73. K. Gregor, et al., International Conference on Machine Learning pp. 1462–1471 (2015)\\n74. E. Mansimov, et al., International Conference on Learning Representations (2016)\\n75. S. Reed, et al., Advances in Neural Information Processing Systems 29 pp. 217–225\\n(2016)\\n76. E. Voita, et al., In Proceedings of the 57th Annual Meeting of the Association for\\nComputational Linguistics, Florence, Italy pp. 5797–5808 (2019)\\n77. G. Kerg, et al., Advances in Neural Information Processing Systems 33 (2020)\\n78. J.B. Cordonnier, A. Loukas, M. Jaggi, International Conference on Learning Repre-\\nsentations (2020)\\n79. Z. Lin, et al., International Conference on Learning Representations (2017)\\n80. R. Paulus, C. Xiong, R. Socher, International Conference on Learning Representations\\n(2018)\\n81. N. Kitaev, D. Klein, In Proceedings of the 56th Annual Meeting of the Association for\\nComputational Linguistics (Long papers) pp. 2676–2686 (2018)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 19, 'page_label': '20', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='(2018)\\n81. N. Kitaev, D. Klein, In Proceedings of the 56th Annual Meeting of the Association for\\nComputational Linguistics (Long papers) pp. 2676–2686 (2018)\\n82. D. Povey, et al., IEEE International Conference on Acoustics, Speech and Signal Pro-\\ncessing (ICASSP), IEEE pp. 5874–5878 (2018)\\n83. A. Vyas, et al., Advances in Neural Information Processing Systems 33 (2020)\\n84. W. Chan, et al., IEEE International Conference on Acoustics, Speech and Signal Pro-\\ncessing (ICASSP), Shanghai pp. 4960—-4964 (2016)\\n85. M. Sperber, et al., In proceedings of Annual Conference of the International Speech\\nCommunication Association (InterSpeech) pp. 3723–3727 (2018)\\n86. L. Kaiser, et al., arXiv preprint arXiv:1706.05137 (2017)\\n87. C. Xu, et al., Proceedings of the 56th Annual Meeting of the Association for Compu-\\ntational Linguistics (Short papers), Melbourne, Australia pp. 778–783 (2018)\\n88. S. Maruf, A. Martins, G. Haﬀari, Proceedings of NAACL-HLT, Minneapolis, Minnesota\\npp. 3092–3102 (2019)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 19, 'page_label': '20', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='tational Linguistics (Short papers), Melbourne, Australia pp. 778–783 (2018)\\n88. S. Maruf, A. Martins, G. Haﬀari, Proceedings of NAACL-HLT, Minneapolis, Minnesota\\npp. 3092–3102 (2019)\\n89. P. Ramachandran, et al., Advances in Neural Information Processing Systems 32 pp.\\n68–80 (2019)\\n90. Y. Li, et al., International Conference on Machine Learning (2019)\\n91. I. Goodfellow, et al., Advances in Neural Information Processing Systems 27 pp. 2672–\\n2680 (2014)\\n92. H. Zhang, et al., International Conference on Machine Learning pp. 7354–7363 (2019)\\n93. T. Xu, et al., In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition (CVPR) pp. 1316–1324 (2018)\\n94. A. Yu, et al., International Conference on Learning Representations (2018)\\n95. J. Zhang, et al., Conference on Uncertainty in Artiﬁcial Intelligence (2018)\\n96. D. Romero, et al., International Conference on Machine Learning (2020)\\n97. R. Al-Rfou, et al., AAAI Conference on Artiﬁcial Intelligence 33, 3159 (2019)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 19, 'page_label': '20', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='96. D. Romero, et al., International Conference on Machine Learning (2020)\\n97. R. Al-Rfou, et al., AAAI Conference on Artiﬁcial Intelligence 33, 3159 (2019)\\n98. J. Du, et al., Proceedings of the 2018 Conference on Empirical Methods in Natural\\nLanguage Processing pp. 2216–2225 (2018)\\n99. X. Li, et al., Advances in Neural Information Processing Systems 33 (2020)\\n100. B. Yang, et al., AAAI Conference on Artiﬁcial Intelligence 33, 387 (2019)\\n101. B. Yang, et al., Proceedings of the 2018 Conference on Empirical Methods in Natural\\nLanguage Processing, Brussels, Belgium pp. 4449–4458 (2018)\\n102. Proceedings of the IEEE International Conference on Computer Vision pp. 3286–3295\\n103. P. Shaw, J. Uszkoreit, A. Vaswani, Proceedings of NAACL-HLT, New Orleans,\\nLouisiana pp. 464–468 (2018)\\n104. T. Shen, et al., AAAI Conference on Artiﬁcial Intelligence pp. 5446–5455 (2018)\\n105. T. Shen, et al., In Proceedings of the 27th International Joint Conference on Artiﬁcial'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 19, 'page_label': '20', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='104. T. Shen, et al., AAAI Conference on Artiﬁcial Intelligence pp. 5446–5455 (2018)\\n105. T. Shen, et al., In Proceedings of the 27th International Joint Conference on Artiﬁcial\\nIntelligence, (IJCAI-18) pp. 4345–4352 (2018)\\n106. H. Le, T. Tran, S. Venkatesh, International Conference on Machine Learning (2020)\\n107. T. Shen, et al., International Conference on Learning Representations (2018)\\n108. S. Bhojanapalli, et al., International Conference on Machine Learning (2020)\\n109. Y. Tay, et al., International Conference on Machine Learning (2020)\\n110. S. Sukhbaatar, et al., Proceedings of the 57th Annual Meeting of the Association for\\nComputational Linguistics, Florence, Italy pp. 331–335 (2019)\\n111. Y. Jernite, et al., International Conference on Learning Representations (2017)\\n112. R. Shu, H. Nakayama, In Proceedings of the First Workshop on Neural Machine Trans-\\nlation, Vancouver, Canada pp. 1–10 (2017)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 20, 'page_label': '21', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Attention Mechanism in Neural Networks: 21\\n113. J. Hao, et al., Proceedings of NAACL-HLT, Minneapolis, Minnesota pp. 1198–1207\\n(2019)\\n114. X. Huang, et al., International Conference on Machine Learning (2020)\\n115. V. Shiv, C. Quirk, Advances in Neural Information Processing Systems 32 pp. 12,081–\\n12,091 (2019)\\n116. Z. Li, et al., International Conference on Machine Learning (2020)\\n117. Y. Hoshen, Advances in Neural Information Processing Systems 30, Long Beach, CA,\\nUSA (2017)\\n118. S. Hu, et al., International Conference on Learning Representations (2021)\\n119. E. Parisotto, R. Salakhutdinov, International Conference on Learning Representations\\n(2021)\\n120. S. Wu, et al., Advances in Neural Information Processing Systems 33 (2020)\\n121. A. Bosselut, et al., Proceedings of the 57th Annual Meeting of the Association for\\nComputational Linguistics (2019)\\n122. D. So, C. Liang, Q. Le, International Conference on Machine Learning (2019)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 20, 'page_label': '21', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Computational Linguistics (2019)\\n122. D. So, C. Liang, Q. Le, International Conference on Machine Learning (2019)\\n123. K. Choi, et al., International Conference on Machine Learning (2020)\\n124. C. Doersch, A. Gupta, A. Zisserman, Advances in Neural Information Processing Sys-\\ntems 33 pp. 21,981–21,993 (2020)\\n125. N. Carion, et al., European Conference on Computer Vision pp. 213—-229 (2020)\\n126. X. Zhu, et al., International Conference on Learning Representations (2021)\\n127. X. Liu, et al., International Conference on Machine Learning pp. 6327–6335 (2020)\\n128. J. Kasai, et al., International Conference on Machine Learning (2020)\\n129. D. Hudson, L. Zitnick, International Conference on Machine Learning pp. 4487–4499\\n(2021)\\n130. A. Radford, et al., Technical Report, OpenAI (2018)\\n131. A. Radford, et al., OpenAI blog p. 9 (2019)\\n132. M. Dehghani, et al., International Conference on Learning Representations (2019)\\n133. N. Parmar, International Conference on Machine Learning (2018)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 20, 'page_label': '21', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='132. M. Dehghani, et al., International Conference on Learning Representations (2019)\\n133. N. Parmar, International Conference on Machine Learning (2018)\\n134. Z. Dai, et al., Proceedings of the 57th Annual Meeting of the Association for Compu-\\ntational Linguistics pp. 2978–2988 (2019)\\n135. E. Parisotto, International Conference on Machine Learning (2020)\\n136. X. Ma, et al., Advances in Neural Information Processing Systems 32 pp. 2232–2242\\n(2019)\\n137. L. Lathauwer, SIAM Journal on Matrix Analysis and Applications 30(3), 1033 (2008)\\n138. L. Tucker, Psychometrika 31(3), 279 (1966)\\n139. J. Devlin, et al., Proceedings of NAACL-HLT 2019 pp. 4171–4186 (2019)\\n140. W. Taylor, Journalism Bulletin 30(4), 415 (1953)\\n141. K. Clark, et al., arXiv preprint arXiv:1906.04341 (2019)\\n142. S. Sun, et al., Proceedings of the 2019 Conference on Empirical Methods in Natural\\nLanguage Processing and the 9th International Joint Conference on Natural Language\\nProcessing, Hong Kong, China pp. 4323–4332 (2019)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 20, 'page_label': '21', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Language Processing and the 9th International Joint Conference on Natural Language\\nProcessing, Hong Kong, China pp. 4323–4332 (2019)\\n143. W. Wang, et al., Advances in Neural Information Processing Systems 33 (2020)\\n144. J. McCarley, R. Chakravarti, A. Sil, arXiv preprint arXiv:1910.06360 (2020)\\n145. O. Zafrir, et al., The 5th Workshop on Energy Eﬃcient Machine Learning and Cogni-\\ntive Computing - NeurIPS (2019)\\n146. M. Joshi, et al., In Proceedings of the 2019 Conference on Empirical Methods in\\nNatural Language Processing and the 9th International Joint Conference on Natural\\nLanguage Processing pp. 5803–5808 (2019)\\n147. L. Gong, et al., International Conference on Machine Learning pp. 2337–2346 (2019)\\n148. Z. Lan, et al., International Conference on Learning Representations (2020)\\n149. S. Goyal, et al., International Conference on Machine Learning (2020)\\n150. X. Jiao, et al., arXiv preprint arXiv:1909.10351 (2019)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 20, 'page_label': '21', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='149. S. Goyal, et al., International Conference on Machine Learning (2020)\\n150. X. Jiao, et al., arXiv preprint arXiv:1909.10351 (2019)\\n151. M. Joshi, et al., Transactions of the Association for Computational Linguistics 8, 64\\n(2020)\\n152. Y. Liu, et al., arXiv preprint arXiv:1907.11692 (2019)\\n153. P. He, et al., International Conference on Learning Representations (2021)\\n154. V. Sanh, et al., the 5th Workshop on Energy Eﬃcient Machine Learning and Cognitive\\nComputing - NeurIPS (2019)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 21, 'page_label': '22', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='22 Derya Soydaner\\n155. W. Wang, et al., International Conference on Learning Representations (2020)\\n156. S. Shen, et al., AAAI Conference on Artiﬁcial Intelligence 34, 8815 (2020)\\n157. J. Lee, et al., Bioinformatics 36(4), 1234 (2020)\\n158. P. Prakash, et al., AAAI Conference on Artiﬁcial Intelligence 35, 453 (2021)\\n159. Z. Wu, et al., International Conference on Learning Representations (2020)\\n160. S. Mehta, et al., International Conference on Learning Representations (2021)\\n161. Y. Tay, et al., International Conference on Learning Representations (2021)\\n162. S. Yun, et al., International Conference on Learning Representations (2018)\\n163. Y. Rong, et al., Advances in Neural Information Processing Systems 33 (2020)\\n164. J. Yang, et al., Advances in Neural Information Processing Systems 34 (2021)\\n165. C. Zhao, et al., International Conference on Learning Representations (2020)\\n166. R. You, et al., Advances in Neural Information Processing Systems 32 (2019)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 21, 'page_label': '22', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='165. C. Zhao, et al., International Conference on Learning Representations (2020)\\n166. R. You, et al., Advances in Neural Information Processing Systems 32 (2019)\\n167. X. Fan, et al., Advances in Neural Information Processing Systems 33 (2020)\\n168. G. Brunner, et al., International Conference on Learning Representations (2020)\\n169. A. Dosovitskiy, et al., International Conference on Learning Representations (2021)\\n170. A. Katharopoulos, et al., International Conference on Machine Learning (2020)\\n171. L. Greengard, V. Rokhlin, Journal of Computational Physics 73(2), 325– (1987)\\n172. T. Nguyen, et al., Advances in Neural Information Processing Systems 34 (2021)\\n173. N. Kitaev, L. Kaiser, A. Levskaya, International Conference on Learning Representa-\\ntions (2020)\\n174. J. Lee, et al., International Conference on Machine Learning pp. 3744–3753 (2019)\\n175. A. Roy, et al., Transactions of the Association for Computational Linguistics pp. 53–68\\n(2020)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 21, 'page_label': '22', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='tions (2020)\\n174. J. Lee, et al., International Conference on Machine Learning pp. 3744–3753 (2019)\\n175. A. Roy, et al., Transactions of the Association for Computational Linguistics pp. 53–68\\n(2020)\\n176. R. Child, et al., arXiv preprint arXiv:1904.10509 (2019)\\n177. G. Correia, V. Niculae, A. Martins, Proceedings of the 2019 Conference on Empirical\\nMethods in Natural Language Processing and the 9th International Joint Conference\\non Natural Language Processing pp. 2174–2184 (2019)\\n178. H. Peng, et al., International Conference on Learning Representations (2021)\\n179. Y. Chen, et al., Advances in Neural Information Processing Systems 34 (2021)\\n180. M. Zaheer, et al., Advances in Neural Information Processing Systems 33 (2020)\\n181. C.Z. Huang, et al., International Conference on Learning Representations (2019)\\n182. J. Lu, et al., Advances in Neural Information Processing Systems 34 (2021)\\n183. Z. Pan, et al., Proceedings of the IEEE/CVF International Conference on Computer'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 21, 'page_label': '22', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='182. J. Lu, et al., Advances in Neural Information Processing Systems 34 (2021)\\n183. Z. Pan, et al., Proceedings of the IEEE/CVF International Conference on Computer\\nVision pp. 377–386 (2021)\\n184. C. Zhu, et al., Advances in Neural Information Processing Systems 34 (2021)\\n185. A. Jaegle, et al., International Conference on Machine Learning pp. 4651–4664 (2021)\\n186. K. Choromanski, et al., International Conference on Learning Representations (2021)\\n187. A. El-Nouby, et al., Advances in Neural Information Processing Systems 34 (2021)\\n188. Q. Yu, et al., Advances in Neural Information Processing Systems 34 (2021)\\n189. Z. Zeng, et al., International Conference on Machine Learning pp. 12,321–12,332 (2021)\\n190. Z. Shen, et al., Proceedings of the IEEE/CVF Winter Conference on Applications of\\nComputer Vision pp. 3531–3539 (2021)\\n191. S. Luo, et al., Advances in Neural Information Processing Systems 34 (2021)\\n192. X. Ma, et al., Advances in Neural Information Processing Systems 34 (2021)'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 0, 'page_label': '1', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nQZhou-Embedding Technical Report\\nPeng Yu, En Xu, Bin Chen, Haibiao Chen, Yinfei Xu\\nKingsoft AI ∗\\nAugust 2025\\nAbstract\\nWe present QZhou-Embedding, a general-purpose contextual text embed-\\nding model with exceptional text representation capabilit ies. Built upon the\\nQwen2.5-7B-Instruct foundation model, we designed a uniﬁe d multi-task frame-\\nwork comprising specialized data transformation and train ing strategies. The\\ndata transformation scheme enables the incorporation of mo re diverse textual\\ntraining datasets, while the task-speciﬁc training strate gies enhance model learn-\\ning eﬃciency. We developed a data synthesis pipeline levera ging LLM API, in-\\ncorporating techniques such as Paraphrasing, Augmentatio n, and Hard negative\\nexample generation to improve the semantic richness and sam ple diﬃculty of\\nthe training set. Additionally, we employ a two-stage train ing strategy, compris-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 0, 'page_label': '1', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='example generation to improve the semantic richness and sam ple diﬃculty of\\nthe training set. Additionally, we employ a two-stage train ing strategy, compris-\\ning initial retrieval-focused pretraining followed by ful l-task ﬁne-tuning, enabling\\nthe embedding model to extend its capabilities based on robu st retrieval perfor-\\nmance. Our model achieves state-of-the-art results on the M TEB and CMTEB\\nbenchmarks, ranking ﬁrst on both leaderboards(August 27, 2 025), simultaneously\\nachieves state-of-the-art performance on tasks including Reranking, Clustering,\\netc. Our ﬁndings demonstrate that higher-quality, more div erse data is crucial for\\nadvancing retrieval model performance, and that leveragin g LLMs’ generative ca-\\npabilities can further optimize data quality for embedding model breakthroughs.\\nOur model weights are released on HuggingFace 1 under Apache 2.0 license. For\\nreproducibility, we provide evaluation code and instructi ons on GitHub 2.\\n1 Introduction'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 0, 'page_label': '1', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Our model weights are released on HuggingFace 1 under Apache 2.0 license. For\\nreproducibility, we provide evaluation code and instructi ons on GitHub 2.\\n1 Introduction\\nText embedding models, which transform natural language text int o mathematical vec-\\ntor representations, play an indispensable role in text mining, quest ion-answering sys-\\ntems, recommendation systems, and retrieval-augmented gener ation. Recently, LLM-\\nbased agent technology has experienced rapid development and wid espread adoption,\\nembedding models, which transform textual or multimodal data into vector represen-\\ntations for knowledge base construction, have signiﬁcantly enhan ced agent systems\\n∗ https://kingsoft.com/\\n1https://huggingface.co/Kingsoft-LLM/QZhou-Embedding\\n2https://github.com/Kingsoft-LLM/QZhou-Embedding\\narXiv:2508.21632v1  [cs.CL]  29 Aug 2025'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 1, 'page_label': '2', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nin terms of real-time performance, long-term memory, data privac y preservation, and\\nknowledge integration capabilities. With the continuous advancemen t of neural net-\\nworks and deep learning, text embeddings have evolved from early s parse representa-\\ntions (e.g., BM25[ 1]) to dense representations based on ﬁne-tuned deep networks s uch\\nas BERT[2] and T5[ 3], leading to signiﬁcant performance improvements[ 4][5][6][7][8]. In\\n2022, the rise of large language models (LLMs), exempliﬁed by ChatG PT[9], ushered in\\na new era of text embeddings based on LLM representations, includ ing models like text-\\nembedding-3-large and RepLLaMA[ 10]. Recent research on optimizing text embedding\\nmodels has explored diverse perspectives and focal points. For ins tance, to address\\nthe limitation of decoder-only architectures—where causal atten tion mechanisms re-\\nstrict token embeddings to unidirectional semantic capture—seve ral approaches have'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 1, 'page_label': '2', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='the limitation of decoder-only architectures—where causal atten tion mechanisms re-\\nstrict token embeddings to unidirectional semantic capture—seve ral approaches have\\nbeen proposed: Echo Embedding[ 11] employs input repetition and instruction design\\nto enable preceding tokens to capture subsequent token semant ics. LLM2Vec[ 12] modi-\\nﬁes attention to bi-directional mechanism to remove backward dep endency constraints.\\nConan-Embedding-v2[13] proposes a novel soft masking mechanism combined with dy-\\nnamic rank reduction. Another widely adopted approach is knowledg e distillation,\\nwhere text embeddings are treated as the ”signal states” repre senting textual seman-\\ntics. By distilling knowledge from high-performing teacher models to s tudent models,\\nthe objective is to optimize the embedding performance. For instan ce, Jasper[ 14] em-\\nploys a multi-stage knowledge distillation framework, combining with mu ltiple carefully'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 1, 'page_label': '2', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='the objective is to optimize the embedding performance. For instan ce, Jasper[ 14] em-\\nploys a multi-stage knowledge distillation framework, combining with mu ltiple carefully\\ndesigned loss functions and ﬁnally achieving superior results. Debat er[16] proposes a\\nstep-by-step thinking mechanism for embedding generation, itera tively optimizing doc-\\nument representations through continuous COT. Distillation is applie d to constrain\\nthe ﬁnal token representation to learn the optimal semantic stat es from these thinking\\nsteps. Additionally, hard negative sampling has emerged as a crucial research direc-\\ntion in text embedding models, serving as a pivotal technique for mod el optimization.\\nANCE[18] identiﬁed that conventional dense retrieval training leads to dimin ishing gra-\\ndient norms during optimization. Thus they developed an asynchron ous Approximate\\nNearest Neighbor (ANN) indexing mechanism that periodically refres hes the negative'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 1, 'page_label': '2', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='dient norms during optimization. Thus they developed an asynchron ous Approximate\\nNearest Neighbor (ANN) indexing mechanism that periodically refres hes the negative\\nsample pool using the current model parameters, thereby ensur ing the maintenance\\nof up-to-date and optimally challenging negative samples. Both Cona n-Embedding[24]\\nand its v2 version incorporated similar dynamic hard negative sampling techniques to\\nenhance model performance. NV-Embed[ 19] implemented an alternative approach by\\nleveraging their previously developed NV-Retriever’s[ 20] positive-aware negative min-\\ning strategy, including TopK-MarginPos and TopKPercPos ﬁltering m echanisms.\\nIn this work, we present QZhou-Embedding, built upon the powerfu l Qwen2.5-7B-\\nInstruct[21] model, which pushes the boundaries of text embedding capabilities. To\\nenhance the model’s semantic understanding, we designed a uniﬁed m ulti-task learn-\\ning framework that not only accommodates more diverse training da ta but also bring'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 1, 'page_label': '2', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='enhance the model’s semantic understanding, we designed a uniﬁed m ulti-task learn-\\ning framework that not only accommodates more diverse training da ta but also bring\\neﬃcient learning across three key tasks: retrieval, natural langu age inference (NLI),\\nand classiﬁcation. Our framework comprises two core components : 1. Data Trans-\\nformation: We carefully adapt data formats to the speciﬁc require ments of retrieval,\\nNLI, and classiﬁcation tasks, enabling eﬀective feature extractio n from heterogeneous\\ndata sources, signiﬁcantly beneﬁting retrieval model training. 2. Training Strategy:\\nWe designed specialized loss functions based on each task’s charact eristics, optimizing\\nmodel training eﬃciency. To further improve the robustness and g eneralization of vec-\\n2'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 2, 'page_label': '3', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\ntor representation, we propose a data synthesis method by emplo ying three techniques\\nto address data scarcity: Paraphrasing & Data augmentation for limited datasets and\\nHard negative generation for negative sample enrichment. Building u pon prior work, we\\ndesigned a strategy named ”Data Grouping Strategy”, enabling ba tch sampling within\\nsingle datasets, inadvertently increasing training diﬃculty through in-batch negative\\nsampling from the same distribution. For model training, we used a tw o-phase train-\\ning approach, through the ﬁrst-stage retrieval training and sec ond-stage full-capability\\ntraining, our model acquires a solid foundation of retrieval capabilit ies, while eﬀectively\\nextending to multiple capability dimensions. Our model achieved state -of-the-art av-\\nerage scores on CMTEB[ 22] and MTEB[ 23] benchmarks, ranking ﬁrst overall on both\\nCMTEB and MTEB leaderboards, demonstrating the eﬀectiveness o f our approach.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 2, 'page_label': '3', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='erage scores on CMTEB[ 22] and MTEB[ 23] benchmarks, ranking ﬁrst overall on both\\nCMTEB and MTEB leaderboards, demonstrating the eﬀectiveness o f our approach.\\nThe contributions of our work are summarized as follows:\\n• We propose a uniﬁed multi-task learning framework that systematic ally coordi-\\nnates both data processing and training pipelines, enhancing divers ity in datasets\\nand eﬃciency in model training ;\\n• We develop advanced data synthesis techniques powered by LLM, in cluding Para-\\nphrasing, Data augmentation, and Hard negative generation. The se methods\\nsigniﬁcantly enhance the quality of training corpora, thereby impro ving model’s\\nrobustness and generalization capabilities;\\n• We emply a two-stage training paradigm: Stage 1 focuses exclusively on retrieval\\ncapability building, establishing strong foundational retrieval perf ormance; and\\nstage 2 implements balanced training with controled retrieval/non-r etrieval task'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 2, 'page_label': '3', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='capability building, establishing strong foundational retrieval perf ormance; and\\nstage 2 implements balanced training with controled retrieval/non-r etrieval task\\nratios, achieving superior performance on classiﬁcation (CLS), pa ir classiﬁcation\\n(PairCLS), and semantic textual similarity (STS) tasks while maintain ing re-\\ntrieval eﬀectiveness;\\n• Our model achieves state-of-the-art performance on both MTE B and CMTEB\\nbenchmarks, which validates the eﬀectiveness of our proposed me thods.\\n2 Related Works\\n2.1 Text Embedding Models\\nText vector representation is a fundamental research area in na tural language processing\\n(NLP) and serves as the cornerstone for language understandin g. Early approaches re-\\nlied on sparse vector representations, such as TF-IDF[\\n25], BM25[26], and LSA[ 27]. With\\nthe advent of pretrained language models, dense contextualized r epresentations based\\non architectures like BERT[ 2] and T5[ 3] became widely studied and applied[ 4][5][6]. In'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 2, 'page_label': '3', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='the advent of pretrained language models, dense contextualized r epresentations based\\non architectures like BERT[ 2] and T5[ 3] became widely studied and applied[ 4][5][6]. In\\nthe era of large language models (LLMs), major advancements hav e led to the devel-\\nopment of LLM-based embedding models, such as text-embedding- 3-small/large (Ope-\\nnAI), E5-Mistral-7B[28], SFR-Embedding-Mistral[29], SFR-Embedding-2R[ 30], GRITLM[31],\\nLLM2Vec[12], RepLLaMA[10], BGE-en-icl[32], NV-Embed[19], gte-Qwen2-7B-Instruct[33],\\nQwen3-Embedding[34], etc. These models beneﬁt from optimized LLM architectures—suc h\\nas RoPE positional encoding[ 35], RMSNorm[ 36], and GeGLU activation[ 37]—combined\\nwith their strong semantic contextualization capabilities acquired th rough large-scale\\n3'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 3, 'page_label': '4', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\npretraining. As a result, LLM-based embeddings achieve superior p erformance in re-\\ntrieval and related tasks.\\n2.2 Embedding Model Training\\nThe mainstream approaches currently involve contrastive learning pretraining on un-\\nsupervised/weakly supervised corpora and supervised contrast ive learning training on\\nhigh-quality labeled positive and negative samples. In unsupervised le arning, early\\nwork like SimCSE[\\n7] proposed feeding continuous inputs of both original and noise-\\naugmented texts while employing contrastive learning to enhance th e model’s dis-\\ncriminative representation capability. For weakly supervised learnin g, gte[ 33] utilized\\nlarge-scale structured data (web search data, title-article pairs , etc.) for pretraining,\\nfollowed by ﬁne-tuning on high-quality open-source retrieval train ing data, achieving\\nperformance comparable to OpenAI embeddings with signiﬁcantly fe wer parameters.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 3, 'page_label': '4', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='followed by ﬁne-tuning on high-quality open-source retrieval train ing data, achieving\\nperformance comparable to OpenAI embeddings with signiﬁcantly fe wer parameters.\\nConan-Embedding[24] and v2 similarly adopted the weakly supervised pretraining &\\nsupervised ﬁne-tuning approach but incorporated techniques like cross-GPU batch loss\\nbalancing, dynamic hard negative mining, and soft masking (v2) to op timize the model.\\nSeed1.6-Embedding[38] employed a phased training strategy combining text and multi-\\nmodal pretraining followed by business-scenario-speciﬁc ﬁne-tun ing, achieving superior\\nrepresentation quality.\\nSubstantial research has also been conducted on modeling diﬀeren t tasks. Piccolo2[\\n39]\\nintroduced multi-task hybrid loss functions for diverse downstrea m tasks, an approach\\nwe also incorporate. SFR-Embedding[ 30] utilized multi-task learning techniques to\\nregularize embeddings, signiﬁcantly enhancing domain data discrimina tion. Xiaobu-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 3, 'page_label': '4', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='we also incorporate. SFR-Embedding[ 30] utilized multi-task learning techniques to\\nregularize embeddings, signiﬁcantly enhancing domain data discrimina tion. Xiaobu-\\nembedding uniﬁed the treatment of major CMTEB problem categorie s from the per-\\nspective of circle loss[ 40], fully leveraging multiple positive examples in original datasets\\nwhile carefully balancing diﬀerent loss weights.\\n2.3 Data Synthesis\\nData quantity and quality are the most critical factors in model opt imization, data\\nsynthesis methods have become a critical research direction due t o the high cost of\\nmanual annotation. Doc2Query[\\n41] and Query2Doc[ 42] employ question-answering\\nmodels to generate pseudo-queries and pseudo-documents resp ectively, enhancing data\\nfor improved RAG performance. Promptagator[ 43] addresses few-shot retrieval sce-\\nnarios by generating queries of diverse intents using few-shot dem onstrations and an-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 3, 'page_label': '4', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='for improved RAG performance. Promptagator[ 43] addresses few-shot retrieval sce-\\nnarios by generating queries of diverse intents using few-shot dem onstrations and an-\\nnotations, eﬀectively improving retrieval capabilities across varyin g intents or distri-\\nbutions. GPL[ 44] utilizes existing T5 encoder-decoder models to generate queries,\\nretrieves similar passages as hard negatives using existing retrieva l models, and em-\\nploys cross-encoders to score each (query, passage) pair. Unn atural Instructions[ 45]\\nleverages prompt and in-context learning (ICL) techniques to gen erate synthetic ex-\\namples through controlled instructions, inputs, and constraints, producing 64k diverse\\ndata entries from several seed examples with promising experiment al results. Qwen3-\\nEmbedding[34] designs a diversiﬁed prompting strategy by assigning document-s peciﬁc\\nroles to simulate potential users querying that document, enabling LLMs to generate'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 3, 'page_label': '4', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Embedding[34] designs a diversiﬁed prompting strategy by assigning document-s peciﬁc\\nroles to simulate potential users querying that document, enabling LLMs to generate\\nstylistically authentic queries that enhance diversity and realism.\\n4'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 4, 'page_label': '5', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\n2.4 Hard Negative Mining Techniques\\nHard negatives serve as essential components in contrastive lear ning for retrieval model\\ntraining. Early work like ANCE[\\n46] proposed an asynchronous ANN indexing mech-\\nanism that periodically updates hard negatives using checkpoint sta tes to maintain\\noptimally challenging samples. Conan-Embedding[ 24] and its v2 version implemented\\na dynamic hard negative sampling strategy by excluding and refresh ing samples when\\ntheir scores fall below a threshold. NV-Retriever[ 47] proposed positive-aware negative\\nmining, introducing TopK-MarginPos and TopKPercPos ﬁltering crite ria to minimize\\nfalse negatives. LGAI-Embedding[ 17] built upon NV-Retriever’s strategy with adap-\\ntive margin-based mining strategies, employing ANNA IR as a teacher retrieval model\\nto identify high-quality hard negatives while using TopKPercPos ﬁlter ing to eliminate\\nfalse negatives.\\n3 Uniﬁed Multi-task Learning Framework'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 4, 'page_label': '5', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='to identify high-quality hard negatives while using TopKPercPos ﬁlter ing to eliminate\\nfalse negatives.\\n3 Uniﬁed Multi-task Learning Framework\\nEmbedding models support numerous downstream tasks including re trieval, reranking,\\nSTS, and classiﬁcation. Given the diversity of these tasks and their associated data\\ncomplexity, we explore a uniﬁed strategy to eﬀectively handle them c ollectively while\\npromoting optimization of the embedding model. Existing research on uniﬁed task pro-\\ncessing includes circle loss[\\n40], which approaches sentence pair similarity from a global\\nperspective by categorizing tasks into class-level labels and pair-w ise labels, Xiaobu-\\nembedding demonstrated signiﬁcant improvements by adopting this approach. Other\\nmodels like Piccolo2[ 39], SFR-Embedding[ 30], NV-Embed[ 47], Conan-Embedding[ 24] ,\\nand Conan-Embedding-v2 have incorporated multi-task learning us ing diverse train-\\ning data with varying label processing methods, some employing task -speciﬁc losses'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 4, 'page_label': '5', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='and Conan-Embedding-v2 have incorporated multi-task learning us ing diverse train-\\ning data with varying label processing methods, some employing task -speciﬁc losses\\n(InfoNCE[48], Cosent[ 49], etc.).\\nOur design principle aims to accommodate more tasks and data types , enabling cross-\\ndomain and cross-task data to eﬀectively enhance embedding capa bilities. We propose\\na uniﬁed multi-task learning framework that categorizes training da ta into three task\\ntypes: retrieval, NLI, and classiﬁcation, with customized data and training solutions\\nfor each, allowing most natural text data to be converted into emb edding training data\\nthrough this framework. The following sections detail the framewo rk’s components and\\nimplementation methods.\\n3.1 Model Architecture\\nEmbedding models based on BERT or T5 [\\n39][15][50][24] exhibit powerful contextual\\nrepresentation capabilities, primarily attributed to their bidirection al attention mech-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 4, 'page_label': '5', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='3.1 Model Architecture\\nEmbedding models based on BERT or T5 [\\n39][15][50][24] exhibit powerful contextual\\nrepresentation capabilities, primarily attributed to their bidirection al attention mech-\\nanisms. However, recent large language models predominantly adop t decoder-only ar-\\nchitectures with unidirectional attention, signiﬁcantly constrainin g tokens’ ability to\\ncapture contextual information. Several studies have address ed this limitation through\\narchitectural modiﬁcations or attention mechanism optimizations[ 12][31][47]. Our work\\nbuilds upon the Qwen2.5-7B-Instruct architecture and checkpoin t due to its exceptional\\nChinese language contextual capabilities. Consequently, we impleme nted the following\\nmodiﬁcations: (1) modifying the original causal attention to bi-dire ctional attention\\n5'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 5, 'page_label': '6', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nFigure 1: QZhou-Embedding Architecture\\nto enable comprehensive context capture, and (2) employing mean pooling with sub-\\nsequent normalization to produce ﬁnal embedding vectors. The mo del architecture is\\nshown in Figure 1\\n3.2 Data Transformation\\n3.2.1 Retrieval-oriented Process\\nWhile open-source datasets such as MS MARCO[\\n64] are readily accessible, they alone\\nare insuﬃcient for further advancing embedding model capabilities, thus we supplement\\nwith data from additional sources, such as news, academic paper a nd QA datasets.\\nGiven the heterogeneous nature of these datasets across doma ins and purposes, we\\ndesign a retrieval-oriented data transformation methodology to c onvert diverse sources\\nand formats into training data suitable for retrieval task. Below we outline selected\\ncategories of training data used for transformation and their pro cessing procedures:\\n• Title-Body/Abstract ”Title-Body/Abstract” type data primarily consists of'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 5, 'page_label': '6', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='categories of training data used for transformation and their pro cessing procedures:\\n• Title-Body/Abstract ”Title-Body/Abstract” type data primarily consists of\\ntitle-body/article pairs typically sourced from online news, articles, documents,\\narXiv publications and Wikipedia. For these data types, the transfo rmation pro-\\ncess involves using the title as the query and the body/abstract as the positive\\nsample. However, since the latter are documents, truncation is ap plied when they\\nexceed the maximum training length.\\n• Claim-Evidence This data type typically presents a claim or statement followed\\nby extracted evidence that either supports or refutes it, commo nly used for multi-\\nhop fact extraction and claim veriﬁcation tasks. Datasets genera lly contain claims\\nand corresponding evidence, with each evidence instance labeled as ”Supports”\\nor ”Refutes”. The transformation process involves: converting the claim portion\\n6'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 6, 'page_label': '7', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\ninto a query sample, for evidence labeled as ”Supports”, the text is treated as a\\npositive sample; for evidence labeled as ”Refutes”, it is converted in to a negative\\nsample.\\n• Question-Answer Question-answering data and conversational Q-A pairs pri-\\nmarily originate from chat platforms and forums. Within the current wave of\\nLLM and reinforcement learning research, such data exhibits rema rkable volume\\nand diversity. Virtually single-turn Q-A datasets(one question pair ed with one\\nanswer) represents the most suitable format for retrieval train ing. For transfor-\\nmation, the ”Question/Query/User” portion is converted into que ries, while the\\n”Answer/Response/Assistant” portion is processed as documen ts.\\n3.2.2 NLI-oriented Process\\nNatural Language Inference (NLI) represents a fundamental capability of NLP models,\\nencompassing tasks such as semantic similarity, textual entailment , and sentiment anal-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 6, 'page_label': '7', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='3.2.2 NLI-oriented Process\\nNatural Language Inference (NLI) represents a fundamental capability of NLP models,\\nencompassing tasks such as semantic similarity, textual entailment , and sentiment anal-\\nysis. This section describes the methodology for transforming and constructing training\\nsets from NLI-style data, using textual semantic similarity (STS) a nd textual entailment\\ntasks as illustrative examples. Our approach distinctively reformula tes NLI tasks into\\ntext\\npair-score formats compatible with Cosent loss[ 49] training strategy, where sample\\npairs are quantitatively scored based on their semantic relationship s. The processing\\nprocedures for each are detailed below:\\n• STS Semantic Textual Similarity (STS) is characterized by its symmetric s e-\\nmantic matching to determine whether two sentences share equiva lent meaning.\\nSTS datasets typically consist of sentence pairs with associated lab els, which may'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 6, 'page_label': '7', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='mantic matching to determine whether two sentences share equiva lent meaning.\\nSTS datasets typically consist of sentence pairs with associated lab els, which may\\nbe binary classiﬁcations (yes/no, true/false) or numerical score s (e.g., 1.2, 3.1,\\n4.8). For binary labels, ”yes”/”true” are mapped to a numerical va lue of 1, while\\n”no”/”false” are converted to 0. The data is then structured int o (query, docu-\\nment, score) triplets. Due to the symmetric nature of STS, each s ingle original\\ndata sample can generate two training triplets by interchanging the query and\\npositive document roles.\\n• Textual Entailment Textual entailment further examines a model’s capabilities\\nin reasoning, typically featuring three-class labels: entailment, neu tral, contradic-\\ntion. Our processing method employs a three-tier scoring system: labels are\\nassigned values of 2, 1, and 0 for entailment, neutral, and contrad iction respec-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 6, 'page_label': '7', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='tion. Our processing method employs a three-tier scoring system: labels are\\nassigned values of 2, 1, and 0 for entailment, neutral, and contrad iction respec-\\ntively. We construct (query, document, score) triplets accordin gly, and similarly\\nleverage symmetry to double the dataset size.\\n3.2.3 CLS-oriented Process\\nClassiﬁcation tasks encompass text categorization and sentiment classiﬁcation scenar-\\nios, it typically follows a (text, label) format, where texts within the s ame category\\nexhibit semantic proximity while distinct boundaries separate diﬀeren t classes. NV-\\nEmbed[\\n47] compared label-based and example-based data construction met hods, with\\nexperimental results demonstrating the superiority of the latter . Adopting the example-\\nbased approach, we process classiﬁcation data (text, label) by us ing the text as query,\\n7'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 7, 'page_label': '8', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nFigure 2: CLS-oriented data transformation\\nsampling other texts sharing the same label as positive examples, an d selecting texts\\nfrom diﬀerent labels as negative examples. Figure 2 provides a detailed schematic\\nillustration of this process.\\n3.3 Training Strategy\\nEach task category—retrieval, NLI, and classiﬁcation—operates within a data construc-\\ntion process respectively, for which we have designed specialized tr aining objectives to\\nto enhance model training eﬃciency. This section elaborates on the design of loss\\nfunctions for retrieval, NLI, and classiﬁcation tasks.\\n3.3.1 Retrieval\\nFor the retrieval task, we adopt the widely used InfoNCE loss[\\n48], but incorporate an\\nimprovement inspired by gte[ 33] by augmenting the original query-negative loss with an\\nadditional query-query loss term. Speciﬁcally, each query within a b atch is treated as a\\nnegative sample for all other queries. The ﬁnal loss formulation is ex plicitly described'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 7, 'page_label': '8', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='additional query-query loss term. Speciﬁcally, each query within a b atch is treated as a\\nnegative sample for all other queries. The ﬁnal loss formulation is ex plicitly described\\nin Equation ( 1).\\nLRetrieval = − 1\\nn\\n∑\\ni\\nlog esim(qi,d +\\ni )/τ\\nesim(qi,d +\\ni )/τ + ∑\\nj esim(qi,d −\\nj )/τ + ∑\\nj̸=i esim(qi,q j )/τ\\n(1)\\n3.3.2 NLI\\nFor NLI tasks, the transformed labels are numerically comparable a nd exhibit ordinal\\nrelationships. We employ Cosent loss[\\n49] to optimize such data, which is designed\\nbased on the principles of Circle loss[ 40]. As a ranking-sensitive loss function, Cosent\\nloss requires only ordinal label information for optimization while demo nstrating faster\\nconvergence. Its mathematical formulation is presented in Equat ion ( 2).\\n8'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 8, 'page_label': '9', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nLNLI = log(1 +\\n∑\\nsim(i,j )>sim(k,l )\\nexp(sim(xk, x l) − sim(xi, x j)\\nτ )) (2)\\n3.3.3 CLS\\nThe classiﬁcation loss also adopts the InfoNCE objective. However , since CLS data is\\nprocessed in an example-based manner, directly applying in-batch n egative sampling\\non classiﬁcation datasets with limited categories may lead to false neg atives from items\\nof diﬀerent classes. Numerous studies have proposed diverse app roaches to address\\nthis issue[\\n51][52][47]. We propose a masking mechanism that appends class labels to\\neach positive and negative sample during preprocessing (recorded as separate variables\\nrather than modifying raw text). During in-batch negative sampling , for each negative\\nsample from other data instances, we check whether its label matc hes the current query’s\\nclass. If matched, the negative loss contribution is masked to zero to prevent erroneous'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 8, 'page_label': '9', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='sample from other data instances, we check whether its label matc hes the current query’s\\nclass. If matched, the negative loss contribution is masked to zero to prevent erroneous\\npenalization; otherwise, it is normally computed. The core loss remain s InfoNCE, with\\nthe CLS loss formulation shown in Equation ( 3). Where Cti denotes the class label of\\nsample ti, and nrepresents the number of negative samples per data instance.\\nLCLS = − 1\\nn\\n∑\\ni\\nlog esim(ti,t +\\ni )/τ\\nZi\\n(3)\\nwhere Zi = esim(ti,t +\\ni )/τ +\\n∑\\nn\\nMASK(ti, t −\\ni,n ) ·esim(ti,t −\\ni,n )/τ +\\n∑\\nj̸=i\\nMASK(ti, t j ) ·esim(ti,t j )/τ +\\n∑\\nj̸=i\\n∑\\nn\\nMASK(ti, t −\\nj,n ) ·esim(ti,t −\\nj,n )/τ\\nand Cti = Ct+\\ni\\nand MASK( ti, t j ) =\\n{\\n0 if Cti = Ctj ,\\n1 otherwise\\n4 Data Synthesis\\nThe production of higher-quality data through data production ha s gained critical im-\\nportance in embedding training. Manual annotation incurs higher co sts and lower\\nproduction eﬃciency, thus developing eﬀective automated data sy nthesis methods has'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 8, 'page_label': '9', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='portance in embedding training. Manual annotation incurs higher co sts and lower\\nproduction eﬃciency, thus developing eﬀective automated data sy nthesis methods has\\nemerged as a key research focus. Recent advancements in large la nguage models (LLMs)\\nhave signiﬁcantly improved their linguistic capabilities, enabling accura te interpretation\\nof human instructions and generation of high-quality outputs. Mult iple existing meth-\\nods have eﬀectively leveraged LLMs to generate high-quality data[\\n28][34], we similarly\\n9'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 9, 'page_label': '10', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nleverages LLM capabilities for data production across three dimens ions: structural di-\\nversity, semantic diversity, and diﬃculty, with dedicated synthesis strategies for each.\\nFor structural diversity, we propose Paraphrasing techniques; for semantic diversity,\\nwe introduce Augmentation methods; and to increase training diﬃcu lty and improve\\nsemantic discriminability, we employ LLMs to generate more challenging hard negative\\nexamples. The following sections detail these methodologies. The co nstraint compo-\\nnents for all data synthesis techniques are speciﬁed in Table 5 of Appendix A.1.\\n4.1 Structural Diversity Enhancement\\nLinguistic structures of text encompass lexical, syntactic, and gr ammatical features,\\nwhich represent relatively surface-level characteristics reﬂect ing word arrangements,\\ncombinations, tenses, voices, and other formal attributes. Emb edding models must'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 9, 'page_label': '10', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='which represent relatively surface-level characteristics reﬂect ing word arrangements,\\ncombinations, tenses, voices, and other formal attributes. Emb edding models must\\naccurately capture underlying semantics despite variations in surf ace form, ensuring\\nrobustness to external structural changes. For example, the following two sentences,\\ndespite structural diﬀerences, should be recognized as semantic ally equivalent:\\n• The cat chased the mouse.\\n• The mouse was chased by the cat.\\nTo eﬀectively train an embedding model that remains invariant to str uctural variations\\nwhile accurately capturing semantic information, we propose a Para phrasing strategy.\\nFor each training sample containing a query and a positive document, we apply LLM-\\nbased paraphrasing to both contents, generating augmented ins tances that preserve\\nsemantic equivalence while introducing structural divergence. The prompt constraints\\nand workﬂow are illustrated in Figure\\n3.\\nFigure 3: LLM-based Paraphrasing Workﬂow'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 9, 'page_label': '10', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='semantic equivalence while introducing structural divergence. The prompt constraints\\nand workﬂow are illustrated in Figure\\n3.\\nFigure 3: LLM-based Paraphrasing Workﬂow\\n4.2 Semantic Diversity Enhancement\\nMerely augmenting data through superﬁcial structural modiﬁcat ions yields negligible\\nimprovements in model capabilities, as generalization relies not only on structural dis-\\nentanglement but also on diverse topics and content to ensure unif orm vector rep-\\nresentations in the spatial domain. Therefore, beyond paraphra sing, we propose an\\naugmentation method using LLM to diversify semantics. The core co ncept is: given a\\n10'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 10, 'page_label': '11', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\ncomplete (query, positive) pair, the model must comprehend the d omain and perspec-\\ntive discussed and learn to expand into diﬀerent topics, aspects, a nd viewpoints while\\nremaining contextually anchored. This process is governed via prom pt constraints. The\\nAugmentation framework is illustrated in Figure 4.\\nFigure 4: Semantic Augmentation Workﬂow\\nFigure 5: Hard Negative Synthesis Workﬂow\\n4.3 More challenging embeddings\\nHard negative examples are crucial for enhancing the performanc e of text embedding\\nmodels, often requiring substantial eﬀort to acquire. Leveraging the linguistic capabili-\\nties of large language models, we design an automated hard negative synthesis method\\ntailored for retrieval datasets. Our domain-speciﬁc experiments demonstrate that large\\nlanguage models can generate examples that are indistinguishable, t he framework is\\nillustrated in Figure\\n5.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 10, 'page_label': '11', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='tailored for retrieval datasets. Our domain-speciﬁc experiments demonstrate that large\\nlanguage models can generate examples that are indistinguishable, t he framework is\\nillustrated in Figure\\n5.\\nDuring Data paraphrasing and Augmentation, we implement task-sp eciﬁc strategies:\\nfor retrieval tasks, we rewrite/expand (query, positive) pairs a nd add them to the orig-\\ninal dataset; for NLI tasks, we rewrite individual sentences by ra ndomly duplicating\\nexisting entries containing the original sentences and replacing the m with rewritten\\nversions to achieve data expansion—without applying augmentation to prevent ambi-\\nguity; for classiﬁcation tasks, we rewrite sentences while retaining their original labels,\\nexample-based processing was applied using the rewritten results, again without em-\\nploying augmentation. We provide several data synthesis examples in Appendix A.3\\nfor reference.\\n11'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 11, 'page_label': '12', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nFigure 6: Training pipeline\\n5 Training Optimization\\n5.1 Data Grouping Strategy\\nPrior works like Linq-Embedding[\\n52] and SFR-Embedding-Mistral[ 30] adopted task-\\nhomogeneous batching, partitioning data by task rather than mixin g them, and sam-\\npling tasks based on weighted randomness during training. Building on this, we propose\\na reﬁned Data Grouping Strategy, extending the granularity from task-level to dataset-\\nlevel partitioning. We posit that dataset-level grouping captures more domain-speciﬁc\\nclustering patterns—samples within the same dataset often exhibit inherent domain\\nsimilarities, while such consistency may not hold across datasets.\\nOur approach partitions training data into subsets by name. During training, only\\nsamples from a single dataset are sampled per batch, with ﬁle pointer s recorded to\\nenable sequential reading in subsequent iterations. For sampling we ights, we adopt\\nthe data sampling strategy from gte['),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 11, 'page_label': '12', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='enable sequential reading in subsequent iterations. For sampling we ights, we adopt\\nthe data sampling strategy from gte[\\n33] and mgte[ 50], scaling weights by dataset size\\nfollowed by normalization. For dataset i with size li, its sampling weight is computed\\nas Equation ( 4)\\npi = lα\\ni∑ m\\nj=1 lα\\nj\\n(4)\\n5.2 Two-Stage Training\\nInspired by NV-Embed’s[\\n47] two-stage contrastive learning instruction tuning tech-\\nnique, we adopt a similar training approach: the ﬁrst stage exclusive ly uses retrieval-\\noriented training data, while the second stage integrates both ret rieval and non-retrieval\\ntasks, the overall training framework is illustrated in the ﬁgure 6. Two key distinctions\\nare incorporated: ﬁrst, we integrate the previously described Da ta Grouping Strat-\\negy; second, we implement global control over the sampling ratio of retrieval training\\ndatasets, since our ﬁndings indicate that naively incorporating add itional data signiﬁ-\\ncantly degrades retrieval performance.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 11, 'page_label': '12', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='datasets, since our ﬁndings indicate that naively incorporating add itional data signiﬁ-\\ncantly degrades retrieval performance.\\nFor global control of sampling ratio, a hyperparameter η is introduced into the sampling\\n12'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 12, 'page_label': '13', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nfunction to control the proportion of retrieval training, ensurin g that throughout the\\nsecond training stage, the computational contribution of retriev al data accounts for η,\\nwhile non-retrieval data constitutes 1 − η. The following set of equations formalizes the\\ncomputational process from partitioned datasets to sampling rat io determination. Let\\nthe training data D = [ d1, d 2, ..., d N ] , where each di represents a distinct dataset (e.g.,\\nMSMARCO passage, SQUAD), with corresponding sizes L = [ l1, l 2, ..., l N ]. Following\\nthe aforementioned strategy, we ﬁrst apply an exponential scalin g factor α , a mask fac-\\ntor M is then applied to ﬁlter retrieval and non-retrieval training sets fo r summation.\\nThe equations are as follows:\\nSret =\\n∑\\ni\\nMi ·lα\\ni\\nSnon ret =\\n∑\\ni\\n(1 − Mi) ·lα\\ni\\nwhere M i =\\n{\\n0 if di ∈ RET,\\n1 else\\nwhere RET denotes the set of retrieval training datasets. The re trieval ratio is then'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 12, 'page_label': '13', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Sret =\\n∑\\ni\\nMi ·lα\\ni\\nSnon ret =\\n∑\\ni\\n(1 − Mi) ·lα\\ni\\nwhere M i =\\n{\\n0 if di ∈ RET,\\n1 else\\nwhere RET denotes the set of retrieval training datasets. The re trieval ratio is then\\nscaled using η to derive the ﬁnal normalized sampling ratios for the training sets:\\nLsamp = [ lsamp\\n1 , l samp\\n2 , ...l samp\\nN ]\\nwhere l samp\\ni =\\n{ ηRET ·lα\\ni\\nSret\\nif di ∈ RET,\\n(1−ηRET )·lα\\ni\\nSnon ret\\nelse\\n6 Experiments\\n6.1 Training Dataset\\nPrimary data sources include bge-en-icl, bge-m3-data, and bge-m ultilingual-gemma2-\\ndata\\n3 . The E5 dataset (approximately 1.5M samples) 4, utilized in E5-Mistral-7B[ 28],\\nEcho Embedding[ 11], and LLM2Vec[ 12], is also incorporated. The aforementioned\\ndatasets include commonly used retrieval training corpora such as MS MARCO (both\\npassage and document versions)[ 64], Natural Questions (NQ)[ 65], ELI5[66], HotpotQA[ 67],\\nMIRACL[68], SQuAD[ 69], FEVER[70], Quora Question Pairs(QQP), and DuReader[ 71],'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 12, 'page_label': '13', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='passage and document versions)[ 64], Natural Questions (NQ)[ 65], ELI5[66], HotpotQA[ 67],\\nMIRACL[68], SQuAD[ 69], FEVER[70], Quora Question Pairs(QQP), and DuReader[ 71],\\netc. Previous researchers have already systematically collected a nd organized these\\ndatasets, making them readily usable, we solely utilized the proposed method to update\\nharder negative samples. Stella’s[ 53] retrieval data llm 5 provides high-quality (query,\\npositive, negative) triplets, while zpoint leverages datasets such a s Huatuo medical QA 6,\\nall above data has been incorporated. Additional data from huggin gface’s sentence-\\ntransformers7 repository includes reddit, hover[ 72], mr-tydi[ 73], law-gpt, and s2orc[ 74].\\n3https://github.com/FlagOpen/FlagEmbedding/tree/master/dataset\\n4https://drive.google.com/ﬁle/d/1YqgaJIzmBIH37XBxpRPCVzV CLh6aOI4/view\\n5https://huggingface.co/datasets/infgrad/retrieval data llm\\n6https://huggingface.co/iampanda/zpoint large embedding zh'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 12, 'page_label': '13', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='4https://drive.google.com/ﬁle/d/1YqgaJIzmBIH37XBxpRPCVzV CLh6aOI4/view\\n5https://huggingface.co/datasets/infgrad/retrieval data llm\\n6https://huggingface.co/iampanda/zpoint large embedding zh\\n7https://huggingface.co/sentence-transformers\\n13'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 13, 'page_label': '14', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nOther sources encompass web questions, BioASQ[ 54], cmrc[ 55], CSL 8, nli for simcse\\n(used in SimCSE[ 7] and GTE[ 33]), MLDR 9, GLUE Benchmark[ 56], Yelp Reviews[ 57]\\nand Weibo Sentiment 10 training sets.\\nWe further integrate MTEB evaluation-related datasets like Imdb- Classiﬁcation[58],\\nMassiveIntent-Classiﬁcation[59], MassiveScenario-Classiﬁcation[59], STS12[60], LCQMC[61],\\nPAWSX[62], and STSB[ 63], we utilized the training split from these datasets with con-\\ntamination exclusion applied to remove samples highly similar to test set s.\\nFor data requiring format conversion, we apply the methodologies d escribed in Sen-\\ntion 3.2. Datasets with limited samples (e.g., subsets of bge and e5 series, Im db-\\nClassiﬁcation, STS12, LCQMC) are augmented via Paraphrasing and Augmentation\\n(typically applied to datasets with fewer than 60k samples), we ultima tely obtained ap-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 13, 'page_label': '14', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Classiﬁcation, STS12, LCQMC) are augmented via Paraphrasing and Augmentation\\n(typically applied to datasets with fewer than 60k samples), we ultima tely obtained ap-\\nproximately 5M high-quality training samples through API interfaces . We deduplicate\\nall training sets and ﬁlter out samples with low query-pos scores usin g GTE-Qwen2-7B-\\nInstruct 11. For retrieval data lacking hard negatives, we employ synthetic ha rd negative\\ngeneration. Due to API cost constraints, only 30% of hard negativ es are synthetically\\ngenerated; the remainder are produced using stella-large-zh-v3 -1792d[53], with top-10\\nto top-30 ranked results selected as hard negatives. The ﬁnal tr aining dataset contains\\n11M quadruples (query, pos, neg, instruction) in total.\\n6.2 Trainset Instructions\\nFor most training data containing instruction formats, we retain th eir original con-\\ntents. For the MTEB training set, we adopt instructions correspo nding to its evalu-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 13, 'page_label': '14', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='6.2 Trainset Instructions\\nFor most training data containing instruction formats, we retain th eir original con-\\ntents. For the MTEB training set, we adopt instructions correspo nding to its evalu-\\nation(consistent with Qwen3-Embedding runtime). For external d ata lacking instruc-\\ntions (e.g., Huatuo, Reddit, Law-GPT, GLUE), we design task-spec iﬁc and domain-\\nadaptive instructions. Partial instruction templates are provided in Appendix\\nA.2.\\n6.3 Training Details\\nAs previously mentioned, we adopt a two-stage training approach. For the ﬁrst-stage\\nretrieval training, we train on all retrieval datasets, with a warm- up step of 300 and\\na learning rate of 3e-5, the total step of training is 32k. In the sec ond stage, we use\\nall training data, set the learning rate to 2e-5, and train for 8k ste ps, keeping all other\\nconﬁgurations the same as in the ﬁrst stage. We employ a batch size of 256 for all data\\nusing the InfoNCE loss (i.e., retrieval and classiﬁcation), considerin g data using the'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 13, 'page_label': '14', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='conﬁgurations the same as in the ﬁrst stage. We employ a batch size of 256 for all data\\nusing the InfoNCE loss (i.e., retrieval and classiﬁcation), considerin g data using the\\ncosent loss (i.e., NLI), due to lower memory consumption from the ab sence of forward\\ncomputation for negative samples, the batch size is set to 768. Acr oss all stages, we\\nemploy bﬂoat16 precision, with 4 hard negative samples and a cosine t emperature of\\n0.02, using Adam optimizer with a weight decay of 0.01. The Data Group ing Strategy\\nremains unchanged between the two stages, except that the sec ond stage incorporates\\nall data with a global retrieval ratio ηRET of 0.72. Unlike existing works that commonly\\n8https://github.com/ydli-ai/CSL?tab=readme-ov-ﬁle\\n9https://huggingface.co/datasets/Shitao/MLDR\\n10https://github.com/SophonPlus/ChineseNlpCorpus?tab=readme-ov-ﬁle\\n11https://huggingface.co/Alibaba-NLP/gte-Qwen2-7B-instruct\\n14'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 14, 'page_label': '15', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nuse LoRA ﬁne-tuning, we employ full-parameter ﬁne-tuning at all st ages to ensure\\nmaximum performance improvement. The query and passage length s are set to 256\\nand 1536 respectively. However, in practice, the model can handle sequences up to 8k\\nin length due to the strong length extrapolation capability of the RoP E[35] positional\\nencoding used in most LLMs. The hyperparameter conﬁgurations f or all training stages\\nare provided in the table 1.\\nTable 1: Training Hyperparameter Speciﬁcations\\nItem Stage1 Stage2\\nWarm-up 300\\nSteps 3e-5 2e-5\\nLR 32k 8k\\nBatch Size InfoNCE 256\\nBatch Size Cosent - 768\\nPrecision bﬂoat16\\nTemperature 0.02\\nOptimizer Adam\\nQuery Length 256\\nPassage Length 1536\\n6.4 Compared Methods\\nWe selected the top-10 ranked models(August 27, 2025) on the MT EB/CMTEB leader-\\nboards prior to the release of QZhou-Embedding as baselines. For M TEB, the compar-\\native models include LGAI-Embedding-Preview['),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 14, 'page_label': '15', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='boards prior to the release of QZhou-Embedding as baselines. For M TEB, the compar-\\native models include LGAI-Embedding-Preview[\\n17], the Seed series (v1.5[ 75] , v1.6[ 38]),\\nQwen series (8B, 4B)[ 34], ritrieve zh v1, xiaobu-embedding-v2, gemini-embedding-001[ 76],\\njasper en vision language v1[14], Linq-Embed-Mistral[52], SFR-Embedding-Mistral[ 30],\\nand NV-Embed-v2[ 47]. For CMTEB, the baseline models comprise the Seed series (as\\nabove), Qwen series (as above), Conan series (v1[ 24], v2[13]), zpoint large embedding zh,\\nand piccolo-large-zh-v2[ 39].\\n6.5 Main Results\\nThis section presents the evaluation results of Qzhou-embedding o n MTEB/CMTEB\\nbenchmarks, alongside comparative scores from the top 10 ranke d models. As detailed\\nin Table\\n2, Table 3, Qzhou-embedding achieves state-of-the-art performance ac ross\\nboth task-level and task-type average metrics, demonstrating the eﬀectiveness of our\\napproach. Furthermore, under MTEB’s oﬃcial ranking protocol, Q zhou-embedding'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 14, 'page_label': '15', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='both task-level and task-type average metrics, demonstrating the eﬀectiveness of our\\napproach. Furthermore, under MTEB’s oﬃcial ranking protocol, Q zhou-embedding\\nsecured the top position on both leaderboards. ( Note: Highlighted maximum values\\nin certain columns may reﬂect the best performance among the liste d models rather\\nthan the overall leaderboard maximum, as exempliﬁed by the MTEB/c lassiﬁcation\\nbenchmark where the top score does not appear in the top 10 mode ls.)\\n15'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 15, 'page_label': '16', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nTable 2: Performance on MTEB(eng, v2)\\nModel Class. Clust. Pair Class. Rerank. STS Retr. Summ. Mean(Task) Mean(TaskType)\\nLGAI-Embedding-Preview 89.97 59.25 88.67 49.13 66.18 86.69 38.93 74.12 68.4\\nSeed1.5-Embedding 89.88 60.83 87.39 50.67 67.45 87.23 36.44 74.76 68.56\\nQwen3-Embedding-8B 90.43 58.57 87.52 51.56 69.44 88.58 34.83 75.22 68.71\\nQwen3-Embedding-4B 89.84 57.51 87.01 50.76 68.46 88.72 34.39 74.6 68.1\\nSeed1.6-embedding 92.42 59.22 85.07 50.28 64.9 86.87 37.1 74.07 67.98\\ngemini-embedding-001 90.05 59.39 87.7 48.59 64.35 85.29 38.28 73.3 67.67\\njasper en vision language v1 90.27 60.52 88.14 50 56.05 84.37 37.19 71.41 66.65\\nLinq-Embed-Mistral 83 54.07 88.44 49.44 60.14 84.69 37.26 69.8 65.29\\nSFR-Embedding-Mistral 80.47 54.93 88.59 50.15 59.33 84.77 36.32 69.31 64.94\\nNV-Embed-v2 87.19 47.66 88.69 49.61 62.84 83.82 35.21 69.81 65\\nQZhou-Embedding(Ours) 88.97 61.65 92.43 51.77 67.12 91.65 33.05 75.97 69.52'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 15, 'page_label': '16', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='NV-Embed-v2 87.19 47.66 88.69 49.61 62.84 83.82 35.21 69.81 65\\nQZhou-Embedding(Ours) 88.97 61.65 92.43 51.77 67.12 91.65 33.05 75.97 69.52\\nTable 3: Performance on CMTEB(cmn, v1)\\nModel Class. Clust. Pair Class. Rerank. STS Retr. Mean(Task) Mean(TaskType)\\nSeed1.6-embedding 77.98 73.11 88.71 71.65 79.69 68.94 75.63 76.68\\nSeed1.5-Embedding 79.37 71.11 89.57 70.14 79.33 66.56 74.87 76.01\\nritrieve zh v1 76.88 66.5 85.98 72.86 76.97 63.92 72.71 73.85\\nConan-embedding-v2 76.47 68.84 92.44 74.41 78.31 65.48 74.24 75.99\\nxiaobu-embedding-v2 76.53 65.17 85.94 72.58 76.49 64.18 72.36 73.48\\nQwen3-Embedding-8B 76.97 80.08 84.23 66.99 78.21 63.53 73.84 75\\nConan-embedding-v1 76.77 66.33 85.68 72.76 76.67 63.67 72.5 73.65\\nzpoint large embedding zh 76.4 62.23 85.75 72.33 76.36 63.86 71.81 72.82\\npiccolo-large-zh-v2 76.42 62.16 85.22 70 74.36 63.46 70.86 71.94\\nQwen3-Embedding-4B 75.46 77.89 83.34 66.05 77.03 61.26 72.27 73.51\\nQZhou-Embedding(Ours) 79.99 70.91 95.07 74.85 78.80 71.89 76.99 78.58'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 15, 'page_label': '16', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Qwen3-Embedding-4B 75.46 77.89 83.34 66.05 77.03 61.26 72.27 73.51\\nQZhou-Embedding(Ours) 79.99 70.91 95.07 74.85 78.80 71.89 76.99 78.58\\n7 Conclusion\\nIn this technical report, we present QZhou-Embedding, a genera l-purpose contextual\\ntext embedding model with exceptional text representation capa bilities. We designed a\\nuniﬁed multi-task framework comprising specialized data transform ation and training\\nstrategies, eﬀectively enhanced the diversity of training data. To further improve the\\nquality of training data and the model’s generalization capabilities, we d eveloped a data\\nsynthesis pipeline leveraging LLM API, incorporating techniques suc h as Paraphrasing,\\nAugmentation, and Hard negative example generation. We employ a t wo-stage training\\nstrategy comprising initial retrieval-focused training followed by fu ll-task ﬁne-tuning,\\nenabling the embedding model to extend its capabilities based on robu st retrieval per-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 15, 'page_label': '16', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='strategy comprising initial retrieval-focused training followed by fu ll-task ﬁne-tuning,\\nenabling the embedding model to extend its capabilities based on robu st retrieval per-\\nformance. The model achieves state-of-the-art results on the MTEB and CMTEB\\nbenchmarks, ranking ﬁrst on both leaderboards. Our ﬁndings est ablish that data qual-\\nity and diversity are pivotal for improving embedding model capabilitie s. In the future,\\nwe will focus on developing multimodal and multilingual embedding models , as well\\nas exploring eﬀective applications of embedding models in agent syste ms, aiming to\\nintegrate cutting-edge technologies to optimize this classical modu le.\\nReferences\\n[1] Robertson, Stephen E., and Steve Walker. ”Some simple eﬀective approximations to\\nthe 2-poisson model for probabilistic weighted retrieval.” In SIGIR’9 4: Proceedings\\n16'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 16, 'page_label': '17', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nof the Seventeenth Annual International ACM-SIGIR Conferen ce on Research and\\nDevelopment in Information Retrieval, organised by Dublin City Univer sity, pp.\\n232-241. London: Springer London, 1994.\\n[2] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutano va. Bert: Pre-\\ntraining of deep bidirectional transformers for language underst anding. arXiv\\npreprint arXiv:1810.04805, 2018.\\n[3] Colin Raﬀel, Noam Shazeer, Adam Roberts, Katherine Lee, Shara n Narang, Michael\\nMatena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of tr ansfer learn-\\ning with a uniﬁed text-to-text transformer. Journal of machine le arning research,\\n21(140):1–67, 2020.\\n[4] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, D axin Jiang,\\nRangan Majumder, and Furu Wei. Text embeddings by weakly-super vised con-\\ntrastive pre-training. arXiv preprint arXiv:2212.03533, 2022.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 16, 'page_label': '17', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Rangan Majumder, and Furu Wei. Text embeddings by weakly-super vised con-\\ntrastive pre-training. arXiv preprint arXiv:2212.03533, 2022.\\n[5] Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Ried el, Piotr Bo-\\njanowski, Armand Joulin, and Edouard Grave. Unsupervised dense information\\nretrieval with contrastive learning. arXiv preprint arXiv:2112.0911 8, 2021.\\n[6] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence em beddings using\\nsiamese bert-networks. arXiv preprint arXiv:1908.10084, 2019.\\n[7] Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple contrastive\\nlearning of sentence embeddings. In Proceedings of the 2021 Conf erence on Empir-\\nical Methods in Natural Language Processing, pages 6894–6910, Online and Punta\\nCana, Dominican Republic. Association for Computational Linguistics .\\n[8] Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hern´ andez ´Abrego, Ji Ma,\\nVincent Y Zhao, Yi Luan, Keith B Hall, Ming-Wei Chang, et al. Large du al encoders'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 16, 'page_label': '17', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='[8] Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hern´ andez ´Abrego, Ji Ma,\\nVincent Y Zhao, Yi Luan, Keith B Hall, Ming-Wei Chang, et al. Large du al encoders\\nare generalizable retrievers. arXiv preprint arXiv:2112.07899, 202 1.\\n[9] Brown, Tom, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D . Kaplan, Pra-\\nfulla Dhariwal, Arvind Neelakantan et al. ”Language models are few-s hot learners.”\\nAdvances in neural information processing systems 33 (2020): 18 77-1901.\\n[10] Ma, Xueguang, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin. ”F ine-tuning\\nllama for multi-stage text retrieval.” In Proceedings of the 47th Int ernational ACM\\nSIGIR Conference on Research and Development in Information Re trieval, pp. 2421-\\n2425. 2024.\\n[11] Springer, Jacob Mitchell, Suhas Kotha, Daniel Fried, Graham Ne ubig, and Aditi\\nRaghunathan. ”Repetition improves language model embeddings.” a rXiv preprint\\narXiv:2402.15449 (2024).\\n[12] BehnamGhader, Parishad, Vaibhav Adlakha, Marius Mosbach, D zmitry Bah-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 16, 'page_label': '17', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Raghunathan. ”Repetition improves language model embeddings.” a rXiv preprint\\narXiv:2402.15449 (2024).\\n[12] BehnamGhader, Parishad, Vaibhav Adlakha, Marius Mosbach, D zmitry Bah-\\ndanau, Nicolas Chapados, and Siva Reddy. ”Llm2vec: Large languag e models are\\nsecretly powerful text encoders.” arXiv preprint arXiv:2404.0596 1 (2024).\\n[13] https://cloud.tencent.com/developer/news/2461911\\n17'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 17, 'page_label': '18', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\n[14] Zhang, Dun, Jiacheng Li, Ziyang Zeng, and Fulong Wang. ”Jaspe r and stella:\\ndistillation of sota embedding models.” arXiv preprint arXiv:2412.19048 (2024).\\n[15] Chen, Jianlv, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng\\nLiu. ”Bge m3-embedding: Multi-lingual, multi-functionality, multi-gran ularity text\\nembeddings through self-knowledge distillation.” arXiv preprint arXiv :2402.03216\\n(2024).\\n[16] Ji, Yifan, Zhipeng Xu, Zhenghao Liu, Yukun Yan, Shi Yu, Yishan L i, Zhiyuan\\nLiu, Yu Gu, Ge Yu, and Maosong Sun. ”Learning more eﬀective repre senta-\\ntions for dense retrieval through deliberate thinking before sear ch.” arXiv preprint\\narXiv:2502.12974 (2025).\\n[17] Choi J, Kim H, Jang H, et al. LG-ANNA-Embedding technical repo rt[J]. arXiv\\npreprint arXiv:2506.07438, 2025.\\n[18] Xiong, Lee, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Pau l Bennett,\\nJunaid Ahmed, and Arnold Overwijk. ”Approximate nearest neighbo r negative con-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 17, 'page_label': '18', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='preprint arXiv:2506.07438, 2025.\\n[18] Xiong, Lee, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Pau l Bennett,\\nJunaid Ahmed, and Arnold Overwijk. ”Approximate nearest neighbo r negative con-\\ntrastive learning for dense text retrieval.” arXiv preprint arXiv:200 7.00808 (2020).\\n[19] Lee, Chankyu, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad\\nShoeybi, Bryan Catanzaro, and Wei Ping. ”Nv-embed: Improved t echniques for\\ntraining llms as generalist embedding models.” arXiv preprint arXiv:2405 .17428\\n(2024).\\n[20] Moreira, Gabriel de Souza P., Radek Osmulski, Mengyao Xu, Rona y Ak, Benedikt\\nSchiﬀerer, and Even Oldridge. ”NV-Retriever: Improving text emb edding models\\nwith eﬀective hard-negative mining.” arXiv preprint arXiv:2407.15831 (2024).\\n[21] Team, Qwen. ”Qwen2 technical report.” arXiv preprint arXiv:24 07.10671 (2024).\\n[22] Xiao, Shitao, Zheng Liu, Peitian Zhang, Niklas Muennighoﬀ, Defu L ian, and Jian-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 17, 'page_label': '18', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='[21] Team, Qwen. ”Qwen2 technical report.” arXiv preprint arXiv:24 07.10671 (2024).\\n[22] Xiao, Shitao, Zheng Liu, Peitian Zhang, Niklas Muennighoﬀ, Defu L ian, and Jian-\\nYun Nie. ”C-pack: Packed resources for general chinese embedd ings.” In Proceedings\\nof the 47th international ACM SIGIR conference on research and development in\\ninformation retrieval, pp. 641-649. 2024. Team, Qwen.\\n[23] Muennighoﬀ, Niklas, Nouamane Tazi, Lo¨ ıc Magne, and Nils Reimers . ”Mteb: Mas-\\nsive text embedding benchmark.” arXiv preprint arXiv:2210.07316 (2 022).\\n[24] Li, Shiyu, Yang Tang, Shizhe Chen, and Xi Chen. ”Conan-embed ding: Gen-\\neral text embedding with more and better negative samples.” arXiv p reprint\\narXiv:2408.15710 (2024).\\n[25] Aizawa, Akiko. ”An information-theoretic perspective of tf–id f measures.” Infor-\\nmation Processing & Management 39, no. 1 (2003): 45-65.\\n[26] Robertson, Stephen E., and Steve Walker. ”Some simple eﬀectiv e approximations'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 17, 'page_label': '18', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='mation Processing & Management 39, no. 1 (2003): 45-65.\\n[26] Robertson, Stephen E., and Steve Walker. ”Some simple eﬀectiv e approximations\\nto the 2-poisson model for probabilistic weighted retrieval.” In SIGI R’94: Proceed-\\nings of the Seventeenth Annual International ACM-SIGIR Confe rence on Research\\nand Development in Information Retrieval, organised by Dublin City Un iversity,\\npp. 232-241. London: Springer London, 1994.\\n18'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 18, 'page_label': '19', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\n[27] Deerwester, Scott, Susan T. Dumais, George W. Furnas, Tho mas K. Landauer, and\\nRichard Harshman. ”Indexing by latent semantic analysis.” Journal of the American\\nsociety for information science 41, no. 6 (1990): 391-407.\\n[28] Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Maj umder, and\\nFuru Wei. Improving text embeddings with large language models. arX iv preprint\\narXiv:2401.00368, 2023b.\\n[29] Meng, Rui, Ye Liu, Shaﬁq Rayhan Joty, Caiming Xiong, Yingbo Zhou , and Semih\\nYavuz. ”Sfrembedding-mistral: enhance text retrieval with tran sfer learning.” Sales-\\nforce AI Research Blog 3 (2024): 6.\\n[30] Meng R, Liu Y, Joty S R, et al. Sfr-embedding-2: Advanced text embedding with\\nmulti-stage training, 2024[J].\\n[31] Muennighoﬀ, Niklas, S. U. Hongjin, Liang Wang, Nan Yang, Furu W ei, Tao Yu,\\nAmanpreet Singh, and Douwe Kiela. ”Generative representational instruction tun-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 18, 'page_label': '19', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='multi-stage training, 2024[J].\\n[31] Muennighoﬀ, Niklas, S. U. Hongjin, Liang Wang, Nan Yang, Furu W ei, Tao Yu,\\nAmanpreet Singh, and Douwe Kiela. ”Generative representational instruction tun-\\ning.” In The Thirteenth International Conference on Learning Rep resentations.\\n2024.\\n[32] Chaofan Li, MingHao Qin, Shitao Xiao, Jianlyu Chen, Kun Luo, Yingx ia Shao,\\nDefu Lian, and Zheng Liu. Making text embedders few-shot learner s. arXiv preprint\\narXiv:2409.15700, 2024.\\n[33] Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie , and Meis-\\nhan Zhang. Towards general text embeddings with multi-stage con trastive learning,\\n2023. URL https://arxiv.org/abs/2308.03281.\\n[34] Zhang, Yanzhao, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, B aosong Yang,\\nPengjun Xie et al. ”Qwen3 Embedding: Advancing Text Embedding and Reranking\\nThrough Foundation Models.” arXiv preprint arXiv:2506.05176 (2025 ).\\n[35] Su, Jianlin, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, an d Yunfeng Liu.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 18, 'page_label': '19', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Through Foundation Models.” arXiv preprint arXiv:2506.05176 (2025 ).\\n[35] Su, Jianlin, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, an d Yunfeng Liu.\\n”Roformer: Enhanced transformer with rotary position embeddin g.” Neurocomput-\\ning 568 (2024): 127063.\\n[36] Zhang, Biao, and Rico Sennrich. ”Root mean square layer norma lization.” Ad-\\nvances in neural information processing systems 32 (2019).\\n[37] Shazeer, Noam. ”Glu variants improve transformer.” arXiv pre print\\narXiv:2002.05202 (2020).\\n[38] https://seed1-6-embedding.github.io/\\n[39] Huang, Junqin, Zhongjie Hu, Zihao Jing, Mengya Gao, and Yichao Wu. ”Pic-\\ncolo2: General text embedding with multi-task hybrid loss training.” a rXiv preprint\\narXiv:2405.06932 (2024).\\n[40] Sun, Yifan, Changmao Cheng, Yuhan Zhang, Chi Zhang, Liang Z heng, Zhongdao\\nWang, and Yichen Wei. ”Circle loss: A uniﬁed perspective of pair similarit y op-\\ntimization.” In Proceedings of the IEEE/CVF conference on comput er vision and'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 18, 'page_label': '19', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Wang, and Yichen Wei. ”Circle loss: A uniﬁed perspective of pair similarit y op-\\ntimization.” In Proceedings of the IEEE/CVF conference on comput er vision and\\npattern recognition, pp. 6398-6407. 2020.\\n19'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 19, 'page_label': '20', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\n[41] Rodrigo Nogueira, Wei Yang, Jimmy Lin, and Kyunghyun Cho. 201 9. Document\\nexpansion by query prediction. ArXiv preprint, abs/1904.08375.\\n[42] Liang Wang, Nan Yang, and Furu Wei. 2023. Query2doc: Query e xpansion with\\nlarge language models. In Proceedings of the 2023 Conference on E mpirical Meth-\\nods in Natural Language Processing, pages 9414–9423, Singapor e. Association for\\nComputational Linguistics.\\n[43] Zhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, An ton Bakalov,\\nKelvin Guu, Keith Hall, and Ming-Wei Chang. 2022. Promptagator: Fe wshot dense\\nretrieval from 8 examples. In The Eleventh International Confer ence on Learning\\nRepresentations.\\n[44] Kexin Wang, Nandan Thakur, Nils Reimers, and Iryna Gurevych. 2022a. GPL:\\nGenerative pseudo labeling for unsupervised domain adaptation of d ense retrieval.\\nIn Proceedings of the 2022 Conference of the North American Cha pter of the'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 19, 'page_label': '20', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Generative pseudo labeling for unsupervised domain adaptation of d ense retrieval.\\nIn Proceedings of the 2022 Conference of the North American Cha pter of the\\nAssociation for Computational Linguistics: Human Language Techn ologies, pages\\n2345–2360, Seattle, United States. Association for Computation al Linguistics.\\n[45] Honovich, Or, Thomas Scialom, Omer Levy, and Timo Schick. ”Unn atural in-\\nstructions: Tuning language models with (almost) no human labor.” ar Xiv preprint\\narXiv:2212.09689 (2022).\\n[46] Xiong, Lee, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Pau l Bennett,\\nJunaid Ahmed, and Arnold Overwijk. ”Approximate nearest neighbo r negative con-\\ntrastive learning for dense text retrieval.” arXiv preprint arXiv:200 7.00808 (2020).\\n[47] Moreira, Gabriel de Souza P., Radek Osmulski, Mengyao Xu, Rona y Ak, Benedikt\\nSchiﬀerer, and Even Oldridge. ”NV-Retriever: Improving text emb edding models\\nwith eﬀective hard-negative mining.” arXiv preprint arXiv:2407.15831 (2024).'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 19, 'page_label': '20', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Schiﬀerer, and Even Oldridge. ”NV-Retriever: Improving text emb edding models\\nwith eﬀective hard-negative mining.” arXiv preprint arXiv:2407.15831 (2024).\\n[48] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representatio n learning with\\ncontrastive predictive coding. arXiv preprint arXiv:1807.03748, 20 18.\\n[49] https://www.kexue.fm/archives/8847\\n[50] Xin Zhang, Yanzhao Zhang, Dingkun Long, Wen Xie, Ziqi Dai, Jialon g Tang, Huan\\nLin, Baosong Yang, Pengjun Xie, Fei Huang, Meishan Zhang, Wenjie Li, and Min\\nZhang. mgte: Generalized long-context text representation and reranking models\\nfor multilingual text retrieval, 2024.\\n[51] Lee, Jinhyuk, Zhuyun Dai, Xiaoqi Ren, Blair Chen, Daniel Cer, Je remy R. Cole,\\nKai Hui et al. ”Gecko: Versatile text embeddings distilled from large la nguage\\nmodels, 2024.” URL https://arxiv. org/abs/2403.20327.\\n[52] Junseong Kim, Seolhwa Lee, Jihoon Kwon, Sangmo Gu, Yejin Kim, M inkyung'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 19, 'page_label': '20', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='models, 2024.” URL https://arxiv. org/abs/2403.20327.\\n[52] Junseong Kim, Seolhwa Lee, Jihoon Kwon, Sangmo Gu, Yejin Kim, M inkyung\\nCho, Jy yong Sohn, and Chanyeol Choi. Linq-embed-mistral: Elevat ing text re-\\ntrieval with improved gpt data through task-speciﬁc control and quality reﬁnement.\\nlinq ai research blog, 2024.\\n[53] https://huggingface.co/dunzhang/stella-large-zh-v3-1792d\\n20'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 20, 'page_label': '21', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\n[54] Tsatsaronis G, Balikas G, Malakasiotis P, et al. An overview of the BIOASQ large-\\nscale biomedical semantic indexing and question answering competitio n[J]. BMC\\nbioinformatics, 2015, 16(1): 138.\\n[55] Cui Y, Liu T, Che W, et al. A span-extraction dataset for Chines e machine reading\\ncomprehension[J]. arXiv preprint arXiv:1810.07366, 2018.\\n[56] Wang A, Singh A, Michael J, et al. GLUE: A multi-task benchmark a nd analysis\\nplatform for natural language understanding[J]. arXiv preprint ar Xiv:1804.07461,\\n2018.\\n[57] Yelp Dataset. Yelp Inc., [Year]. Available: https://www.yelp.com/dataset\\n[58] Maas A, Daly R E, Pham P T, et al. Learning word vectors for sent iment analy-\\nsis[C]//Proceedings of the 49th annual meeting of the association f or computational\\nlinguistics: Human language technologies. 2011: 142-150.\\n[59] Jack FitzGerald, Christopher Hench, Charith Peris, Scott Mac kie, Kay Rottmann,'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 20, 'page_label': '21', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='linguistics: Human language technologies. 2011: 142-150.\\n[59] Jack FitzGerald, Christopher Hench, Charith Peris, Scott Mac kie, Kay Rottmann,\\nAna Sanchez, Aaron Nash, Liam Urbach, Vishesh Kakarala, Richa Sin gh, Swetha\\nRanganath, Laurie Crist, Misha Britan, Wouter Leeuwis, Gokhan Tu r, and Prem\\nNatarajan. 2022. Massive: A 1m-example multilingual natural langu age understand-\\ning dataset with 51 typologically-diverse languages.\\n[60] Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre . 2012. Semeval-\\n2012 task 6: A pilot on semantic textual similarity. In * SEM 2012: The First\\nJoint Conference on Lexical and Computational Semantics–Volume 1: Proceedings\\nof the main conference and the shared task, and Volume 2: Procee dings of the Sixth\\nInternational Workshop on Semantic Evaluation (SemEval 2012), pages 385–393.\\n[61] Liu, Xin, Qingcai Chen, Chong Deng, Huajun Zeng, Jing Chen, Do ngfang Li,\\nand Buzhou Tang. ”Lcqmc: A large-scale chinese question matching corpus.” In'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 20, 'page_label': '21', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='[61] Liu, Xin, Qingcai Chen, Chong Deng, Huajun Zeng, Jing Chen, Do ngfang Li,\\nand Buzhou Tang. ”Lcqmc: A large-scale chinese question matching corpus.” In\\nProceedings of the 27th international conference on computatio nal linguistics, pp.\\n1952-1962. 2018.\\n[62] Yang, Yinfei, Yuan Zhang, Chris Tar, and Jason Baldridge. ”PAW S-X: A\\ncross-lingual adversarial dataset for paraphrase identiﬁcation .” arXiv preprint\\narXiv:1908.11828 (2019).\\n[63] Cer, Daniel, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and L ucia Specia.\\n”Semeval-2017 task 1: Semantic textual similarity-multilingual and c ross-lingual\\nfocused evaluation.” arXiv preprint arXiv:1708.00055 (2017).\\n[64] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh T iwary, Rangan\\nMajumder, and Li Deng. 2016. MS MARCO: A human generated mach ine read-\\ning comprehension dataset. In Proceedings of the Workshop on Co gnitive Com-\\nputation: Integrating neural and symbolic approaches 2016 co-lo cated with the'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 20, 'page_label': '21', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='ing comprehension dataset. In Proceedings of the Workshop on Co gnitive Com-\\nputation: Integrating neural and symbolic approaches 2016 co-lo cated with the\\n30th Annual Conference on Neural Information Processing Syst ems (NIPS 2016),\\nBarcelona, Spain, December 9, 2016, volume 1773 of CEUR Worksho p Proceedings.\\nCEUR-WS.org.\\n21'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 21, 'page_label': '22', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\n[65] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redﬁeld, Michael Collins , Ankur\\nParikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Ke nton Lee,\\net al. Natural questions: a benchmark for question answering res earch. Transactions\\nof the Association for Computational Linguistics, 7:453–466, 2019 .\\n[66] Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jaso n Weston, and\\nMichael Auli. 2019. ELI5: Long Form Question Answering. In Procee dings of\\nthe 57th Annual Meeting of the Association for Computational Ling uistics, pages\\n3558–3567, Florence, Italy. Association for Computational Lingu istics.\\n[67] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan\\nSalakhutdinov, and Christopher D. Manning. HotpotQA: A dataset for diverse,\\nexplainable multi-hop question answering. In Proceedings of the 201 8 Conference\\non Empirical Methods in Natural Language Processing, pp. 2369–2 380, Brussels,'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 21, 'page_label': '22', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='explainable multi-hop question answering. In Proceedings of the 201 8 Conference\\non Empirical Methods in Natural Language Processing, pp. 2369–2 380, Brussels,\\nBelgium, October-November 2018. Association for Computational Linguistics. doi:\\n10.18653/v1/D18-1259. URL https://aclanthology.org/D18-125 9.\\n[68] Xinyu Zhang, Nandan Thakur, Odunayo Ogundepo, Ehsan Kama lloo, David\\nAlfonso-Hermelo, Xiaoguang Li, Qun Liu, Mehdi Rezagholizadeh, and Jimmy Lin.\\nMiracl: A multilingual retrieval dataset covering 18 diverse language s. Transactions\\nof the Association for Computational Linguistics, 11:1114–1131, 2 023.\\n[69] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Per cy Liang.\\nSquad: 100,000+ questions for machine comprehension of text. ar Xiv preprint\\narXiv:1606.05250, 2016.\\n[70] James Thorne, Andreas Vlachos, Christos Christodoulopoulos , and Arpit Mit-\\ntal. Fever: a large-scale dataset for fact extraction and veriﬁca tion. arXiv preprint\\narXiv:1803.05355, 2018.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 21, 'page_label': '22', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='[70] James Thorne, Andreas Vlachos, Christos Christodoulopoulos , and Arpit Mit-\\ntal. Fever: a large-scale dataset for fact extraction and veriﬁca tion. arXiv preprint\\narXiv:1803.05355, 2018.\\n[71] Wei He, Kai Liu, Jing Liu, Yajuan Lyu, Shiqi Zhao, Xinyan Xiao, Yu an Liu,\\nYizhong Wang, Hua Wu, Qiaoqiao She, Xuan Liu, Tian Wu, and Haifeng Wa ng.\\n2018. DuReader: a Chinese Machine Reading Comprehension Datase t from Real-\\nworld Applications. In Proceedings of the Workshop on Machine Read ing for Ques-\\ntion Answering, pages 37–46, Melbourne, Australia. Association fo r Computational\\nLinguistics.\\n[72] Yichen Jiang, Shikha Bordia, Zheng Zhong, Charles Dognin, Mane esh Singh, and\\nMohit Bansal. 2020. HoVer: A Dataset for Many-Hop Fact Extract ion And Claim\\nVeriﬁcation. In Findings of the Association for Computational Lingu istics: EMNLP\\n2020, pages 3441–3460, Online. Association for Computational Lin guistics.\\n[73] Zhang X, Ma X, Shi P, et al. Mr. TyDi: A multi-lingual benchmark fo r dense'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 21, 'page_label': '22', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='2020, pages 3441–3460, Online. Association for Computational Lin guistics.\\n[73] Zhang X, Ma X, Shi P, et al. Mr. TyDi: A multi-lingual benchmark fo r dense\\nretrieval[J]. arXiv preprint arXiv:2108.08787, 2021.\\n[74] Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Danie l Weld. 2020.\\nS2ORC: The Semantic Scholar Open Research Corpus. In Proceedin gs of the 58th\\nAnnual Meeting of the Association for Computational Linguistics, p ages 4969–4983,\\nOnline. Association for Computational Linguistics.\\n22'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 22, 'page_label': '23', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\n[75] https://huggingface.co/spaces/mteb/leaderboard\\n[76] Jinhyuk Lee, Feiyang Chen, Sahil Dua, Daniel Cer, Madhuri Sha nbhogue, Iftekhar\\nNaim, Gustavo Hernandez /acute.ts1Abrego, Zhe Li, Kaifeng Chen, Henrique Schechter\\nVera, et al. Gemini embedding: Generalizable embeddings from gemini. arXiv\\npreprint arXiv:2503.07891, 2025b.\\nA Appendix\\nA.1 Framework Constraints\\nTable 4: Speciﬁcations of framework constraints\\nItem Explanation\\nKeep core semantics Preserving the core semantic content, which is the\\nmost critical requirement.\\nDiversity in morphology,\\nsyntax, grammar, tense,\\nrhetoric, etc\\nVariations in lexical composition, syntactic struc-\\nture, grammatical rules, and tense usage are per-\\nmitted.\\nLength within ±15% The length deviation from the original sentence\\nshould not exceed 15%.\\nKeep language The language used must be consistent with the\\noriginal sentence.\\nClose in ﬁeld The content must remain strictly aligned with the'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 22, 'page_label': '23', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='should not exceed 15%.\\nKeep language The language used must be consistent with the\\noriginal sentence.\\nClose in ﬁeld The content must remain strictly aligned with the\\ndomain of the given sentence.\\nTopic transfer, expansion,\\nextension, prohibiting pure\\nrewriting\\nTopic shifting, extension, or elaboration is permit-\\nted, but purely paraphrased content (identical to\\nthe original topic) is prohibited.\\nPOS is the perfect\\nanswer(necessary &\\nsuﬃcient)\\nPositive examples must be unambiguous and pre-\\ncisely address the query (necessity condition) while\\ncontaining exclusively relevant content without ex-\\ntraneous information (suﬃciency condition).\\nHard NEG: Worse than\\nPOS:\\n- Semantic deviation\\n(inadequate)\\n- Including irrelevant\\ninformation(unnecessary)\\n- Diﬀerent aspects of the\\nsame topic\\nHard negative examples must exhibit inferior qual-\\nity compared to positive instances, with noise in-\\ntroduced through three strategies: 1) semantic de-\\nviation (failing to accurately address the query),'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 22, 'page_label': '23', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='ity compared to positive instances, with noise in-\\ntroduced through three strategies: 1) semantic de-\\nviation (failing to accurately address the query),\\n2) incorporation of irrelevant information, or 3)\\nmaintaining the same topic but diverging in as-\\npects.\\nImitation: syntax, sentence\\nstructure, structural\\nGenerating hard negative examples by emulating\\nthe structural and syntactic patterns of the given\\npositive instance is a critical step to maximize dis-\\ncriminative challenge for the model.\\n23'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 23, 'page_label': '24', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nA.2 Instruction Examples\\nTable 5: Instruction for partial training data\\nDataset Instruction\\nHuatuo Given a medical question, retrieve user replies that\\nbest answer the question\\nReddit Retrieve the paragraph most semantically similar\\nto the given statement\\nLaw-GPT Retrieve relevant legal provisions or interpreta-\\ntions for the given case\\nMNLI/SNLI Retrieve semantically similar text\\nYelp Classify the customer review of businesses\\nWeibo Classify the sentiment of Weibo comments\\nA.3 Data Synthesis Examples\\nNote: The text highlighted in yellow represents the original sentence, fo llowed by the\\nsynthetically generated sentence.\\nTable 6: Paraphrasing Example (1)\\nquery pos\\nWhat is the best credit\\ncard for someone with no\\ncredit history?\\nIf you’ve never had a credit card before a likely\\nreason can be due to lack of credit history. You\\ncan apply for a department store card.\\nWhat’s the ideal credit\\ncard for a person without\\nany credit history?'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 23, 'page_label': '24', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='reason can be due to lack of credit history. You\\ncan apply for a department store card.\\nWhat’s the ideal credit\\ncard for a person without\\nany credit history?\\nIf you’ve never had a credit card, it’s probably\\nbecause you don’t have a credit history. A depart-\\nment store card could be a good option to apply\\nfor.\\nWhat’s the top credit card\\nchoice for someone who has\\nno credit history?\\nIf you’ve never owned a credit card, it’s probably\\nbecause you don’t have a credit history. A depart-\\nment store card might be a good option to con-\\nsider.\\n24'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 24, 'page_label': '25', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nTable 7: Paraphrasing Example (2)\\nquery pos\\nWhich English Poet\\nLaureate wrote ’The Faerie\\nQueene’?\\nEnglish Renaissance to begin, shakily, in the 1520s,\\nand it continued until perhaps 1620. England had\\na strong tradition of literature in the English ver-\\nnacular, which gradually increased as English use\\nof the printing press became common during the\\nmid 16th century. By the time of Elizabethan liter-\\nature a vigorous literary culture in both drama and\\npoetry included poets such as Edmund Spenser,\\nwhose verse epic ’The Faerie Queene’ had a strong\\ninﬂuence on English literature but was eventu-\\nally overshadowed by the lyrics of William Shake-\\nspeare, Thomas Wyatt and others. Typically, the\\nworks of these playwrights and poets circulated in\\nmanuscript form.\\nWho was the English Poet\\nLaureate that penned ’The\\nFaerie Queene’?\\nThe English Renaissance began, albeit slowly, in\\nthe 1520s and likely lasted until around 1620. Eng-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 24, 'page_label': '25', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='manuscript form.\\nWho was the English Poet\\nLaureate that penned ’The\\nFaerie Queene’?\\nThe English Renaissance began, albeit slowly, in\\nthe 1520s and likely lasted until around 1620. Eng-\\nland boasted a robust literary tradition in the En-\\nglish language, which expanded as the printing\\npress became more widely adopted during the mid-\\n16th century. By the time of Elizabethan litera-\\nture, a dynamic literary scene in both drama and\\npoetry emerged, featuring writers like Edmund\\nSpenser, whose epic poem The Faerie Queene sig-\\nniﬁcantly inﬂuenced English literature, although\\nit was eventually overshadowed by the works of\\nWilliam Shakespeare, Thomas Wyatt, and others.\\nGenerally, the writings of these poets and play-\\nwrights circulated in manuscript form.\\nWhich English Poet\\nLaureate authored ’The\\nFaerie Queene’?\\nThe English Renaissance began somewhat uncer-\\ntainly in the 1520s, continuing through to around\\n1620. England had a rich literary heritage in its'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 24, 'page_label': '25', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Laureate authored ’The\\nFaerie Queene’?\\nThe English Renaissance began somewhat uncer-\\ntainly in the 1520s, continuing through to around\\n1620. England had a rich literary heritage in its\\nnative tongue, which expanded as the printing\\npress became more widely used in the mid-16th\\ncentury. By the Elizabethan era, a thriving lit-\\nerary culture in both drama and poetry emerged,\\nwith poets like Edmund Spenser, whose epic The\\nFaerie Queene profoundly impacted English liter-\\nature, though it was eventually eclipsed by the\\nworks of William Shakespeare, Thomas Wyatt,\\nand others. Generally, the writings of these play-\\nwrights and poets were circulated in manuscript\\nform.\\n25'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 25, 'page_label': '26', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nTable 8: Augmentation Example\\nquery pos neg\\nWhat is the best\\ncredit card for\\nsomeone with no\\ncredit history?\\nIf you’ve never had a credit\\ncard before a likely reason\\ncan be due to lack of credit\\nhistory. You can apply for a\\ndepartment store card.\\n-\\nWhich credit card is\\neasiest to get\\napproved for with bad\\ncredit?\\nFor those with poor credit\\nscores, secured credit cards\\nfrom Capital One or Dis-\\ncover are often the most ac-\\ncessible options since they\\nrequire a refundable de-\\nposit.\\nSome premium travel cards\\nlike Chase Sapphire have\\ngreat rewards but usually\\nneed excellent credit. Store\\ncards may accept lower\\nscores but have high inter-\\nest rates.\\nWhat credit cards\\ncan I get as a college\\nstudent with no\\ncredit?\\nStudents without credit his-\\ntory often qualify for starter\\ncards like Discover it Stu-\\ndent or Capital One Jour-\\nney, which are designed to\\nhelp build credit.\\nPremium cards like Amer-\\nican Express Platinum re-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 25, 'page_label': '26', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='tory often qualify for starter\\ncards like Discover it Stu-\\ndent or Capital One Jour-\\nney, which are designed to\\nhelp build credit.\\nPremium cards like Amer-\\nican Express Platinum re-\\nquire good credit history,\\nthough some banks oﬀer\\nstudent accounts with debit\\ncards.\\nWhich English Poet\\nLaureate wrote ’The\\nFaerie Queene’?\\n...By the time of Eliz-\\nabethan literature a vig-\\norous literary culture in\\nboth drama and poetry in-\\ncluded poets such as Ed-\\nmund Spenser, whose verse\\nepic ’The Faerie Queene’\\nhad a strong inﬂuence on\\nEnglish literature but was\\neventually overshadowed by\\nthe lyrics of William ...\\n-\\nWhat major epic\\npoem did Edmund\\nSpenser write during\\nQueen Elizabeth’s\\nreign?\\nEdmund Spenser composed\\n’The Faerie Queene’, an\\nallegorical epic poem that\\nbecame one of the most\\nsigniﬁcant works of Eliz-\\nabethan literature though\\nlater eclipsed by Shake-\\nspeare’s popularity.\\nChristopher Marlowe’s\\n’Hero and Leander’ was an-\\nother notable Elizabethan\\npoem, but unlike Spenser’s'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 25, 'page_label': '26', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='signiﬁcant works of Eliz-\\nabethan literature though\\nlater eclipsed by Shake-\\nspeare’s popularity.\\nChristopher Marlowe’s\\n’Hero and Leander’ was an-\\nother notable Elizabethan\\npoem, but unlike Spenser’s\\nwork it wasn’t an epic\\nallegory.\\nWhich poet created\\n’Paradise Lost’ during\\nthe English\\nRenaissance?\\nJohn Milton authored the\\nepic poem ’Paradise Lost’\\nin the 17th century, a mon-\\numental work that explored\\nbiblical themes through\\nblank verse and became\\na cornerstone of English\\nliterature.\\nWilliam Blake’s ’The Mar-\\nriage of Heaven and Hell’\\nalso dealt with religious\\nthemes, though it was more\\nprophetic than epic in style\\ncompared to Milton’s mas-\\nterpiece.\\n26'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 26, 'page_label': '27', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nTable 9: Hard-Negative Generation Example\\nquery pos neg\\nWhat territory was\\nKing Hussein afraid\\nIsrael would obtain?\\n...Hussein was nonetheless\\nwary that an Egyptian-\\nIsraeli war would risk the\\nWest Bank’s occupation by\\nIsrael...\\n-\\nWhat territory was\\nKing Hussein afraid\\nIsrael would obtain?\\n...Hussein was nonetheless\\nwary that an Egyptian-\\nIsraeli war would risk the\\nWest Bank’s occupation by\\nIsrael...\\nKing Hussein expressed\\nconcerns about potential\\nIsraeli expansion during\\nthe Arab-Israeli conﬂicts,\\nthough his warnings to\\nNasser were delayed and\\ninitially dismissed, while\\nother Arab leaders focused\\nmore on direct military\\npreparations against Israel.\\nWhat territory was\\nKing Hussein afraid\\nIsrael would obtain?\\n...Hussein was nonetheless\\nwary that an Egyptian-\\nIsraeli war would risk the\\nWest Bank’s occupation by\\nIsrael...\\nKing Hussein expressed\\nconcerns about potential\\nIsraeli territorial expansion\\nduring the 1967 tensions,'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 26, 'page_label': '27', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='wary that an Egyptian-\\nIsraeli war would risk the\\nWest Bank’s occupation by\\nIsrael...\\nKing Hussein expressed\\nconcerns about potential\\nIsraeli territorial expansion\\nduring the 1967 tensions,\\nthough his warnings were\\ndelayed in reaching Nasser\\nand mixed with broader\\nregional tensions, while\\nEgyptian military move-\\nments in Sinai were already\\nunderway under Amer’s\\norders.\\n27'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Title: A Survey of Object Detection: From Region Proposals to End-to-End \\nTransformers \\nAuthor: [Your Name/Institution] Date: October 2025 \\n \\nAbstract \\nObject detection, a fundamental task in computer vision, involves identifying and localizing \\ninstances of objects within an image or video. It goes beyond simple image classification by not only \\ndetermining the class of an object but also providing a bounding box that precisely outlines its \\nlocation. This paper provides a comprehensive survey of the evolution of object detection \\nmethodologies, primarily focusing on the deep learning era. We begin by contextualizing the \\nproblem with a brief overview of traditional computer vision techniques. The core of the review is \\ndedicated to the two dominant paradigms in deep learning-based detectors: two-stage detectors, \\nexemplified by the R-CNN family (R-CNN, Fast R-CNN, Faster R-CNN), which prioritize accuracy'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='dedicated to the two dominant paradigms in deep learning-based detectors: two-stage detectors, \\nexemplified by the R-CNN family (R-CNN, Fast R-CNN, Faster R-CNN), which prioritize accuracy \\nthrough a region proposal mechanism; and single-stage detectors, such as YOLO and SSD, which \\noptimize for speed by performing detection in a single pass. We then explore key architectural \\ncomponents like backbone networks, anchor boxes, and non-maximum suppression. The survey \\nculminates with a discussion of modern architectures, including the paradigm-shifting DETR \\n(DEtection TRansformer), which reframes object detection as an end-to-end set prediction problem. \\nFinally, we cover standard evaluation metrics, common datasets, real-world applications, and the \\nongoing challenges and future directions that are shaping the field. \\nKeywords: Object Detection, Computer Vision, Deep Learning, R-CNN, YOLO, SSD, Transformer, \\nDETR, Bounding Box, mAP.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Table of Contents \\n1. Introduction 1.1. Defining Object Detection: Classification and Localization 1.2. Distinction \\nfrom Other Vision Tasks 1.3. The Importance of Object Detection 1.4. Paper Structure \\n2. Background and Foundational Concepts 2.1. Traditional Computer Vision Approaches \\n(Viola-Jones, HOG) 2.2. The Sliding Window Method 2.3. The Deep Learning Revolution \\n3. Two-Stage Object Detectors: A Focus on Accuracy 3.1. The \"Propose, then Classify\" \\nParadigm 3.2. R-CNN: Regions with CNN Features 3.3. Fast R-CNN: Sharing Computation \\n3.4. Faster R-CNN: The Region Proposal Network (RPN) \\n4. Single-Stage Object Detectors: A Focus on Speed 4.1. The \"Single Pass\" Paradigm 4.2. \\nYOLO: You Only Look Once 4.3. SSD: Single Shot MultiBox Detector \\n5. Key Architectural Components and Innovations 5.1. Backbone Networks: The Feature \\nExtractors 5.2. Anchor Boxes: The Priors for Prediction 5.3. Non-Maximum Suppression \\n(NMS): Pruning Redundant Detections'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Extractors 5.2. Anchor Boxes: The Priors for Prediction 5.3. Non-Maximum Suppression \\n(NMS): Pruning Redundant Detections \\n6. Modern Architectures and the Rise of Transformers 6.1. Balancing Speed and Accuracy: \\nEfficientDet 6.2. DETR: End-to-End Object Detection with Transformers \\n7. Evaluation Metrics and Datasets 7.1. Intersection over Union (IoU) 7.2. Average Precision \\n(AP) and mean Average Precision (mAP) 7.3. Landmark Datasets (PASCAL VOC, COCO) \\n8. Applications and Real-World Use Cases 8.1. Autonomous Vehicles 8.2. Medical Imaging \\n8.3. Retail and Inventory Management 8.4. Security and Surveillance \\n9. Challenges and Future Directions 9.1. Detecting Small and Occluded Objects 9.2. The \\nSpeed vs. Accuracy Trade-off 9.3. Domain Adaptation and Generalization 9.4. Few-Shot and \\nZero-Shot Detection \\n10. Conclusion \\n11. References'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='1. Introduction \\n1.1. Defining Object Detection: Classification and Localization \\nObject detection is a core computer vision task concerned with answering two fundamental questions \\nabout an image: \"What objects are in this image?\" and \"Where are they located?\". The first \\nquestion is a classification task, assigning a class label (e.g., \"cat,\" \"car,\" \"person\") to an object. The \\nsecond is a localization task, providing a tight-fitting bounding box (typically defined by x/y \\ncoordinates and width/height) around each identified object. \\n1.2. Distinction from Other Vision Tasks \\nIt is crucial to distinguish object detection from related tasks: \\n• Image Classification: Simply assigns one label to an entire image (e.g., \"this is a picture of a \\ncat\"). \\n• Semantic Segmentation: Assigns a class label to every pixel in the image but does not \\ndistinguish between different instances of the same object (e.g., all pixels belonging to any \\nperson are labeled \"person\").'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='distinguish between different instances of the same object (e.g., all pixels belonging to any \\nperson are labeled \"person\"). \\n• Instance Segmentation: Assigns a class label to every pixel and differentiates between \\nobject instances (e.g., \"person 1,\" \"person 2,\" \"person 3\"). Object detection can be seen as a \\nprecursor to this more complex task. \\n1.3. The Importance of Object Detection \\nThe ability to detect and locate objects is foundational to how machines perceive and interact with \\nthe physical world. It is the technology that enables self-driving cars to see pedestrians and other \\nvehicles, allows doctors to identify tumors in medical scans, and helps robots navigate complex \\nenvironments. Its broad applicability has made it one of the most actively researched areas in \\nartificial intelligence. \\n1.4. Paper Structure \\nThis paper will trace the evolution of object detection methods, beginning with a brief look at pre-'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='artificial intelligence. \\n1.4. Paper Structure \\nThis paper will trace the evolution of object detection methods, beginning with a brief look at pre-\\ndeep learning techniques. We will then delve into the two primary families of deep learning'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='detectors: two-stage and single-stage. We will discuss their core components, modern architectures \\nincluding Transformers, and conclude with evaluation metrics, applications, and future challenges. \\n \\n2. Background and Foundational Concepts \\n2.1. Traditional Computer Vision Approaches \\nBefore deep learning, object detection relied on hand-crafted features. Methods like the Viola-Jones \\nframework (famous for real-time face detection) used simple Haar-like features and a cascade of \\nclassifiers. Other approaches used more complex feature descriptors like HOG (Histogram of \\nOriented Gradients), often paired with a classifier like a Support Vector Machine (SVM), to \\nidentify objects. These methods were effective for specific tasks but were brittle and did not \\ngeneralize well. \\n2.2. The Sliding Window Method \\nA common technique was the sliding window approach. A window of a fixed size would be slid'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='generalize well. \\n2.2. The Sliding Window Method \\nA common technique was the sliding window approach. A window of a fixed size would be slid \\nacross all possible locations and scales of an image. For each window, a feature descriptor would be \\ncomputed and fed to a classifier. This method was computationally exhaustive and prone to errors. \\n2.3. The Deep Learning Revolution \\nThe success of AlexNet in the 2012 ImageNet classification challenge marked a turning point. \\nResearchers quickly realized that the rich, hierarchical features learned automatically by \\nConvolutional Neural Networks (CNNs) were far more powerful than any hand-crafted features. \\nThis discovery paved the way for the modern era of object detection. \\n \\n3. Two-Stage Object Detectors: A Focus on Accuracy \\nTwo-stage detectors break the object detection problem into two distinct steps, a paradigm that \\ngenerally leads to higher accuracy at the cost of speed. \\n3.1. The \"Propose, then Classify\" Paradigm'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content=\"The core idea is to first generate a sparse set of region proposals—areas of the image that are likely \\nto contain an object. In the second stage, a classifier is run only on these proposed regions to \\ndetermine the object's class and refine the bounding box. \\n3.2. R-CNN: Regions with CNN Features \\nR-CNN was the first major breakthrough in applying deep learning to this paradigm. However, its \\nprocess was slow and cumbersome: \\n1. Generate ~2000 region proposals using an external algorithm like Selective Search. \\n2. Warp/resize each proposed region to a fixed size. \\n3. Pass each warped region independently through a pre-trained CNN to extract features. \\n4. Use a set of SVMs to classify the object in each region. \\n3.3. Fast R-CNN: Sharing Computation \\nFast R-CNN made a significant improvement. Instead of running the CNN 2000 times, it passes the \\nentire image through the CNN just once to generate a feature map. The region proposals are then\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Fast R-CNN made a significant improvement. Instead of running the CNN 2000 times, it passes the \\nentire image through the CNN just once to generate a feature map. The region proposals are then \\nprojected onto this feature map. A novel RoI (Region of Interest) Pooling layer extracts a fixed-size \\nfeature vector from each proposed region, which is then fed into a classifier. This shared computation \\nmade the process much faster. \\n3.4. Faster R-CNN: The Region Proposal Network (RPN) \\nThe bottleneck in Fast R-CNN was the external Selective Search algorithm for proposing regions. \\nFaster R-CNN introduced the Region Proposal Network (RPN), a small neural network that learns \\nto generate high-quality region proposals directly from the CNN features. By integrating the RPN, \\nFaster R-CNN became the first truly end-to-end, unified deep learning object detector, setting a new \\nstandard for accuracy. \\n \\n4. Single-Stage Object Detectors: A Focus on Speed'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Faster R-CNN became the first truly end-to-end, unified deep learning object detector, setting a new \\nstandard for accuracy. \\n \\n4. Single-Stage Object Detectors: A Focus on Speed \\nSingle-stage detectors remove the region proposal step and instead perform localization and \\nclassification in a single forward pass of the network, making them extremely fast and suitable for \\nreal-time applications.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='4.1. The \"Single Pass\" Paradigm \\nThese models treat object detection as a regression problem. They look at the image once and \\ndirectly predict a set of bounding boxes and their corresponding class probabilities. \\n4.2. YOLO: You Only Look Once \\nThe YOLO family of models is renowned for its speed. YOLO divides the input image into a grid. \\nFor each grid cell, the model simultaneously predicts: \\n• Several bounding boxes. \\n• A \"confidence\" score for each box, indicating how likely it is to contain an object. \\n• Class probabilities for the object within the box. This unified architecture allows for end-to-\\nend training and blazingly fast inference speeds, making it ideal for video processing. \\n4.3. SSD: Single Shot MultiBox Detector \\nSSD aimed to find a middle ground between the speed of YOLO and the accuracy of Faster R-CNN. \\nIts key innovation is using feature maps from multiple layers of the backbone network to make'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='SSD aimed to find a middle ground between the speed of YOLO and the accuracy of Faster R-CNN. \\nIts key innovation is using feature maps from multiple layers of the backbone network to make \\npredictions. By making predictions at different scales, SSD is much better at detecting objects of \\nvarious sizes, particularly small ones, compared to the original YOLO. \\n \\n5. Key Architectural Components and Innovations \\nModern detectors, whether two-stage or single-stage, share several common components. \\n5.1. Backbone Networks: The Feature Extractors \\nThe backbone is a deep CNN (like ResNet, VGG, or MobileNet) pre-trained on a large image \\nclassification dataset (e.g., ImageNet). Its role is to act as a powerful feature extractor, converting the \\nraw pixel data of an image into rich, hierarchical feature maps that can be used for detection. \\n5.2. Anchor Boxes: The Priors for Prediction \\nInstead of predicting bounding boxes from scratch, most detectors predict offsets relative to a set of'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='5.2. Anchor Boxes: The Priors for Prediction \\nInstead of predicting bounding boxes from scratch, most detectors predict offsets relative to a set of \\npre-defined default boxes called anchor boxes. These anchors have various sizes and aspect ratios'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='and are tiled across the image at different locations. Using anchors reframes the problem from \\npredicting absolute coordinates to refining a well-placed prior, which makes learning easier for the \\nnetwork. \\n5.3. Non-Maximum Suppression (NMS): Pruning Redundant Detections \\nA detector will often output multiple, highly overlapping bounding boxes for the same object. NMS \\nis a crucial post-processing step that cleans up these redundant detections. It sorts all boxes by their \\nconfidence scores, keeps the box with the highest score, and suppresses (discards) any other boxes \\nthat have a high overlap with it. \\n \\n6. Modern Architectures and the Rise of Transformers \\n6.1. Balancing Speed and Accuracy: EfficientDet \\nThe EfficientDet family of models introduced a systematic way to scale detectors for different \\nresource constraints. It uses a highly efficient backbone (EfficientNet) and a novel feature fusion'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='The EfficientDet family of models introduced a systematic way to scale detectors for different \\nresource constraints. It uses a highly efficient backbone (EfficientNet) and a novel feature fusion \\nmechanism (BiFPN) to achieve state-of-the-art efficiency, balancing high accuracy with low \\ncomputational cost. \\n6.2. DETR: End-to-End Object Detection with Transformers \\nDETR (DEtection TRansformer) represents a major paradigm shift. It completely eliminates the need \\nfor hand-crafted components like anchor boxes and NMS. DETR uses a standard Transformer \\nencoder-decoder architecture, similar to those used in NLP. It treats object detection as a direct set \\nprediction problem: the model ingests image features and directly outputs the final set of unique \\nobject detections. This simplifies the detection pipeline significantly and has opened up a new and \\nexciting research direction. \\n \\n7. Evaluation Metrics and Datasets \\n7.1. Intersection over Union (IoU)'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='IoU is the fundamental metric used to measure the \"correctness\" of a predicted bounding box. It is \\ncalculated as the area of overlap between the predicted box and the ground-truth box, divided by the \\narea of their union. A detection is typically considered a \"true positive\" if its IoU with a ground-truth \\nbox is above a certain threshold (e.g., 0.5). \\n7.2. Average Precision (AP) and mean Average Precision (mAP) \\nAverage Precision (AP) is the primary metric for evaluating the performance of a detector on a \\nsingle object class. It is calculated from the precision-recall curve and effectively measures the \\ndetector\\'s accuracy across all confidence levels. Mean Average Precision (mAP) is the average of \\nthe AP values across all object classes and is the standard metric for comparing different object \\ndetection models. \\n7.3. Landmark Datasets \\nThe field has been driven by large-scale, publicly available datasets, most notably PASCAL VOC'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='detection models. \\n7.3. Landmark Datasets \\nThe field has been driven by large-scale, publicly available datasets, most notably PASCAL VOC \\nand Microsoft COCO (Common Objects in Context). The COCO dataset, with its large number of \\nobject categories and instances per image, is the current benchmark for modern object detectors. \\n \\n8. Applications and Real-World Use Cases \\nObject detection is a deployed and impactful technology across numerous industries. \\n• Autonomous Vehicles: Detecting cars, pedestrians, cyclists, and traffic signals is essential \\nfor safe navigation. \\n• Medical Imaging: Assisting radiologists by automatically locating tumors, lesions, or other \\nanomalies in X-rays, CT scans, and MRIs. \\n• Retail: Powering cashier-less stores, monitoring shelf inventory, and analyzing customer foot \\ntraffic. \\n• Security and Surveillance: Automatically detecting intruders, unattended baggage, or \\nmonitoring crowd density. \\n \\n9. Challenges and Future Directions'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Despite immense progress, several challenges remain. \\n• Detecting Small and Occluded Objects: Models still struggle to reliably detect objects that \\nare very small, far away, or partially hidden. \\n• The Speed vs. Accuracy Trade-off: While models are becoming more efficient, the \\nfundamental trade-off between real-time speed and maximum accuracy remains a key design \\nconsideration. \\n• Domain Adaptation and Generalization: A model trained on daytime, sunny weather data \\nmay fail when deployed at night or in the rain. Improving robustness to new environments is \\na major challenge. \\n• Few-Shot and Zero-Shot Detection: Training models to detect new object categories with \\nvery few (or zero) labeled examples is an active and important area of research.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content=\"10. Conclusion \\nObject detection has undergone a remarkable transformation, moving from slow, brittle systems \\nbased on hand-crafted features to highly accurate and efficient end-to-end deep learning models. The \\nevolution from the methodical two-stage R-CNN family to the rapid single-stage YOLO and SSD \\ndetectors, and now to the elegant, anchor-free Transformer-based models like DETR, showcases the \\nfield's rapid pace of innovation. As a core enabling technology for machine perception, object \\ndetection continues to solve critical real-world problems and will undoubtedly remain a central focus \\nof AI research for years to come. \\n \\n11. References \\n[Viola & Jones, 2001] Viola, P., & Jones, M. (2001). Rapid object detection using a boosted cascade \\nof simple features. Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision \\nand Pattern Recognition. \\n[Girshick et al., 2014] Girshick, R., Donahue, J., Darrell, T., & Malik, J. (2014). Rich feature\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='and Pattern Recognition. \\n[Girshick et al., 2014] Girshick, R., Donahue, J., Darrell, T., & Malik, J. (2014). Rich feature \\nhierarchies for accurate object detection and semantic segmentation. Proceedings of the IEEE \\nconference on computer vision and pattern recognition. \\n[Ren et al., 2015] Ren, S., He, K., Girshick, R., & Sun, J. (2015). Faster R-CNN: Towards real-time \\nobject detection with region proposal networks. Advances in neural information processing systems. \\n[Redmon et al., 2016] Redmon, J., Divvala, S., Girshick, R., & Farhadi, A. (2016). You only look \\nonce: Unified, real-time object detection. Proceedings of the IEEE conference on computer vision \\nand pattern recognition. \\n[Liu et al., 2016] Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C. Y., & Berg, A. C. \\n(2016). SSD: Single shot multibox detector. European conference on computer vision. \\n[Carion et al., 2020] Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., & Zagoruyko, S.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='(2016). SSD: Single shot multibox detector. European conference on computer vision. \\n[Carion et al., 2020] Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., & Zagoruyko, S. \\n(2020). End-to-end object detection with transformers. European conference on computer vision.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:54:40+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:54:40+01:00', 'source': '..\\\\data\\\\pdf\\\\proposal.pdf', 'total_pages': 8, 'page': 0, 'page_label': '1', 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='Title: Self-Training with Uncertainty-Aware Style Transfer for Cross-Domain \\nObject Detection \\nPrincipal Investigator: [Your Name] Affiliation: [Your Institution/Research Group] Date: October \\n15, 2025 \\n \\nAbstract \\nModern object detection models achieve remarkable performance but suffer a significant drop in \\naccuracy when deployed in environments (target domains) that differ from their training data (source \\ndomain). This problem of domain shift is a major obstacle to the real-world application of \\ntechnologies like autonomous driving, where a vehicle must operate reliably in diverse weather, \\nlighting, and geographic conditions. This proposal outlines a research project to develop a novel \\nframework for unsupervised domain adaptation in object detection. We propose a method that \\ncombines generative style transfer with a robust self-training mechanism. Specifically, we will use a \\nCycle-Consistent Generative Adversarial Network (CycleGAN) to translate images between'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:54:40+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:54:40+01:00', 'source': '..\\\\data\\\\pdf\\\\proposal.pdf', 'total_pages': 8, 'page': 0, 'page_label': '1', 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='combines generative style transfer with a robust self-training mechanism. Specifically, we will use a \\nCycle-Consistent Generative Adversarial Network (CycleGAN) to translate images between \\ndomains, artificially augmenting the training data. More importantly, we will enhance a self-training \\npipeline by incorporating uncertainty estimation. By using techniques like Monte Carlo Dropout, \\nour model will only leverage pseudo-labels from the target domain in which it has high confidence, \\npreventing the accumulation of errors from incorrect labels. We hypothesize that this uncertainty-\\naware approach will make the self-training process more stable and effective, leading to a significant \\nimprovement in object detection performance in unseen target domains. The proposed research will \\nbe evaluated on benchmark datasets like Cityscapes and Foggy Cityscapes, with the goal of creating \\nmore robust and reliable perception systems.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:54:40+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:54:40+01:00', 'source': '..\\\\data\\\\pdf\\\\proposal.pdf', 'total_pages': 8, 'page': 0, 'page_label': '1', 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='be evaluated on benchmark datasets like Cityscapes and Foggy Cityscapes, with the goal of creating \\nmore robust and reliable perception systems. \\nKeywords: Research Proposal, Object Detection, Domain Adaptation, Self-Training, Uncertainty \\nEstimation, Style Transfer, Autonomous Vehicles.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:54:40+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:54:40+01:00', 'source': '..\\\\data\\\\pdf\\\\proposal.pdf', 'total_pages': 8, 'page': 1, 'page_label': '2', 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='Table of Contents \\n1. Introduction and Problem Statement 1.1. The Success and Brittleness of Modern Detectors \\n1.2. The Challenge of Domain Shift 1.3. Research Questions and Objectives 1.4. Proposed \\nContribution \\n2. Literature Review 2.1. State-of-the-Art Object Detection Models 2.2. Unsupervised Domain \\nAdaptation (UDA) 2.3. UDA Techniques in Object Detection 2.3.1. Adversarial Training \\nMethods 2.3.2. Style Transfer and Image-to-Image Translation 2.3.3. Self-Training and \\nPseudo-Labeling \\n3. Proposed Methodology 3.1. Overall Framework Architecture 3.2. Module 1: Cross-Domain \\nStyle Transfer 3.3. Module 2: Self-Training with Pseudo-Labeling 3.4. The Core Innovation: \\nUncertainty-Aware Label Filtering \\n4. Experimental Setup and Evaluation 4.1. Datasets and Benchmarks 4.2. Baseline Models \\nfor Comparison 4.3. Evaluation Metrics 4.4. Implementation Details \\n5. Expected Results and Broader Impact 5.1. Hypothesized Performance Gains 5.2. Impact'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:54:40+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:54:40+01:00', 'source': '..\\\\data\\\\pdf\\\\proposal.pdf', 'total_pages': 8, 'page': 1, 'page_label': '2', 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='for Comparison 4.3. Evaluation Metrics 4.4. Implementation Details \\n5. Expected Results and Broader Impact 5.1. Hypothesized Performance Gains 5.2. Impact \\non Autonomous Systems and Robotics 5.3. Contribution to the Field of AI \\n6. Plan of Work and Timeline 6.1. Phase 1: Literature Review and Environment Setup \\n(Months 1-2) 6.2. Phase 2: Implementation of Core Modules (Months 3-6) 6.3. Phase 3: \\nExperimentation and Analysis (Months 7-10) 6.4. Phase 4: Dissemination of Results (Months \\n11-12) \\n7. Ethical Considerations \\n8. Conclusion \\n9. References'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:54:40+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:54:40+01:00', 'source': '..\\\\data\\\\pdf\\\\proposal.pdf', 'total_pages': 8, 'page': 2, 'page_label': '3', 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='1. Introduction and Problem Statement \\n1.1. The Success and Brittleness of Modern Detectors \\nDeep learning-based object detectors like Faster R-CNN and YOLO have become incredibly \\naccurate, forming the perception backbone for many emerging technologies. However, their success \\nis predicated on the assumption that the training and testing data are drawn from the same statistical \\ndistribution. \\n1.2. The Challenge of Domain Shift \\nIn the real world, this assumption is frequently violated. A model trained exclusively on clear, sunny \\nday driving data may fail catastrophically when deployed at night, in the rain, or in a city with \\ndifferent architecture. This phenomenon is known as domain shift. Manually annotating data for \\nevery possible domain is prohibitively expensive and unscalable. Therefore, Unsupervised Domain \\nAdaptation (UDA), which aims to adapt a model trained on a labeled source domain to an unlabeled \\ntarget domain, is a critical area of research.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:54:40+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:54:40+01:00', 'source': '..\\\\data\\\\pdf\\\\proposal.pdf', 'total_pages': 8, 'page': 2, 'page_label': '3', 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='Adaptation (UDA), which aims to adapt a model trained on a labeled source domain to an unlabeled \\ntarget domain, is a critical area of research. \\n1.3. Research Questions and Objectives \\nThis research aims to answer the following question: How can we make an object detector robust to \\ndomain shift without requiring any labeled data from the new domain? Our primary objectives are: \\n1. To design a framework that leverages both image-level style translation and model-level self-\\ntraining. \\n2. To develop a novel mechanism to filter noisy pseudo-labels generated during self-training by \\nestimating model uncertainty. \\n3. To empirically validate the proposed framework and demonstrate its superiority over existing \\nUDA methods. \\n1.4. Proposed Contribution \\nThe main contribution of this work will be an uncertainty-aware self-training framework. While \\nself-training is a common technique, it is often unstable because the model can reinforce its own'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:54:40+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:54:40+01:00', 'source': '..\\\\data\\\\pdf\\\\proposal.pdf', 'total_pages': 8, 'page': 2, 'page_label': '3', 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='The main contribution of this work will be an uncertainty-aware self-training framework. While \\nself-training is a common technique, it is often unstable because the model can reinforce its own \\nmistakes by trusting incorrect \"pseudo-labels.\" By introducing a principled mechanism for the model'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:54:40+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:54:40+01:00', 'source': '..\\\\data\\\\pdf\\\\proposal.pdf', 'total_pages': 8, 'page': 3, 'page_label': '4', 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='to gauge its own uncertainty, we can select only the most reliable pseudo-labels, leading to more \\nstable and effective adaptation. \\n \\n2. Literature Review \\n2.1. State-of-the-Art Object Detection Models \\nOur work will build upon established object detection architectures. We will consider both a two-\\nstage detector (e.g., Faster R-CNN) and a Transformer-based detector (e.g., DETR) as the base \\nmodels for our adaptation framework. \\n2.2. Unsupervised Domain Adaptation (UDA) \\nUDA is a well-established field in machine learning. The central goal is to leverage a label-rich \\nsource domain to learn a task in a label-scarce target domain. \\n2.3. UDA Techniques in Object Detection \\n• Adversarial Training: These methods use a \"domain discriminator\" network that tries to \\ndistinguish between features from the source and target domains. The main network is then \\ntrained to produce features that can \"fool\" this discriminator, thereby learning domain-\\ninvariant features.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:54:40+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:54:40+01:00', 'source': '..\\\\data\\\\pdf\\\\proposal.pdf', 'total_pages': 8, 'page': 3, 'page_label': '4', 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='trained to produce features that can \"fool\" this discriminator, thereby learning domain-\\ninvariant features. \\n• Style Transfer: Generative models like GANs are used to translate images from the source \\nstyle to the target style (e.g., making a sunny image look foggy). This creates a synthetic \\nlabeled dataset in the target style. \\n• Self-Training: This involves using an initial model to make predictions on the unlabeled \\ntarget data. The most confident predictions are then treated as \"pseudo-labels\" and are used to \\nretrain the model. This approach is powerful but risks error accumulation if the pseudo-labels \\nare noisy. Our proposed work directly addresses this key limitation. \\n \\n3. Proposed Methodology \\n3.1. Overall Framework Architecture'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:54:40+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:54:40+01:00', 'source': '..\\\\data\\\\pdf\\\\proposal.pdf', 'total_pages': 8, 'page': 4, 'page_label': '5', 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='The proposed system will consist of three interconnected modules operating on a base object \\ndetector. The model will be trained on labeled source data (e.g., sunny images) and unlabeled target \\ndata (e.g., rainy images). \\n3.2. Module 1: Cross-Domain Style Transfer \\nWe will first train a CycleGAN model to learn the mappings between the source and target domains. \\nThis will allow us to translate a source image into a \"fake\" target image (e.g., sunny -> rainy) and \\nvice-versa. This provides a basic form of data augmentation, allowing the detector to see labeled \\nimages that look like they are from the target domain. \\n3.3. Module 2: Self-Training with Pseudo-Labeling \\nIn parallel, we will use the model trained on the source data to generate predictions (bounding boxes \\nand classes) for the unlabeled target domain images. These predictions will serve as initial pseudo-\\nlabels. \\n3.4. The Core Innovation: Uncertainty-Aware Label Filtering'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:54:40+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:54:40+01:00', 'source': '..\\\\data\\\\pdf\\\\proposal.pdf', 'total_pages': 8, 'page': 4, 'page_label': '5', 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content=\"and classes) for the unlabeled target domain images. These predictions will serve as initial pseudo-\\nlabels. \\n3.4. The Core Innovation: Uncertainty-Aware Label Filtering \\nThis is the central component of our proposal. Instead of naively trusting all pseudo-labels above a \\nsimple confidence threshold, we will estimate the model's uncertainty for each prediction. We will \\nuse Monte Carlo Dropout, a technique where dropout is applied at inference time over multiple \\nforward passes. The variance in the resulting predictions serves as a strong indicator of model \\nuncertainty. We will then filter the pseudo-labels using a combined score of confidence and low \\nuncertainty. Only the most certain and confident pseudo-labels will be added to a replay buffer used \\nto fine-tune the detector, making the adaptation process robust to noise. \\n \\n4. Experimental Setup and Evaluation \\n4.1. Datasets and Benchmarks\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:54:40+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:54:40+01:00', 'source': '..\\\\data\\\\pdf\\\\proposal.pdf', 'total_pages': 8, 'page': 4, 'page_label': '5', 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='to fine-tune the detector, making the adaptation process robust to noise. \\n \\n4. Experimental Setup and Evaluation \\n4.1. Datasets and Benchmarks \\nWe will focus on autonomous driving scenarios. The primary experiment will be adapting from the \\nCityscapes dataset (clear weather) to the Foggy Cityscapes dataset. We will also evaluate on other \\ncommon shifts, such as adapting from synthetic data (Sim10k) to real-world data (KITTI).'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:54:40+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:54:40+01:00', 'source': '..\\\\data\\\\pdf\\\\proposal.pdf', 'total_pages': 8, 'page': 5, 'page_label': '6', 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='4.2. Baseline Models for Comparison \\nWe will compare our method against three baselines: \\n1. A \"Lower Bound\" model trained only on source data. \\n2. A state-of-the-art adversarial training method for UDA. \\n3. A standard self-training method without uncertainty awareness. \\n4.3. Evaluation Metrics \\nPerformance will be measured using the standard object detection metric, mean Average Precision \\n(mAP), calculated on the labeled validation set of the target domain. \\n \\n5. Expected Results and Broader Impact \\n5.1. Hypothesized Performance Gains \\nWe expect our uncertainty-aware framework to significantly outperform the baselines. We \\nhypothesize that by reducing the noise in the pseudo-labeling process, our model will adapt more \\neffectively, resulting in a 5-10% absolute improvement in mAP on the target domain compared to \\nstandard self-training methods. \\n5.2. Impact on Autonomous Systems and Robotics \\nA more robust perception system directly translates to increased safety and reliability for'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:54:40+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:54:40+01:00', 'source': '..\\\\data\\\\pdf\\\\proposal.pdf', 'total_pages': 8, 'page': 5, 'page_label': '6', 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='standard self-training methods. \\n5.2. Impact on Autonomous Systems and Robotics \\nA more robust perception system directly translates to increased safety and reliability for \\nautonomous vehicles, drones, and industrial robots. This research could help bridge the gap between \\ndevelopment and real-world deployment of these technologies. \\n \\n6. Plan of Work and Timeline \\nThe project is planned for a 12-month period. \\n• Phase 1 (Months 1-2): In-depth literature review; setting up the computational environment \\nand baseline models.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:54:40+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:54:40+01:00', 'source': '..\\\\data\\\\pdf\\\\proposal.pdf', 'total_pages': 8, 'page': 6, 'page_label': '7', 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='• Phase 2 (Months 3-6): Implementation of the style transfer module and the uncertainty-\\naware self-training loop. \\n• Phase 3 (Months 7-10): Conducting extensive experiments on benchmark datasets, \\nanalyzing results, and performing ablation studies. \\n• Phase 4 (Months 11-12): Writing a research paper for submission to a top-tier computer \\nvision conference (e.g., CVPR, ICCV) and finalizing the project report. \\n \\n7. Ethical Considerations \\nThe primary application of this research is to enhance safety in autonomous systems. However, \\nobject detection technology can also be used for surveillance. Our research will be conducted \\ntransparently, and we will focus our evaluation on publicly available datasets related to driving. We \\nwill not use private or personally identifiable data. The code and models will be made publicly \\navailable to ensure reproducibility and encourage positive use. \\n \\n8. Conclusion'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:54:40+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:54:40+01:00', 'source': '..\\\\data\\\\pdf\\\\proposal.pdf', 'total_pages': 8, 'page': 6, 'page_label': '7', 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='will not use private or personally identifiable data. The code and models will be made publicly \\navailable to ensure reproducibility and encourage positive use. \\n \\n8. Conclusion \\nThis research proposal addresses the critical problem of domain shift in object detection. By \\nproposing a novel framework that integrates style transfer with a more robust, uncertainty-aware \\nself-training mechanism, we aim to significantly advance the state of the art in unsupervised domain \\nadaptation. The successful completion of this project will produce more reliable perception models, \\nthereby accelerating the safe and responsible deployment of AI in real-world applications. \\n \\n9. References \\n[Hoffman et al., 2018] Hoffman, J., Tzeng, E., Park, T., Zhu, J. Y., Isola, P., Saenko, K., ... & \\nDarrell, T. (2018). CyCADA: Cycle-consistent adversarial domain adaptation. International \\nconference on machine learning.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:54:40+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:54:40+01:00', 'source': '..\\\\data\\\\pdf\\\\proposal.pdf', 'total_pages': 8, 'page': 7, 'page_label': '8', 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='[Saito et al., 2017] Saito, K., Watanabe, K., Ushiku, Y., & Harada, T. (2017). Asymmetric tri-\\ntraining for unsupervised domain adaptation. International conference on machine learning. \\n[Gal & Ghahramani, 2016] Gal, Y., & Ghahramani, Z. (2016). Dropout as a Bayesian approximation: \\nRepresenting model uncertainty in deep learning. International conference on machine learning. \\n[Zou et al., 2018] Zou, Y., Yu, Z., Kumar, B., & Wang, J. (2018). Unsupervised domain adaptation \\nfor semantic segmentation via class-balanced self-training. European conference on computer vision.')]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks=split_documents(all_pdf_documents)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188b49d2",
   "metadata": {},
   "source": [
    "## Embedding And vectorStoreDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "20f2b5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "import uuid\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92da1bac",
   "metadata": {},
   "source": [
    " ### Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "174f9bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model:all-MiniLM-L6-v2\n",
      "Model loaded successfully;384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.EmbeddingManager at 0x1d984f96e70>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbeddingManager:\n",
    "    \"\"\"Handles document embedding using SentenceTrans\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        \"\"\"\n",
    "        Initializes the EmbeddingManager with a specified SentenceTransformer model.\n",
    "        Args:\n",
    "            model_name (str): The name of the SentenceTransformer model to use.\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self._load_model()\n",
    "    def _load_model(self):\n",
    "        \"\"\"Loads the SentenceTransformer model.\"\"\"\n",
    "        try:\n",
    "            print(f\"Loading embedding model:{self.model_name}\")\n",
    "            self.model = SentenceTransformer(self.model_name)\n",
    "            print(f\"Model loaded successfully;{self.model.get_sentence_embedding_dimension()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model{self.model_name}:{e}\")\n",
    "            raise  \n",
    "    def generate_embeddings(self, texts: List[str])-> np.ndarray :\n",
    "        \"\"\"\n",
    "        Generate embeddings for a list of texts\n",
    "        \n",
    "        Args:\n",
    "            texts: List of text strings to embed\n",
    "            \n",
    "        Returns:\n",
    "            numpy array of embeddings with shape (len(texts), embedding_dim)\n",
    "        \"\"\"\n",
    "              \n",
    "        if not self.model:\n",
    "            raise ValueError(\"Model not loaded\")\n",
    "        \n",
    "        print(f\"Generating embedding for a {len(texts)} texts....\")\n",
    "        embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "        print(f\"Generated embeddings with shape :{embeddings.shape}\")\n",
    "        return embeddings\n",
    "    \n",
    "#intialize   the embedding  manager\n",
    "embedding_manager = EmbeddingManager()\n",
    "embedding_manager  \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfc7ff3",
   "metadata": {},
   "source": [
    "## VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0be0083c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store initialized. Collection: pdf_documents\n",
      "Existing documents in collection: 252\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.VectorStore at 0x1d9809a6ed0>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VectorStore:\n",
    "    \"\"\"Manages document embeddings in a ChromaDB vector store\"\"\"\n",
    "    \n",
    "    def __init__(self, collection_name: str = \"pdf_documents\", persist_directory: str = \"../data/vector_store\"):\n",
    "        \"\"\"\n",
    "        Initialize the vector store\n",
    "        \n",
    "        Args:\n",
    "            collection_name: Name of the ChromaDB collection\n",
    "            persist_directory: Directory to persist the vector store\n",
    "        \"\"\"\n",
    "        self.collection_name = collection_name\n",
    "        self.persist_directory = persist_directory\n",
    "        self.client = None\n",
    "        self.collection = None\n",
    "        self._initialize_store()\n",
    "\n",
    "    def _initialize_store(self):\n",
    "        \"\"\"Initialize ChromaDB client and collection\"\"\"\n",
    "        try:\n",
    "            # Create persistent ChromaDB client\n",
    "            os.makedirs(self.persist_directory, exist_ok=True)\n",
    "            self.client = chromadb.PersistentClient(path=self.persist_directory)\n",
    "            \n",
    "            # Get or create collection\n",
    "            self.collection = self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"description\": \"PDF document embeddings for RAG\"}\n",
    "            )\n",
    "            print(f\"Vector store initialized. Collection: {self.collection_name}\")\n",
    "            print(f\"Existing documents in collection: {self.collection.count()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "    def add_documents(self, documents: List[Any], embeddings: np.ndarray):\n",
    "        \"\"\"\n",
    "        Add documents and their embeddings to the vector store\n",
    "        \n",
    "        Args:\n",
    "            documents: List of LangChain documents\n",
    "            embeddings: Corresponding embeddings for the documents\n",
    "        \"\"\"\n",
    "        if len(documents) != len(embeddings):\n",
    "            raise ValueError(\"Number of documents must match number of embeddings\")\n",
    "        \n",
    "        print(f\"Adding {len(documents)} documents to vector store...\")\n",
    "        \n",
    "        # Prepare data for ChromaDB\n",
    "        ids = []\n",
    "        metadatas = []\n",
    "        documents_text = []\n",
    "        embeddings_list = []\n",
    "        \n",
    "        for i, (doc, embedding) in enumerate(zip(documents, embeddings)):\n",
    "            # Generate unique ID\n",
    "            doc_id = f\"doc_{uuid.uuid4().hex[:8]}_{i}\"\n",
    "            ids.append(doc_id)\n",
    "            \n",
    "            # Prepare metadata\n",
    "            metadata = dict(doc.metadata)\n",
    "            metadata['doc_index'] = i\n",
    "            metadata['content_length'] = len(doc.page_content)\n",
    "            metadatas.append(metadata)\n",
    "            \n",
    "            # Document content\n",
    "            documents_text.append(doc.page_content)\n",
    "            \n",
    "            # Embedding\n",
    "            embeddings_list.append(embedding.tolist())\n",
    "        \n",
    "        # Add to collection\n",
    "        try:\n",
    "            self.collection.add(\n",
    "                ids=ids,\n",
    "                embeddings=embeddings_list,\n",
    "                metadatas=metadatas,\n",
    "                documents=documents_text\n",
    "            )\n",
    "            print(f\"Successfully added {len(documents)} documents to vector store\")\n",
    "            print(f\"Total documents in collection: {self.collection.count()}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to vector store: {e}\")\n",
    "            raise\n",
    "\n",
    "vectorstore=VectorStore()\n",
    "vectorstore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e237ead5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 0, 'page_label': '1', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Attention Mechanism in Neural Networks:\\nWhere it Comes and Where it Goes\\nDerya Soydaner\\nReceived: 22 July 2021 / Accepted: 27 April 2022\\nAbstract A long time ago in the machine learning literature, the idea of\\nincorporating a mechanism inspired by the human visual system into neural\\nnetworks was introduced. This idea is named the attention mechanism, and it\\nhas gone through a long development period. Today, many works have been\\ndevoted to this idea in a variety of tasks. Remarkable performance has re-\\ncently been demonstrated. The goal of this paper is to provide an overview\\nfrom the early work on searching for ways to implement attention idea with\\nneural networks until the recent trends. This review emphasizes the impor-\\ntant milestones during this progress regarding diﬀerent tasks. By this way,\\nthis study aims to provide a road map for researchers to explore the current\\ndevelopment and get inspired for novel approaches beyond the attention.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 0, 'page_label': '1', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='this study aims to provide a road map for researchers to explore the current\\ndevelopment and get inspired for novel approaches beyond the attention.\\nKeywords Attention mechanism ·Neural networks ·Deep learning ·Survey\\n1 Introduction\\nHuman eye sees the world in an interesting way. We suppose as if we see the\\nentire scene at once, but this is an illusion created by the subconscious part\\nof our brain [1]. According to the Scanpath theory [2,3], when the human eye\\nlooks at an image, it can see only a small patch in high resolution. This small\\npatch is called thefovea. It can see the rest of the image in low resolution which\\nis called the periphery. To recognize the entire scene, the eye performs feature\\nextraction based on the fovea. The eye is moved to diﬀerent parts of the image\\nuntil the information obtained from the fovea is suﬃcient for recognition [4].\\nThese eye movements are called saccades. The eye makes successive ﬁxations\\nDerya Soydaner'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 0, 'page_label': '1', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='until the information obtained from the fovea is suﬃcient for recognition [4].\\nThese eye movements are called saccades. The eye makes successive ﬁxations\\nDerya Soydaner\\nDepartment of Brain and Cognition, University of Leuven (KU Leuven), Leuven, Belgium\\nTel.: +32-16710471\\nE-mail: derya.soydaner@kuleuven.be\\narXiv:2204.13154v1  [cs.LG]  27 Apr 2022'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='2 Derya Soydaner\\nuntil the recognition task is complete. This sequential process happens so\\nquickly that we feel as if it happens all at once.\\nBiologically, this is called visual attention system . Visual attention is de-\\nﬁned as the ability to dynamically restrict processing to a subset of the visual\\nﬁeld [5]. It seeks answers for two main questions: What and where to look?\\nVisual attention has been extensively studied in psychology and neuroscience;\\nfor reviews see [6,7,8,9,10]. Besides, there is a large amount of literature on\\nmodeling eye movements [11,12,13,14]. These studies have been a source of\\ninspiration for many artiﬁcial intelligence tasks. It has been discovered that\\nthe attention idea is useful from image recognition to machine translation.\\nTherefore, diﬀerent types of attention mechanisms inspired from the human\\nvisual system have been developed for years. Since the success of deep neural\\nnetworks has been at the forefront for these artiﬁcial intelligence tasks, these'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='visual system have been developed for years. Since the success of deep neural\\nnetworks has been at the forefront for these artiﬁcial intelligence tasks, these\\nmechanisms have been integrated into neural networks for a long time.\\nThis survey is about the journey of attention mechanisms used with neu-\\nral networks. Researchers have been investigating ways to strengthen neural\\nnetwork architectures with attention mechanisms for many years. The pri-\\nmary aim of these studies is to reduce computational burden and to improve\\nthe model performance as well. Previous work reviewed the attention mecha-\\nnisms from diﬀerent perspectives [15], or examined them in context of natural\\nlanguage processing (NLP) [16,17]. However, in this study, we examine the\\ndevelopment of attention mechanisms over the years, and recent trends. We\\nbegin with the ﬁrst attempts to integrate the visual attention idea to neural\\nnetworks, and continue until the most modern neural networks armed with at-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='begin with the ﬁrst attempts to integrate the visual attention idea to neural\\nnetworks, and continue until the most modern neural networks armed with at-\\ntention mechanisms. One of them is the Transformer, which is used for many\\nstudies including the GPT-3 language model [18], goes beyond convolutions\\nand recurrence by replacing them with only attention layers [19]. Finally, we\\ndiscuss how much more can we move forward, and what’s next?\\n2 From the Late 1980s to Early 2010s: The Attention Awakens\\nThe ﬁrst attempts at adapting attention mechanisms to neural networks go\\nback to the late 1980s. One of the early studies is the improved version of\\nthe Neocognitron [20] with selective attention [21]. This study is then mod-\\niﬁed to recognize and segment connected characters in cursive handwriting\\n[22]. Another study describes VISIT, a novel model that concentrates on its\\nrelationship to a number of visual areas of the brain [5]. Also, a novel archi-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 1, 'page_label': '2', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='[22]. Another study describes VISIT, a novel model that concentrates on its\\nrelationship to a number of visual areas of the brain [5]. Also, a novel archi-\\ntecture named Signal Channelling Attentional Network (SCAN) is presented\\nfor attentional scanning [23].\\nEarly work on improving the attention idea for neural networks includes\\na variety of tasks such as target detection [24]. In another study, a visual at-\\ntention system extracts regions of interest by combining the bottom-up and\\ntop-down information from the image [25]. A recognition model based on se-\\nlective attention which analyses only a small part of the image at each step,\\nand combines results in time is described [4]. Besides, a model based on the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 2, 'page_label': '3', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Attention Mechanism in Neural Networks: 3\\nconcept of selective tuning is proposed [26]. As the years go by, several studies\\nthat use the attention idea in diﬀerent ways have been presented for visual\\nperception and recognition [27,28,29,30].\\nBy the 2000s, the studies on making attention mechanisms more useful for\\nneural networks continued. In the early years, a model that integrates an at-\\ntentional orienting where pathway and an object recognition what pathway is\\npresented [31]. A computational model of human eye movements is proposed\\nfor an object class detection task [32]. A serial model is presented for visual pat-\\ntern recognition gathering Markov models and neural networks with selective\\nattention on the handwritten digit recognition and face recognition problems\\n[33]. In that study, a neural network analyses image parts and generates pos-\\nterior probabilities as observations to the Markov model. Also, attention idea'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 2, 'page_label': '3', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='[33]. In that study, a neural network analyses image parts and generates pos-\\nterior probabilities as observations to the Markov model. Also, attention idea\\nis used for object recognition [34], and the analysis of a scene [35]. An inter-\\nesting study proposes to learn sequential attention in real-world visual object\\nrecognition using a Q-learner [36]. Besides, a computational model of visual\\nselective attention is described to automatically detect the most relevant parts\\nof a color picture displayed on a television screen [37]. The attention idea is\\nalso used for identifying and tracking objects in multi-resolution digital video\\nof partially cluttered environments [38].\\nIn 2010, the ﬁrst implemented system inspired by the fovea of human retina\\nwas presented for image classiﬁcation [39]. This system jointly trains a re-\\nstricted Boltzmann machine (RBM) and an attentional component called the\\nﬁxation controller. Similarly, a novel attentional model is implemented for si-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 2, 'page_label': '3', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='stricted Boltzmann machine (RBM) and an attentional component called the\\nﬁxation controller. Similarly, a novel attentional model is implemented for si-\\nmultaneous object tracking and recognition that is driven by gaze data [40].\\nBy taking advantage of reinforcement learning, a novel recurrent neural net-\\nwork (RNN) is described for image classiﬁcation [41]. Deep Attention Selective\\nNetwork (DasNet), a deep neural network with feedback connections that are\\nlearned through reinforcement learning to direct selective attention to certain\\nfeatures extracted from images, is presented [42]. Additionally, a deep learning\\nbased framework using attention has been proposed for generative modeling\\n[43].\\n3 2015: The Rise of Attention\\nIt can be said that 2015 is the golden year of attention mechanisms. Because\\nthe number of attention studies has grown like an avalanche after three main\\nstudies presented in that year. The ﬁrst one proposed a novel approach for'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 2, 'page_label': '3', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='the number of attention studies has grown like an avalanche after three main\\nstudies presented in that year. The ﬁrst one proposed a novel approach for\\nneural machine translation (NMT) [44]. As it is known, most of the NMT\\nmodels belong to a family of encoder-decoders [45,46], with an encoder and a\\ndecoder for each language. However, compressing all the necessary information\\nof a source sentence into a ﬁxed-length vector is an important disadvantage of\\nthis encoder-decoder approach. This usually makes it diﬃcult for the neural\\nnetwork to capture all the semantic details of a very long sentence [1].\\nThe idea that [44] introduced is an extension to the conventional NMT\\nmodels. This extension is composed of an encoder and decoder as shown in'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 3, 'page_label': '4', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='4 Derya Soydaner\\nFig. 1 The extension to the conventional NMT models that is proposed by [44]. It generates\\nthe t-th target word yt given a source sentence ( x1,x2,...,x T ).\\nFig 1. The ﬁrst part, encoder, is a bidirectional RNN (BiRNN) [47] that takes\\nword vectors as input. The forward and backward states of BiRNN are com-\\nputed. Then, an annotation aj for each word xj is obtained by concatenating\\nthese forward and backward hidden states. Thus, the encoder maps the input\\nsentence to a sequence of annotations ( a1,...,a Tx ). By using a BiRNN rather\\nthan conventional RNN, the annotation of each word can summarize both\\nthe preceding words and the following words. Besides, the annotation aj can\\nfocus on the words around xj because of the inherent nature of RNNs that\\nrepresenting recent inputs better.\\nIn decoder, a weight αij of each annotation aj is obtained by using its\\nassociated energy eij that is computed by a feedforward neural network f as'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 3, 'page_label': '4', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='representing recent inputs better.\\nIn decoder, a weight αij of each annotation aj is obtained by using its\\nassociated energy eij that is computed by a feedforward neural network f as\\nin Eq. (1). This neural network f is deﬁned as an alignment model that can\\nbe jointly trained with the proposed architecture. In order to reduce compu-\\ntational burden, a multilayer perceptron (MLP) with a single hidden layer is\\nproposed as f. This alignment model tells us about the relation between the\\ninputs around position j and the output at position i. By this way, the decoder\\napplies an attention mechanism. As it is seen in Eq. (2), the αij is the output\\nof softmax function:\\neij = f(hi−1,aj) (1)\\nαij = exp(eij)∑Tx\\nk=1 exp(eik)\\n(2)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 4, 'page_label': '5', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Attention Mechanism in Neural Networks: 5\\nHere, the probability αij determines the importance of annotation aj with\\nrespect to the previous hidden state hi−1. Finally, the context vector ci is\\ncomputed as a weighted sum of these annotations as follows [44]:\\nci =\\nTx∑\\nj=1\\nαijaj (3)\\nBased on the decoder state, the context and the last generated word, the\\ntarget word yt is predicted. In order to generate a word in a translation, the\\nmodel searches for the most relevant information in the source sentence to\\nconcentrate. When it ﬁnds the appropriate source positions, it makes the pre-\\ndiction. By this way, the input sentence is encoded into a sequence of vectors\\nand a subset of these vectors is selected adaptively by the decoder that is rel-\\nevant to predicting the target [44]. Thus, it is no longer necessary to compress\\nall the information of a source sentence into a ﬁxed-length vector.\\nThe second study is the ﬁrst visual attention model in image captioning'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 4, 'page_label': '5', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='all the information of a source sentence into a ﬁxed-length vector.\\nThe second study is the ﬁrst visual attention model in image captioning\\n[48]. Diﬀerent from the previous study [44], it uses a deep convolutional neural\\nnetwork (CNN) as an encoder. This architecture is an extension of the neural\\nnetwork [49] that encodes an image into a compact representation, followed by\\nan RNN that generates a corresponding sentence. Here, the annotation vectors\\nai ∈RD are extracted from a lower convolutional layer, each of which is a D-\\ndimensional representation corresponding to a part of the image. Thus, the\\ndecoder selectively focuses on certain parts of an image by weighting a subset\\nof all the feature vectors [48]. This extended architecture uses attention for\\nsalient features to dynamically come to the forefront instead of compressing\\nthe entire image into a static representation.\\nThe context vector ct represents the relevant part of the input image at'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 4, 'page_label': '5', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='salient features to dynamically come to the forefront instead of compressing\\nthe entire image into a static representation.\\nThe context vector ct represents the relevant part of the input image at\\ntime t. The weight αi of each annotation vector is computed similar to Eq. (2),\\nwhereas its associated energy is computed similar to Eq. (1) by using an MLP\\nconditioned on the previous hidden state ht−1. The remarkable point of this\\nstudy is a new mechanism φ that computes ct from the annotation vectors ai\\ncorresponding to the features extracted at diﬀerent image locations:\\nct = φ(\\n{\\nai\\n}\\n,\\n{\\nαi\\n}\\n) (4)\\nThe deﬁnition of the φ function causes two variants of attention mecha-\\nnisms: The hard (stochastic) attention mechanism is trainable by maximizing\\nan approximate variational lower bound, i.e., by REINFORCE [50]. On the\\nother side, the soft (deterministic) attention mechanism is trainable by stan-\\ndard backpropagation methods. The hard attention deﬁnes a location variable'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 4, 'page_label': '5', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='other side, the soft (deterministic) attention mechanism is trainable by stan-\\ndard backpropagation methods. The hard attention deﬁnes a location variable\\nst, and uses it to decide where to focus attention when generating the t-th\\nword. When the hard attention is applied, the attention locations are con-\\nsidered as intermediate latent variables. It assigns a multinoulli distribution\\nparametrized by αi, and ct becomes a random variable. Here, st,i is deﬁned\\nas a one-hot variable which is set to 1 if the i-th location is used to extract\\nvisual features [48]:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 5, 'page_label': '6', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='6 Derya Soydaner\\np(st,i = 1|sj<t,a) = αt,i (5)\\nct =\\n∑\\ni\\nst,iai (6)\\nWhereas learning hard attention requires sampling the attention location\\nst each time, the soft attention mechanism computes a weighted annotation\\nvector similar to [44] and takes the expectation of the context vectorct directly:\\nEp(st|α)[ct] =\\nL∑\\ni=1\\nαt,iai (7)\\nFurthermore, in training the deterministic version of the model, an alterna-\\ntive method namely doubly stochastic attention, is proposed with an additional\\nconstraint added to the training objective to encourage the model to pay equal\\nattention to all parts of the image.\\nThe third study should be emphasized presents two classes of attention\\nmechanisms for NMT: the global attention that always attends to all source\\nwords, and the local attention that only looks at a subset of source words at\\na time [51]. These mechanisms derive the context vector ct in diﬀerent ways:\\nWhereas the global attention considers all the hidden states of the encoder, the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 5, 'page_label': '6', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='a time [51]. These mechanisms derive the context vector ct in diﬀerent ways:\\nWhereas the global attention considers all the hidden states of the encoder, the\\nlocal one selectively focuses on a small window of context. In global attention, a\\nvariable-length alignment vector is derived similar to Eq. (2). Here, the current\\ntarget hidden state ht is compared with each source hidden state ¯hs by using a\\nscore function instead of the associated energy eij. Thus, the alignment vector\\nwhose size equals the number of time steps on the source side is derived.\\nGiven the alignment vector as weights, the context vector ct is computed as\\nthe weighted average over all the source hidden states. Here, score is referred\\nas a content-based function, and three diﬀerent alternatives are considered [51].\\nOn the other side, the local attention is diﬀerentiable. Firstly, an aligned\\nposition pt is generated for each target word at a time t. Then, a window'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 5, 'page_label': '6', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='On the other side, the local attention is diﬀerentiable. Firstly, an aligned\\nposition pt is generated for each target word at a time t. Then, a window\\ncentered around the source position pt is used to compute the context vector\\nas a weighted average of the source hidden states within the window. The\\nlocal attention selectively focuses on a small window of context, and obtains\\nthe alignment vector from the current target state ht and the source states ¯hs\\nin the window [51].\\nThe introduction of these novel mechanisms in 2015 triggered the rise of\\nattention for neural networks. Based on the proposed attention mechanisms,\\nsigniﬁcant research has been conducted in a variety of tasks. In order to imag-\\nine the attention idea in neural networks better, two visual examples are shown\\nin Fig. 2. A neural image caption generation task is seen in the top row that\\nimplements an attention mechanism [48]. Then, the second example shows'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 5, 'page_label': '6', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='in Fig. 2. A neural image caption generation task is seen in the top row that\\nimplements an attention mechanism [48]. Then, the second example shows\\nhow the attention mechanisms can be used for visual question answering [52].\\nBoth examples demonstrate how attention mechanisms focus on parts of input\\nimages.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 6, 'page_label': '7', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Attention Mechanism in Neural Networks: 7\\nFig. 2 Examples of the attention mechanism in visual. (Top) Attending to the correct\\nobject in neural image caption generation [48]. (Bottom) Visualization of original image\\nand question pairs, and co-attention maps namely word-level, phrase-level and question-\\nlevel, respectively [52].\\n4 2015-2016: Attack of the Attention\\nDuring two years from 2015, the attention mechanisms were used for diﬀerent\\ntasks, and novel neural network architectures were presented applying these\\nmechanisms. After the memory networks [53] that require a supervision signal\\ninstructing them how to use their memory cells, the introduction of the neural\\nTuring machine [54] allows end-to-end training without this supervision signal,\\nvia the use of a content-based soft attention mechanism [1]. Then, end-to-end\\nmemory network [55] that is a form of memory network based on a recurrent\\nattention mechanism is proposed.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 6, 'page_label': '7', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='via the use of a content-based soft attention mechanism [1]. Then, end-to-end\\nmemory network [55] that is a form of memory network based on a recurrent\\nattention mechanism is proposed.\\nIn these years, an attention mechanism called self-attention, sometimes\\ncalled intra-attention, was successfully implemented within a neural network\\narchitecture namely Long Short-Term Memory-Networks (LSTMN) [56]. It\\nmodiﬁes the standard LSTM structure by replacing the memory cell with a\\nmemory network [53]. This is because memory networks have a set of key\\nvectors and a set of value vectors, whereas LSTMs maintain a hidden vector\\nand a memory vector [56]. In contrast to attention idea in [44], memory and\\nattention are added within a sequence encoder in LSTMN. In order to compute\\na representation of a sequence, self-attention is described as relating diﬀerent\\npositions of it [19]. One of the ﬁrst approaches of self-attention is applied for\\nnatural language inference [57].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 6, 'page_label': '7', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='a representation of a sequence, self-attention is described as relating diﬀerent\\npositions of it [19]. One of the ﬁrst approaches of self-attention is applied for\\nnatural language inference [57].\\nMany attention-based models have been proposed for neural image cap-\\ntioning [58], abstractive sentence summarization [59], speech recognition [60,\\n61], automatic video captioning [62], neural machine translation [63], and rec-\\nognizing textual entailment [64]. Diﬀerent attention-based models perform vi-\\nsual question answering [65,66,67]. An attention-based CNN is presented for\\nmodeling sentence pairs [68]. A recurrent soft attention based model learns to\\nfocus selectively on parts of the video frames and classiﬁes videos [69].\\nOn the other side, several neural network architectures have been pre-\\nsented in a variety of tasks. For instance, Stacked Attention Network (SAN)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 7, 'page_label': '8', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='8 Derya Soydaner\\nis described for image question answering [70]. Deep Attention Recurrent Q-\\nNetwork (DARQN) integrates soft and hard attention mechanisms into the\\nstructure of Deep Q-Network (DQN) [71]. Wake-Sleep Recurrent Attention\\nModel (WS-RAM) speeds up the training time for image classiﬁcation and\\ncaption generation tasks [72]. alignDRAW model, an extension of the Deep\\nRecurrent Attention Writer (DRAW) [73], is a generative model of images\\nfrom captions using a soft attention mechanism [74]. Generative Adversarial\\nWhat-Where Network (GAWWN) synthesizes images given instructions de-\\nscribing what content to draw in which location [75].\\n5 The Transformer: Return of the Attention\\nAfter the proposed attention mechanisms in 2015, researchers published stud-\\nies that mostly modifying or implementing them to diﬀerent tasks. However,\\nin 2017, a novel neural network architecture, namely the Transformer, based\\nentirely on self-attention was presented [19]. The Transformer achieved great'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 7, 'page_label': '8', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='in 2017, a novel neural network architecture, namely the Transformer, based\\nentirely on self-attention was presented [19]. The Transformer achieved great\\nresults on two machine translation tasks in addition to English constituency\\nparsing. The most impressive point about this architecture is that it contains\\nneither recurrence nor convolution. The Transformer performs well by replac-\\ning the conventional recurrent layers in encoder-decoder architecture used for\\nNMT with self-attention.\\nThe Transformer is composed of encoder-decoder stacks each of which has\\nsix identical layers within itself. In Fig. 3, one encoder-decoder stack is shown\\nto illustrate the model [19]. Each stack includes only attention mechanisms\\nand feedforward neural networks. As this architecture does not include any\\nrecurrent or convolutional layer, information about the relative or absolute\\npositions in the input sequence is given at the beginning of both encoder and\\ndecoder using positional encodings.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 7, 'page_label': '8', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='recurrent or convolutional layer, information about the relative or absolute\\npositions in the input sequence is given at the beginning of both encoder and\\ndecoder using positional encodings.\\nThe calculations of self-attention are slightly diﬀerent from the mechanisms\\ndescribed so far in this paper. It uses three vectors namely query, key and\\nvalue for each word. These vectors are computed by multiplying the input with\\nweight matrices Wq, Wk and Wv which are learned during training. In general,\\neach value is weighted by a function of the query with the corresponding key.\\nThe output is computed as a weighted sum of the values. Based on this idea,\\ntwo attention mechanisms are proposed: In the ﬁrst one, called scaled dot-\\nproduct attention, the dot products of the query with all keys are computed\\nas given in the right side of Fig. 3. Each result is divided to the square root of\\nthe dimension of the keys to have more stable gradients. They pass into the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 7, 'page_label': '8', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='as given in the right side of Fig. 3. Each result is divided to the square root of\\nthe dimension of the keys to have more stable gradients. They pass into the\\nsoftmax function, thus the weights for the values are obtained. Finally each\\nsoftmax score is multiplied with the value as given in Eq. (8). The authors\\npropose computing the attention on a set of queries simultaneously by taking\\nqueries and keys of dimension dk, and values of dimension dv as inputs. The\\nkeys, queries and values are packed together into matricesK, Q and V. Finally,\\nthe output matrix is obtained as follows [19]:'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 8, 'page_label': '9', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Attention Mechanism in Neural Networks: 9\\nFig. 3 The Transformer architecture and the attention mechanisms it uses in detail [19].\\n(Left) The Transformer with one encoder-decoder stack. (Center) Multi-head attention.\\n(Right) Scaled dot-product attention.\\nAttention(Q,K,V ) = softmax(QKT\\n√dk\\n)V (8)\\nThis calculation is performed by every word against the other words. This\\nleads to having values of each word relative to each other. For instance, if\\nthe word x2 is not relevant for the word x1, then the softmax score gives low\\nprobability scores. As a result, the corresponding value is decreased. This leads\\nto an increase in the value of relevant words, and those of others decrease. In\\nthe end, every word obtains a new value for itself.\\nAs seen from Fig. 3, the Transformer model does not directly use scaled\\ndot-product attention. But the attention mechanism it uses is based on these\\ncalculations. The second mechanism proposed, called the multi-head attention,'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 8, 'page_label': '9', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='dot-product attention. But the attention mechanism it uses is based on these\\ncalculations. The second mechanism proposed, called the multi-head attention,\\nlinearly projects the queries, keys and values h times with diﬀerent, learned\\nlinear projections to dq, dk and dv dimensions, respectively [19]. The attention\\nfunction is performed in parallel on each of these projected versions of queries,\\nkeys and values, i.e., heads. By this way, dv-dimensional output values are\\nobtained. In order to get the ﬁnal values, they are concatenated and projected\\none last time as shown in the center of Fig. 3. By this way, the self-attention is\\ncalculated multiple times using diﬀerent sets of query, key and value vectors.\\nThus, the model can jointly attend to information at diﬀerent positions [19]:\\nMultiHead(Q,K,V ) = Concat(head1,...,head h)WO (9)\\nwherehead i = Attention(QWQ\\ni ,KW K\\ni ,VW V\\ni )\\nIn the decoder part of the Transformer, masked multi-head attention is'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 8, 'page_label': '9', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='MultiHead(Q,K,V ) = Concat(head1,...,head h)WO (9)\\nwherehead i = Attention(QWQ\\ni ,KW K\\ni ,VW V\\ni )\\nIn the decoder part of the Transformer, masked multi-head attention is\\napplied ﬁrst to ensure that only previous word embeddings are used when\\ntrying to predict the next word in the sentence. Therefore, the embeddings\\nthat shouldn’t be seen by the decoder are masked by multiplying with zero.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 9, 'page_label': '10', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='10 Derya Soydaner\\nAn interesting study examines the contribution made by individual atten-\\ntion heads in the encoder [76]. Also, there is an evaluation of the eﬀects of\\nself-attention on gradient propagation in recurrent networks [77]. For a deeper\\nanalysis of multi-head self-attention mechanism from a theoretical perspective\\nsee [78].\\nSelf-attention has been used successfully in a variety of tasks including\\nsentence embedding [79] and abstractive summarization [80]. It is shown that\\nself-attention can lead to improvements to discriminative constituency parser\\n[81], and speech recognition as well [82,83]. Also, the listen-attend-spell model\\n[84] has been improved with the self-attention for acoustic modeling [85].\\nAs soon as these self-attention mechanisms were proposed, they have been\\nincorporated with deep neural networks for a wide range of tasks. For instance,\\na deep learning model learned a number of large-scale tasks from multiple do-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 9, 'page_label': '10', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='incorporated with deep neural networks for a wide range of tasks. For instance,\\na deep learning model learned a number of large-scale tasks from multiple do-\\nmains with the aid of self-attention mechanism [86]. Novel self-attention neural\\nmodels are proposed for cross-target stance classiﬁcation [87] and NMT [88].\\nAnother study points out that a fully self-attentional model can reach com-\\npetitive predictive performance on ImageNet classiﬁcation and COCO object\\ndetection tasks [89]. Besides, developing novel attention mechanisms has been\\ncarried out such as area attention, a novel mechanism that can be used along\\nmulti-head attention [90]. It attends to areas in the memory by deﬁning the\\nkey of an area as the mean vector of the key of each item, and deﬁning the\\nvalue as the sum of all value vectors in the area.\\nWhen a novel mechanism is proposed, it is inevitable to incorporate it\\ninto the GAN framework [91]. Self-Attention Generative Adversarial Networks'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 9, 'page_label': '10', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='value as the sum of all value vectors in the area.\\nWhen a novel mechanism is proposed, it is inevitable to incorporate it\\ninto the GAN framework [91]. Self-Attention Generative Adversarial Networks\\n(SAGANs) [92] introduce a self-attention mechanism into convolutional GANs.\\nDiﬀerent from the traditional convolutional GANs, SAGAN generates high-\\nresolution details using cues from all feature locations. Similarly, Attentional\\nGenerative Adversarial Network (AttnGAN) is presented for text to image\\ngeneration [93]. On the other side, a machine reading and question answering\\narchitecture called QANet [94] is proposed without any recurrent networks. It\\nuses self-attention to learn the global interaction between each pair of words\\nwhereas convolution captures the local structure of the text. In another study,\\nGated Attention Network (GaAN) controls the importance of each attention\\nhead’s output by introducing gates [95]. Another interesting study introduces'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 9, 'page_label': '10', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Gated Attention Network (GaAN) controls the importance of each attention\\nhead’s output by introducing gates [95]. Another interesting study introduces\\nattentive group convolutions with a generalization of visual self-attention [96].\\nA deep transformer model is implemented for language modeling over long\\nsequences [97].\\n5.1 Self-attention variants\\nIn recent years, self-attention has become an important research direction\\nwithin the deep learning community. Self-attention idea has been examined\\nin diﬀerent aspects. For example, self-attention is handled in a multi-instance\\nlearning framework [98]. The idea of Sparse Adaptive Connection (SAC) is\\npresented for accelerating and structuring self-attention [99]. The research on'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 10, 'page_label': '11', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Attention Mechanism in Neural Networks: 11\\nTable 1 Summary of Notation\\nSymbol Deﬁnition\\na annotation\\nc context vector\\nα weight\\ne energy\\nf feedforward neural network\\nh hidden state\\nφ hard (stochastic) / soft (deterministic) attention\\ns location variable\\np source position\\nK, Q, V keys, queries and values matrices, respectively\\nWq, Wk, Wv weight matrices for queries, keys and values, respectively\\nimproving self-attention continues as well [100,101,102]. Besides, based on the\\nself-attention mechanisms proposed in the Transformer, important studies that\\nmodify the self-attention have been presented. Some of the most recent and\\nprominent studies are summarized below.\\nRelation-aware self-attention It extends the self-attention mechanism by\\nregarding representations of the relative positions, or distances between se-\\nquence elements [103]. Thus, it can consider the pairwise relationships between\\ninput elements. This type of attention mechanism deﬁnes vectors to represent'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 10, 'page_label': '11', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='quence elements [103]. Thus, it can consider the pairwise relationships between\\ninput elements. This type of attention mechanism deﬁnes vectors to represent\\nthe edge between two inputs. It provides learning two distinct edge represen-\\ntations that can be shared across attention heads without requiring additional\\nlinear transformations.\\nDirectional self-attention (DiSA) A novel neural network architecture\\nfor learning sentence embedding named Directional Self-Attention Network\\n(DiSAN) [104] uses directional self-attention followed by a multi-dimensional\\nattention mechanism. Instead of computing a single importance score for each\\nword based on the word embedding, multi-dimensional attention computes a\\nfeature-wise score vector for each token. To extend this mechanism to the self-\\nattention, two variants are presented: The ﬁrst one, called multi-dimensional\\n‘token2token’ self-attention generates context-aware coding for each element.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 10, 'page_label': '11', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='attention, two variants are presented: The ﬁrst one, called multi-dimensional\\n‘token2token’ self-attention generates context-aware coding for each element.\\nThe second one, called multi-dimensional ‘source2token’ self-attention com-\\npresses the sequence into a vector [104]. On the other side, directional self-\\nattention produces context-aware representations with temporal information\\nencoded by using positional masks. By this way, directional information is en-\\ncoded. First, the input sequence is transformed to a sequence of hidden states\\nby a fully connected layer. Then, multi-dimensional token2token self-attention\\nis applied to these hidden states. Hence, context-aware vector representations\\nare generated for all elements from the input sequence.\\nReinforced self-attention (ReSA)A sentence-encoding model named Re-\\ninforced Self-Attention Network (ReSAN)uses reinforced self-attention (ReSA)\\nthat integrates soft and hard attention mechanisms into a single model. ReSA'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 11, 'page_label': '12', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='12 Derya Soydaner\\nselects a subset of head tokens, and relates each head token to a small sub-\\nset of dependent tokens to generate their context-aware representations [105].\\nFor this purpose, a novel hard attention mechanism called reinforced sequence\\nsampling (RSS) , which selects tokens from an input sequence in parallel and\\ntrained via policy gradient, is proposed. Given an input sequence, RSS gener-\\nates an equal-length sequence of binary random variables that indicates both\\nthe selected and discarded ones. On the other side, the soft attention provides\\nreward signals back for training the hard attention. The proposed RSS pro-\\nvides a sparse mask to self-attention. ReSA uses two RSS modules to extract\\nthe sparse dependencies between each pair of selected tokens.\\nOuter product attention (OPA)Self-Attentive Associative Memory (SAM)\\nis a novel operator based upon outer product attention (OPA) [106]. This at-\\ntention mechanism is an extension of dot-product attention [19]. OPA diﬀers'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 11, 'page_label': '12', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='is a novel operator based upon outer product attention (OPA) [106]. This at-\\ntention mechanism is an extension of dot-product attention [19]. OPA diﬀers\\nusing element-wise multiplication, outer product, and tanh function instead of\\nsoftmax.\\nBidirectional block self-attention (Bi-BloSA)Another mechanism, bidi-\\nrectional block self-attention (Bi-BloSA) which is simply a masked block self-\\nattention (mBloSA) with forward and backward masks to encode the tempo-\\nral order information is presented [107]. Here, mBloSA is composed of three\\nparts from its bottom to top namely intra-block self-attention , inter-block\\nself-attention and the context fusion . It splits a sequence into several length-\\nequal blocks, and applies an intra-block self-attention to each block indepen-\\ndently. Then, inter-block self-attention processes the outputs for all blocks.\\nThis stacked self-attention model results a reduction in the amount of mem-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 11, 'page_label': '12', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='dently. Then, inter-block self-attention processes the outputs for all blocks.\\nThis stacked self-attention model results a reduction in the amount of mem-\\nory compared to a single one applied to the whole sequence. Finally, a feature\\nfusion gate combines the outputs of intra-block and inter-block self-attention\\nwith the original input, to produce the ﬁnal context-aware representations of\\nall tokens.\\nFixed multi-head attentionThe ﬁxed multi-head attention proposes ﬁxing\\nthe head size of the Transformer in the aim of improving the representation\\npower [108]. This study emphasizes its importance by setting the head size of\\nattention units to input sequence length.\\nSparse sinkhorn attentionIt is based on the idea of diﬀerentiable sorting\\nof internal representations within the self-attention module [109]. Instead of\\nallowing tokens to only attend to tokens within the same block, it operates\\non block sorted sequences. Each token attends to tokens in the sorted block.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 11, 'page_label': '12', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='allowing tokens to only attend to tokens within the same block, it operates\\non block sorted sequences. Each token attends to tokens in the sorted block.\\nThus, tokens that may be far apart in the unsorted sequence can be considered.\\nAdditionally, a variant of this mechanism named SortCut sinkhorn attention\\napplies a post-sorting truncation of the input sequence.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 12, 'page_label': '13', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Attention Mechanism in Neural Networks: 13\\nAdaptive attention spanAdaptive attention span is proposed as an alter-\\nnative to self-attention [110]. It learns the attention span of each head inde-\\npendently. To this end, a masking function inspired by [111] is used to control\\nthe attention span for each head. The purpose of this novel mechanism is to\\nreduce the computational burden of the Transformer. Additionally, dynamic\\nattention span approach is presented to dynamically change the attention span\\nbased on the current input as an extension [51,112].\\n5.2 Transformer variants\\nDiﬀerent from developing novel self-attention mechanisms, several studies have\\nbeen published in the aim of improving the performance of the Transformer.\\nThese studies mostly modify the model architecture. For instance, an addi-\\ntional recurrence encoder is preferred to model recurrence for Transformer di-\\nrectly [113]. In another study, a new weight initialization scheme is applied to'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 12, 'page_label': '13', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='tional recurrence encoder is preferred to model recurrence for Transformer di-\\nrectly [113]. In another study, a new weight initialization scheme is applied to\\nimprove Transformer optimization [114]. A novel positional encoding scheme\\nis used to extend the Transformer to tree-structured data [115]. Investigating\\nmodel size by handling Transformer width and depth for eﬃcient training is\\nalso an active research area [116]. Transformer is used in reinforcement learn-\\ning settings [117,118,119] and for time series forecasting in adversarial training\\nsetting [120].\\nBesides, many Transformer variants have been presented in the recent past.\\nCOMmonsEnse Transformer (COMET) is introduced for automatic construc-\\ntion of commonsense knowledge bases [121]. Evolved Transformer applies neu-\\nral architecture search for a better Transformer model [122]. Transformer Au-\\ntoencoder is a sequential autoencoder for conditional music generation [123].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 12, 'page_label': '13', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='ral architecture search for a better Transformer model [122]. Transformer Au-\\ntoencoder is a sequential autoencoder for conditional music generation [123].\\nCrossTransformer takes a small number of labeled images and an unlabeled\\nquery, and computes distances between spatially-corresponding features to in-\\nfer class membership [124]. DEtection TRansformer (DETR) is a new design\\nfor object detection systems [125], and Deformable DETR is an improved ver-\\nsion that achieves better performance in less time [126]. FLOw-bAsed Trans-\\nformER (FLOATER) emphasizes the importance of position encoding in the\\nTransformer, and models the position information via a continuous dynamical\\nmodel [127]. Disentangled Context (DisCo) Transformer simultaneously gener-\\nates all tokens given diﬀerent contexts by predicting every word in a sentence\\nconditioned on an arbitrary subset of the rest of the words [128]. Genera-\\ntive Adversarial Transformer (GANsformer) is presented for visual generative'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 12, 'page_label': '13', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='conditioned on an arbitrary subset of the rest of the words [128]. Genera-\\ntive Adversarial Transformer (GANsformer) is presented for visual generative\\nmodeling [129].\\nRecent work has demonstrated signiﬁcant performance on NLP tasks. In\\nOpenAI GPT, there is a left-to-right architecture, where every token can only\\nattend to previous tokens in the self-attention layers of the Transformer [130].\\nGPT-2 [131] and GPT-3 [18] models have improved the progress. In addition\\nto these variants, some prominent Transformer-based models are summarized\\nbelow.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 13, 'page_label': '14', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='14 Derya Soydaner\\nUniversal Transformer A generalization of the Transformer model named\\nthe Universal Transformer [132] iteratively computes representations Ht at\\nstep t for all positions in the sequence in parallel. To this end, it uses the\\nscaled dot-product attention in Eq. (8) where d is the number of columns\\nof Q, K and V. In the Universal Transformer, the multi-head self-attention\\nwith k heads is used. The representations Ht is mapped to queries, keys and\\nvalues with aﬃne projections using learned parameter matrices WQ ∈ℜd×d/k,\\nWK ∈ℜd×d/k, WV ∈ℜd×d/k and WO ∈ℜd×d [132]:\\nMultiHead(Ht) = Concat(head1,...,head k)WO (10)\\nwherehead i = Attention(HtWQ\\ni ,HtWK\\ni ,HtWV\\ni )\\nImage TransformerImage Transformer [133] demonstrates that self-attention\\nbased models can also be well-suited for images instead of text. This Trans-\\nformer type restricts the self-attention mechanism to attend to local neigh-\\nborhoods. Thus, the size of images that the model can process is increased.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 13, 'page_label': '14', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='former type restricts the self-attention mechanism to attend to local neigh-\\nborhoods. Thus, the size of images that the model can process is increased.\\nIts larger receptive ﬁelds allow the Image Transformer to signiﬁcantly improve\\nthe model performance on image generation as well as image super-resolution.\\nTransformer-XL This study aims to improve the ﬁxed-length context of the\\nTransformer [19] for language modeling. Transformer-XL [134] makes model-\\ning very long-term dependency possible by reusing the hidden states obtained\\nin previous segments. Hence, information can be propagated through the recur-\\nrent connections. In order to reuse the hidden states without causing temporal\\nconfusion, Transformer-XL uses relative positional encodings. Based on this\\narchitecture, a modiﬁed version named the Gated Transformer-XL (GTrXL)\\nis presented in the reinforcement learning setting [135].\\nTensorized TransformerTensorized Transformer[136] compresses the multi-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 13, 'page_label': '14', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='is presented in the reinforcement learning setting [135].\\nTensorized TransformerTensorized Transformer[136] compresses the multi-\\nhead attention in Transformer. To this end, it uses a novel self-attention model\\nmulti-linear attention with Block-Term Tensor Decomposition (BTD) [137]. It\\nbuilds a single-block attention based on the Tucker decomposition [138]. Then,\\nit uses a multi-linear attention constructed by a BTD to compress the multi-\\nhead attention mechanism. In Tensorized Transformer, the factor matrices are\\nshared across multiple blocks.\\nBERT The Bidirectional Encoder Representations fromTransformers (BERT)\\naims to pre-train deep bidirectional representations from unlabeled text [139].\\nBERT uses a multilayer bidirectional Transformer as the encoder. Besides,\\ninspired by the Cloze task [140], it has a masked language model pre-training\\nobjective. BERT randomly masks some of the tokens from the input, and pre-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 13, 'page_label': '14', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='inspired by the Cloze task [140], it has a masked language model pre-training\\nobjective. BERT randomly masks some of the tokens from the input, and pre-\\ndicts the original vocabulary id of the masked word based only on its context.\\nThis model can pre-train a deep bidirectional Transformer. In all layers, the'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 14, 'page_label': '15', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Attention Mechanism in Neural Networks: 15\\npre-training is carried out by jointly conditioning on both left and right con-\\ntext. BERT diﬀers from the left-to-right language model pre-training from this\\naspect.\\nRecently, BERT model has been examined in detail. For instance, the be-\\nhaviour of attention heads are analysed [141]. Various methods have been\\ninvestigated for compressing [142,143], pruning [144], and quantization [145].\\nAlso, BERT model has been considered for diﬀerent tasks such as coreference\\nresolution [146]. A novel method is proposed in order to accelerate BERT\\ntraining [147].\\nFurthermore, various BERT variants have been presented. ALBERT aims\\nto increase the training speed of BERT, and presents two parameter reduction\\ntechniques [148]. Similarly, PoWER-BERT [149] is developed to improve the\\ninference time of BERT. This scheme is also used to accelerate ALBERT. Also,\\nTinyBERT is proposed to accelerate inference and reduce model size while'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 14, 'page_label': '15', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='inference time of BERT. This scheme is also used to accelerate ALBERT. Also,\\nTinyBERT is proposed to accelerate inference and reduce model size while\\nmaintaining accuracy [150]. In order to obtain better representations, Span-\\nBERT is proposed as a pre-training method [151]. As a robustly optimized\\nBERT approach, RoBERTa shows that BERT was signiﬁcantly undertrained\\n[152]. Also, DeBERTa improves RoBERTa using the disentangled attention\\nmechanism [153]. On the other side, DistilBERT shows that it is possible to\\nreach similar performances using much smaller language models pre-trained\\nwith knowledge distillation [154]. StructBERT proposes two novel lineariza-\\ntion strategies [155]. Q-BERT is introduced for quantizing BERT models [156],\\nBioBERT is for biomedical text mining [157], and RareBERT is for rare dis-\\nease diagnosis [158].\\nSince 2017 when the Transformer was presented, research directions have\\ngenerally focused on novel self-attention mechanisms, adapting the Trans-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 14, 'page_label': '15', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='ease diagnosis [158].\\nSince 2017 when the Transformer was presented, research directions have\\ngenerally focused on novel self-attention mechanisms, adapting the Trans-\\nformer for various tasks, or making them more understandable. In one of the\\nmost recent studies, NLP becomes possible in the mobile setting with Lite\\nTransformer. It applies long-short range attention where some heads specialize\\nin the local context modeling while the others specialize in the long-distance\\nrelationship modeling [159]. A deep and light-weight Transformer DeLighT\\n[160] and a hypernetwork-based model namely HyperGrid Transformers [161]\\nperform with fewer parameters. Graph Transformer Network is introduced\\nfor learning node representations on heterogeneous graphs [162] and diﬀerent\\napplications are performed for molecular data [163] or textual graph represen-\\ntation [164]. Also, Transformer-XH applies eXtra Hop attention for structured'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 14, 'page_label': '15', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='applications are performed for molecular data [163] or textual graph represen-\\ntation [164]. Also, Transformer-XH applies eXtra Hop attention for structured\\ntext data [165]. AttentionXML is a tree-based model for extreme multi-label\\ntext classiﬁcation [166]. Besides, attention mechanism is handled in a Bayesian\\nframework [167]. For a better understanding of Transformers, an identiﬁabil-\\nity analysis of self-attention weights is conducted in addition to presenting\\neﬀective attention to improve explanatory interpretations [168]. Lastly,Vision\\nTransformer (ViT) processes an image using a standard Transformer encoder\\nas used in NLP by interpreting it as a sequence of patches, and performs well\\non image classiﬁcation tasks [169].'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 15, 'page_label': '16', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='16 Derya Soydaner\\n5.3 What about complexity?\\nAll these aforementioned studies undoubtedly demonstrate signiﬁcant success.\\nBut success not make one great. The Transformer also brings a very high\\ncomputational complexity and memory cost. The necessity of storing atten-\\ntion matrix to compute the gradients with respect to queries, keys and val-\\nues causes a non-negligible quadratic computation and memory requirements.\\nTraining the Transformer is a slow process for very long sequences because\\nof its quadratic complexity. There is also time complexity which is quadratic\\nwith respect to the sequence length. In order to improve the Transformer in\\nthis respect, recent studies have been conducted to improve this issue. One\\nof them is Linear Transformer which expresses the self-attention as a linear\\ndot-product of kernel feature maps [170]. Linear Transformer reduces both\\nmemory and time complexity by changing the self-attention from the softmax'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 15, 'page_label': '16', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='dot-product of kernel feature maps [170]. Linear Transformer reduces both\\nmemory and time complexity by changing the self-attention from the softmax\\nfunction in Eq. (8) to a feature map based dot-product attention. Its per-\\nformance is competitive with the vanilla Transformer architecture on image\\ngeneration and automatic speech recognition tasks while being faster during\\ninference. On the other side, FMMformers which use the idea of thefast multi-\\npole method (FMM) [171] outperform the linear Transformer by decomposing\\nthe attention matrix into near-ﬁeld and far-ﬁeld attention with linear time and\\nmemory complexity [172].\\nAnother suggestion made in response to the Transformer’s quadratic na-\\nture is The Reformer that replaces dot-product attention by one that uses\\nlocality-sensitive hashing [173]. It reduces the complexity but one limitation\\nof the Reformer is its requirement for the queries and keys to be identical. Set'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 15, 'page_label': '16', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='locality-sensitive hashing [173]. It reduces the complexity but one limitation\\nof the Reformer is its requirement for the queries and keys to be identical. Set\\nTransformer aims to reduce computation time of self-attention from quadratic\\nto linear by using an attention mechanism based on sparse Gaussian process\\nliterature [174]. Routing Transformer aims to reduce the overall complexity\\nof attention by learning dynamic sparse attention patterns by using routing\\nattention with clustering [175]. It applies k-means clustering to model sparse\\nattention matrices. At ﬁrst, queries and keys are assigned to clusters. The at-\\ntention scheme is determined by considering only queries and keys from the\\nsame cluster. Thus, queries are routed to keys belonging to the same cluster\\n[175].\\nSparse Transformer introduces sparse factorizations of the attention ma-\\ntrix by using factorized self-attention, and avoids the quadratic growth of com-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 15, 'page_label': '16', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='[175].\\nSparse Transformer introduces sparse factorizations of the attention ma-\\ntrix by using factorized self-attention, and avoids the quadratic growth of com-\\nputational burden [176]. It also shows the possibility of modeling sequences\\nof length one million or more by using self-attention in theory. In the Trans-\\nformer, all the attention heads with the softmax attention assign a non-zero\\nweight to all context words. Adaptively Sparse Transformer replaces softmax\\nwith α-entmax which is a diﬀerentiable generalization of softmax allowing\\nlow-scoring words to receive precisely zero weight [177]. By means of context-\\ndependent sparsity patterns, the attention heads become ﬂexible in the Adap-\\ntively Sparse Transformer. Random feature attention approximates softmax\\nattention with random feature methods [178]. Skyformer replaces softmax\\nwith a Gaussian kernel and adapts Nystr¨ om method [179]. A sparse atten-'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 16, 'page_label': '17', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Attention Mechanism in Neural Networks: 17\\ntion mechanism named BIGBIRD aims to reduce the quadratic dependency\\nof Transformer-based models to linear [180]. Diﬀerent from the similar stud-\\nies, BIGBIRD performs well for genomics data alongside NLP tasks such as\\nquestion answering.\\nMusic Transformer [181] shows that self-attention can also be useful for\\nmodeling music. This study emphasizes the infeasibility of the relative po-\\nsition representations introduced by [103] for long sequences because of the\\nquadratic intermediate relative information in the sequence length. Therefore,\\nthis study presents an extended version of relative attention named relative\\nlocal attention that improves the relative attention for longer musical com-\\npositions by reducing its intermediate memory requirement to linear in the\\nsequence length. A softmax-free Transformer (SOFT ) is presented to improve\\nthe computational eﬃciency of ViT. It uses Gaussian kernel function instead'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 16, 'page_label': '17', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='sequence length. A softmax-free Transformer (SOFT ) is presented to improve\\nthe computational eﬃciency of ViT. It uses Gaussian kernel function instead\\nof the dot-product similarity [182].\\nAdditionally, various approaches have been presented in Hierarchical Vi-\\nsual Transformer [183], Long-Short Transformer (Transformer-LS)[184], Per-\\nceiver [185], and Performer [186]. Image Transformer based on the cross-\\ncovariance matrix between keys and queries is applied [187], and a new vi-\\nsion Transformer is proposed [188]. Furthermore, a Bernoulli sampling atten-\\ntion mechanism decreases the quadratic complexity to linear [189]. A novel\\nlinearized attention mechanism performs well on object detection, instance\\nsegmentation, and stereo depth estimation [190]. A study shows that kernel-\\nized attention with relative positional encoding can be calculated using Fast\\nFourier Transform and it leads to get rid of the quadratic complexity for long'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 16, 'page_label': '17', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='ized attention with relative positional encoding can be calculated using Fast\\nFourier Transform and it leads to get rid of the quadratic complexity for long\\nsequences [191]. A linear uniﬁed nested attention mechanism namely Luna\\nuses two nested attention functions to approximate the softmax attention in\\nTransformer to achieve linear time and space complexity [192].\\n6 Concluding Remarks: A New Hope\\nInspired by the human visual system, the attention mechanisms in neural net-\\nworks have been developing for a long time. In this study, we examine this\\nduration beginning with its roots up to the present time. Some mechanisms\\nhave been modiﬁed, or novel mechanisms have emerged in this period. Today,\\nthis journey has reached a very important stage. The idea of incorporating\\nattention mechanisms into deep neural networks has led to state-of-the-art re-\\nsults for a large variety of tasks. Self-attention mechanisms and GPT-n family'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 16, 'page_label': '17', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='attention mechanisms into deep neural networks has led to state-of-the-art re-\\nsults for a large variety of tasks. Self-attention mechanisms and GPT-n family\\nmodels have become a new hope for more advanced models. These promising\\nprogress bring the questions whether the attention could help further devel-\\nopment, replace the popular neural network layers, or could be a better idea\\nthan the existing attention mechanisms? It is still an active research area and\\nmuch to learn we still have, but it is obvious that more powerful systems are\\nawaiting when neural networks and attention mechanisms join forces.'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 17, 'page_label': '18', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='18 Derya Soydaner\\nConﬂict of interest\\nThe author declares that she has no conﬂict of interest.\\nReferences\\n1. I. Goodfellow, Y. Bengio, A. Courville, The MIT Press (2016)\\n2. D. Noton, L. Stark, Scientiﬁc American 224(6), 34 (1971)\\n3. D. Noton, L. Stark, Vision Research 11, 929 (1971)\\n4. E. Alpaydın, Advances in Neural Information Processing Systems 8 pp. 771–777 (1995)\\n5. S. Ahmad, Advances in Neural Information Processing Systems 4 pp. 420–427 (1991)\\n6. M. Posner, S. Petersen, Annual Review of Neuroscience 13(1), 25 (1990)\\n7. C. Bundesen, Psychological Review 97(4), 523 (1990)\\n8. R. Desimone, J. Duncan, Annual Review of Neuroscience 18(1), 193 (1995)\\n9. M. Corbetta, G. Shulman, Nature Reviews Neuroscience 3(3), 201 (2002)\\n10. S. Petersen, M. Posner, Annual Review of Neuroscience 35, 73 (2012)\\n11. R. Rimey, C. Brown, Technical Report, University of Rochester (1990)\\n12. B. Sheliga, L. Riggio, G. Rizzolatti, Experimental Brain Research 98(3), 507 (1994)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 17, 'page_label': '18', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='11. R. Rimey, C. Brown, Technical Report, University of Rochester (1990)\\n12. B. Sheliga, L. Riggio, G. Rizzolatti, Experimental Brain Research 98(3), 507 (1994)\\n13. B. Sheliga, L. Riggio, G. Rizzolatti, Experimental Brain Research 105(2), 261 (1995)\\n14. J. Hoﬀman, B. Subramaniam, Perception and Psychophysics 57(6), 787 (1995)\\n15. S. Chaudhari, et al., ACM Transactions on Intelligent Systems and Technology (TIST)\\npp. 1–32 (2021)\\n16. A. Galassi, et al., IEEE Transactions on Neural Networks and Learning Systems (2020)\\n17. J. Lee, et al., ACM Transactions on Knowledge Discovery from Data (TKDD) 13(6),\\n1 (2019)\\n18. T. Brown, et al., Advances in Neural Information Processing Systems 33 pp. 1877–1901\\n(2020)\\n19. A. Vaswani, et al., Advances in Neural Information Processing Systems 30 pp. 5998–\\n6008 (2017)\\n20. K. Fukushima, Biological Cybernetics 36, 193 (1980)\\n21. K. Fukushima, Applied Optics 26(23), 4985 (1987)\\n22. K. Fukushima, T. Imagawa, Neural Networks 6(1), 33 (1993)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 17, 'page_label': '18', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='6008 (2017)\\n20. K. Fukushima, Biological Cybernetics 36, 193 (1980)\\n21. K. Fukushima, Applied Optics 26(23), 4985 (1987)\\n22. K. Fukushima, T. Imagawa, Neural Networks 6(1), 33 (1993)\\n23. E. Postma, H.V. den Herik, P. Hudson, Neural Networks 10(6), 993 (1997)\\n24. J. Schmidhuber, R. Huber, International Journal of Neural Systems pp. 125–134 (1991)\\n25. R. Milanese, et al., IEEE Computer Society Conference on Computer Vision and Pat-\\ntern Recoginition, Seattle, WA, USA pp. 781–785 (1994)\\n26. J. Tsotsos, et al., Artiﬁcial Intelligence 78(1-2), 507 (1995)\\n27. S. Culhane, J. Tsotsos, Proceedings of the 11th IAPR International Conference on\\nPattern Recognition, The Hague, Netherlands pp. 36–40 (1992)\\n28. D. Reisfeld, H. Wolfson, Y. Yeshurun, International Journal of Computer Vision14(2),\\n119 (1995)\\n29. I. Rybak, et al., Vision Research 38(15-16), 2387 (1998)\\n30. J. Keller, et al., Pattern Analysis and Applications 2(3) (1999)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 17, 'page_label': '18', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='119 (1995)\\n29. I. Rybak, et al., Vision Research 38(15-16), 2387 (1998)\\n30. J. Keller, et al., Pattern Analysis and Applications 2(3) (1999)\\n31. F. Miau, L. Itti, Proceedings of the 23rd Annual International Conference of the IEEE\\nEngineering in Medicine and Biology Society, Istanbul, Turkey pp. 789–792 (2001)\\n32. W. Zhang, et al., Advances in Neural Information Processing Systems 19 pp. 1609–1616\\n(2006)\\n33. A. Salah, E. Alpaydın, L. Akarun, IEEE Transactions on Pattern Analysis and Machine\\nIntelligence 24(3), 420 (2002)\\n34. D. Walther, et al., International Workshop on Biologically Motivated Computer Vision,\\nSpringer, Berlin, Heidelberg pp. 472–479 (2002)\\n35. K. Schill, et al., Journal of Electronic Imaging 10(1), 152 (2001)\\n36. L. Paletta, G. Fritz, C. Seifert, International Conference on Machine Learning (2005)\\n37. O.L. Meur, et al., IEEE Transactions on Pattern Analysis and Machine Intelligence\\n28(5), 802– (2006)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 18, 'page_label': '19', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Attention Mechanism in Neural Networks: 19\\n38. S. Gould, et al., International Joint Conference on Artiﬁcial Intelligence (IJCAI) pp.\\n2115–2121 (2007)\\n39. H. Larochelle, G. Hinton, Advances in Neural Information Processing Systems 23 pp.\\n1243–1251 (2010)\\n40. L. Bazzani, et al., International Conference on Machine Learning (2011)\\n41. V. Mnih, et al., Advances in Neural Information Processing Systems 27 pp. 2204–2212\\n(2014)\\n42. M. Stollenga, et al., Advances in Neural Information Processing Systems 27 pp. 3545–\\n3553 (2014)\\n43. Y. Tang, N. Srivastava, R. Salakhutdinov, Advances in Neural Information Processing\\nSystems 27 (2014)\\n44. D. Bahdanau, K. Cho, Y. Bengio, International Conference on Learning Representa-\\ntions (2015)\\n45. I. Sutskever, O. Vinyals, Q. Le, Advances in Neural Information Processing Systems\\n27 pp. 3104–3112 (2014)\\n46. K. Cho, et al., Proceedings of the 2014 Conference on Empirical Methods in Natural\\nLanguage Processing (EMNLP) pp. 1724–1734 (2014)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 18, 'page_label': '19', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='27 pp. 3104–3112 (2014)\\n46. K. Cho, et al., Proceedings of the 2014 Conference on Empirical Methods in Natural\\nLanguage Processing (EMNLP) pp. 1724–1734 (2014)\\n47. M. Schuster, K. Paliwal, IEEE Transactions on Signal Processing 45(11), 2673 (1997)\\n48. K. Xu, et al., International Conference on Machine Learning pp. 2048–2057 (2015)\\n49. O. Vinyals, et al., In Proceedings of the IEEE Conference on Computer Vision and\\nPattern Recognition pp. 3156–3164 (2015)\\n50. R. Williams, Machine Learning 8(3-4), 229 (1992)\\n51. M.T. Luong, H.P..C. Manning, Proceedings of the 2015 Conference on Empirical Meth-\\nods in Natural Language Processing, Lisbon, Portugal pp. 1412–1421 (2015)\\n52. J. Lu, et al., Advances in Neural Information Processing Systems 29 (2016)\\n53. J. Weston, S. Chopra, A. Bordes, International Conference on Learning Representa-\\ntions (2014)\\n54. A. Graves, G. Wayne, I. Danihelka, arXiv preprint arXiv:1410.5401 (2014)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 18, 'page_label': '19', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='53. J. Weston, S. Chopra, A. Bordes, International Conference on Learning Representa-\\ntions (2014)\\n54. A. Graves, G. Wayne, I. Danihelka, arXiv preprint arXiv:1410.5401 (2014)\\n55. S. Sukhbaatar, et al., Advances in Neural Information Processing Systems 28 pp. 2440–\\n2448 (2015)\\n56. J. Cheng, L. Dong, M. Lapata, Proceedings of the 2016 Conference on Empirical\\nMethods in Natural Language Processing pp. 551–561 (2016)\\n57. A. Parikh, et al., Proceedings of the 2016 Conference on Empirical Methods in Natural\\nLanguage Processing, Austin, Texas pp. 2249–2255 (2016)\\n58. Q. You, et al., In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition (CVPR), Las Vegas, NV pp. 4651–4659 (2016)\\n59. A. Rush, S. Chopra, J. Weston, Proceedings of the 2015 Conference on Empirical\\nMethods in Natural Language Processing, Lisbon, Portugal pp. 379–389 (2015)\\n60. D. Yu, et al., Interspeech pp. 17–21 (2016)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 18, 'page_label': '19', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Methods in Natural Language Processing, Lisbon, Portugal pp. 379–389 (2015)\\n60. D. Yu, et al., Interspeech pp. 17–21 (2016)\\n61. J. Chorowski, et al., Advances in Neural Information Processing Systems 28 pp. 577–\\n585 (2015)\\n62. M. Zanﬁr, E. Marinoiu, C. Sminchisescu, In Asian Conference on Computer Vision,\\nSpringer, Cham pp. 104—-119 (2016)\\n63. Y. Cheng, et al., Proceedings of the 25th International Joint Conference on Artiﬁcial\\nIntelligence (2016)\\n64. T. Rockt International Conference on Learning Representations (2016)\\n65. Y. Zhu, et al., Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition pp. 4995–5004 (2016)\\n66. K. Chen, et al., arXiv preprint arXiv:1511.05960 (2015)\\n67. H. Xu, K. Saenko, In European Conference on Computer Vision pp. 451–466 (2016)\\n68. W. Yin, et al., Transactions of the Association for Computational Linguistics 4, 259\\n(2016)\\n69. S. Sharma, R. Kiros, R. Salakhutdinov, International Conference on Learning Repre-\\nsentations (2016)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 18, 'page_label': '19', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='(2016)\\n69. S. Sharma, R. Kiros, R. Salakhutdinov, International Conference on Learning Repre-\\nsentations (2016)\\n70. Z. Yang, et al., In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition pp. 21–29 (2016)\\n71. I. Sorokin, et al., arXiv preprint arXiv:1512.01693 (2015)\\n72. J. Ba, et al., Advances in Neural Information Processing Systems 28 pp. 2593–2601\\n(2015)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 19, 'page_label': '20', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='20 Derya Soydaner\\n73. K. Gregor, et al., International Conference on Machine Learning pp. 1462–1471 (2015)\\n74. E. Mansimov, et al., International Conference on Learning Representations (2016)\\n75. S. Reed, et al., Advances in Neural Information Processing Systems 29 pp. 217–225\\n(2016)\\n76. E. Voita, et al., In Proceedings of the 57th Annual Meeting of the Association for\\nComputational Linguistics, Florence, Italy pp. 5797–5808 (2019)\\n77. G. Kerg, et al., Advances in Neural Information Processing Systems 33 (2020)\\n78. J.B. Cordonnier, A. Loukas, M. Jaggi, International Conference on Learning Repre-\\nsentations (2020)\\n79. Z. Lin, et al., International Conference on Learning Representations (2017)\\n80. R. Paulus, C. Xiong, R. Socher, International Conference on Learning Representations\\n(2018)\\n81. N. Kitaev, D. Klein, In Proceedings of the 56th Annual Meeting of the Association for\\nComputational Linguistics (Long papers) pp. 2676–2686 (2018)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 19, 'page_label': '20', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='(2018)\\n81. N. Kitaev, D. Klein, In Proceedings of the 56th Annual Meeting of the Association for\\nComputational Linguistics (Long papers) pp. 2676–2686 (2018)\\n82. D. Povey, et al., IEEE International Conference on Acoustics, Speech and Signal Pro-\\ncessing (ICASSP), IEEE pp. 5874–5878 (2018)\\n83. A. Vyas, et al., Advances in Neural Information Processing Systems 33 (2020)\\n84. W. Chan, et al., IEEE International Conference on Acoustics, Speech and Signal Pro-\\ncessing (ICASSP), Shanghai pp. 4960—-4964 (2016)\\n85. M. Sperber, et al., In proceedings of Annual Conference of the International Speech\\nCommunication Association (InterSpeech) pp. 3723–3727 (2018)\\n86. L. Kaiser, et al., arXiv preprint arXiv:1706.05137 (2017)\\n87. C. Xu, et al., Proceedings of the 56th Annual Meeting of the Association for Compu-\\ntational Linguistics (Short papers), Melbourne, Australia pp. 778–783 (2018)\\n88. S. Maruf, A. Martins, G. Haﬀari, Proceedings of NAACL-HLT, Minneapolis, Minnesota\\npp. 3092–3102 (2019)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 19, 'page_label': '20', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='tational Linguistics (Short papers), Melbourne, Australia pp. 778–783 (2018)\\n88. S. Maruf, A. Martins, G. Haﬀari, Proceedings of NAACL-HLT, Minneapolis, Minnesota\\npp. 3092–3102 (2019)\\n89. P. Ramachandran, et al., Advances in Neural Information Processing Systems 32 pp.\\n68–80 (2019)\\n90. Y. Li, et al., International Conference on Machine Learning (2019)\\n91. I. Goodfellow, et al., Advances in Neural Information Processing Systems 27 pp. 2672–\\n2680 (2014)\\n92. H. Zhang, et al., International Conference on Machine Learning pp. 7354–7363 (2019)\\n93. T. Xu, et al., In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition (CVPR) pp. 1316–1324 (2018)\\n94. A. Yu, et al., International Conference on Learning Representations (2018)\\n95. J. Zhang, et al., Conference on Uncertainty in Artiﬁcial Intelligence (2018)\\n96. D. Romero, et al., International Conference on Machine Learning (2020)\\n97. R. Al-Rfou, et al., AAAI Conference on Artiﬁcial Intelligence 33, 3159 (2019)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 19, 'page_label': '20', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='96. D. Romero, et al., International Conference on Machine Learning (2020)\\n97. R. Al-Rfou, et al., AAAI Conference on Artiﬁcial Intelligence 33, 3159 (2019)\\n98. J. Du, et al., Proceedings of the 2018 Conference on Empirical Methods in Natural\\nLanguage Processing pp. 2216–2225 (2018)\\n99. X. Li, et al., Advances in Neural Information Processing Systems 33 (2020)\\n100. B. Yang, et al., AAAI Conference on Artiﬁcial Intelligence 33, 387 (2019)\\n101. B. Yang, et al., Proceedings of the 2018 Conference on Empirical Methods in Natural\\nLanguage Processing, Brussels, Belgium pp. 4449–4458 (2018)\\n102. Proceedings of the IEEE International Conference on Computer Vision pp. 3286–3295\\n103. P. Shaw, J. Uszkoreit, A. Vaswani, Proceedings of NAACL-HLT, New Orleans,\\nLouisiana pp. 464–468 (2018)\\n104. T. Shen, et al., AAAI Conference on Artiﬁcial Intelligence pp. 5446–5455 (2018)\\n105. T. Shen, et al., In Proceedings of the 27th International Joint Conference on Artiﬁcial'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 19, 'page_label': '20', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='104. T. Shen, et al., AAAI Conference on Artiﬁcial Intelligence pp. 5446–5455 (2018)\\n105. T. Shen, et al., In Proceedings of the 27th International Joint Conference on Artiﬁcial\\nIntelligence, (IJCAI-18) pp. 4345–4352 (2018)\\n106. H. Le, T. Tran, S. Venkatesh, International Conference on Machine Learning (2020)\\n107. T. Shen, et al., International Conference on Learning Representations (2018)\\n108. S. Bhojanapalli, et al., International Conference on Machine Learning (2020)\\n109. Y. Tay, et al., International Conference on Machine Learning (2020)\\n110. S. Sukhbaatar, et al., Proceedings of the 57th Annual Meeting of the Association for\\nComputational Linguistics, Florence, Italy pp. 331–335 (2019)\\n111. Y. Jernite, et al., International Conference on Learning Representations (2017)\\n112. R. Shu, H. Nakayama, In Proceedings of the First Workshop on Neural Machine Trans-\\nlation, Vancouver, Canada pp. 1–10 (2017)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 20, 'page_label': '21', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Attention Mechanism in Neural Networks: 21\\n113. J. Hao, et al., Proceedings of NAACL-HLT, Minneapolis, Minnesota pp. 1198–1207\\n(2019)\\n114. X. Huang, et al., International Conference on Machine Learning (2020)\\n115. V. Shiv, C. Quirk, Advances in Neural Information Processing Systems 32 pp. 12,081–\\n12,091 (2019)\\n116. Z. Li, et al., International Conference on Machine Learning (2020)\\n117. Y. Hoshen, Advances in Neural Information Processing Systems 30, Long Beach, CA,\\nUSA (2017)\\n118. S. Hu, et al., International Conference on Learning Representations (2021)\\n119. E. Parisotto, R. Salakhutdinov, International Conference on Learning Representations\\n(2021)\\n120. S. Wu, et al., Advances in Neural Information Processing Systems 33 (2020)\\n121. A. Bosselut, et al., Proceedings of the 57th Annual Meeting of the Association for\\nComputational Linguistics (2019)\\n122. D. So, C. Liang, Q. Le, International Conference on Machine Learning (2019)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 20, 'page_label': '21', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Computational Linguistics (2019)\\n122. D. So, C. Liang, Q. Le, International Conference on Machine Learning (2019)\\n123. K. Choi, et al., International Conference on Machine Learning (2020)\\n124. C. Doersch, A. Gupta, A. Zisserman, Advances in Neural Information Processing Sys-\\ntems 33 pp. 21,981–21,993 (2020)\\n125. N. Carion, et al., European Conference on Computer Vision pp. 213—-229 (2020)\\n126. X. Zhu, et al., International Conference on Learning Representations (2021)\\n127. X. Liu, et al., International Conference on Machine Learning pp. 6327–6335 (2020)\\n128. J. Kasai, et al., International Conference on Machine Learning (2020)\\n129. D. Hudson, L. Zitnick, International Conference on Machine Learning pp. 4487–4499\\n(2021)\\n130. A. Radford, et al., Technical Report, OpenAI (2018)\\n131. A. Radford, et al., OpenAI blog p. 9 (2019)\\n132. M. Dehghani, et al., International Conference on Learning Representations (2019)\\n133. N. Parmar, International Conference on Machine Learning (2018)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 20, 'page_label': '21', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='132. M. Dehghani, et al., International Conference on Learning Representations (2019)\\n133. N. Parmar, International Conference on Machine Learning (2018)\\n134. Z. Dai, et al., Proceedings of the 57th Annual Meeting of the Association for Compu-\\ntational Linguistics pp. 2978–2988 (2019)\\n135. E. Parisotto, International Conference on Machine Learning (2020)\\n136. X. Ma, et al., Advances in Neural Information Processing Systems 32 pp. 2232–2242\\n(2019)\\n137. L. Lathauwer, SIAM Journal on Matrix Analysis and Applications 30(3), 1033 (2008)\\n138. L. Tucker, Psychometrika 31(3), 279 (1966)\\n139. J. Devlin, et al., Proceedings of NAACL-HLT 2019 pp. 4171–4186 (2019)\\n140. W. Taylor, Journalism Bulletin 30(4), 415 (1953)\\n141. K. Clark, et al., arXiv preprint arXiv:1906.04341 (2019)\\n142. S. Sun, et al., Proceedings of the 2019 Conference on Empirical Methods in Natural\\nLanguage Processing and the 9th International Joint Conference on Natural Language\\nProcessing, Hong Kong, China pp. 4323–4332 (2019)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 20, 'page_label': '21', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='Language Processing and the 9th International Joint Conference on Natural Language\\nProcessing, Hong Kong, China pp. 4323–4332 (2019)\\n143. W. Wang, et al., Advances in Neural Information Processing Systems 33 (2020)\\n144. J. McCarley, R. Chakravarti, A. Sil, arXiv preprint arXiv:1910.06360 (2020)\\n145. O. Zafrir, et al., The 5th Workshop on Energy Eﬃcient Machine Learning and Cogni-\\ntive Computing - NeurIPS (2019)\\n146. M. Joshi, et al., In Proceedings of the 2019 Conference on Empirical Methods in\\nNatural Language Processing and the 9th International Joint Conference on Natural\\nLanguage Processing pp. 5803–5808 (2019)\\n147. L. Gong, et al., International Conference on Machine Learning pp. 2337–2346 (2019)\\n148. Z. Lan, et al., International Conference on Learning Representations (2020)\\n149. S. Goyal, et al., International Conference on Machine Learning (2020)\\n150. X. Jiao, et al., arXiv preprint arXiv:1909.10351 (2019)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 20, 'page_label': '21', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='149. S. Goyal, et al., International Conference on Machine Learning (2020)\\n150. X. Jiao, et al., arXiv preprint arXiv:1909.10351 (2019)\\n151. M. Joshi, et al., Transactions of the Association for Computational Linguistics 8, 64\\n(2020)\\n152. Y. Liu, et al., arXiv preprint arXiv:1907.11692 (2019)\\n153. P. He, et al., International Conference on Learning Representations (2021)\\n154. V. Sanh, et al., the 5th Workshop on Energy Eﬃcient Machine Learning and Cognitive\\nComputing - NeurIPS (2019)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 21, 'page_label': '22', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='22 Derya Soydaner\\n155. W. Wang, et al., International Conference on Learning Representations (2020)\\n156. S. Shen, et al., AAAI Conference on Artiﬁcial Intelligence 34, 8815 (2020)\\n157. J. Lee, et al., Bioinformatics 36(4), 1234 (2020)\\n158. P. Prakash, et al., AAAI Conference on Artiﬁcial Intelligence 35, 453 (2021)\\n159. Z. Wu, et al., International Conference on Learning Representations (2020)\\n160. S. Mehta, et al., International Conference on Learning Representations (2021)\\n161. Y. Tay, et al., International Conference on Learning Representations (2021)\\n162. S. Yun, et al., International Conference on Learning Representations (2018)\\n163. Y. Rong, et al., Advances in Neural Information Processing Systems 33 (2020)\\n164. J. Yang, et al., Advances in Neural Information Processing Systems 34 (2021)\\n165. C. Zhao, et al., International Conference on Learning Representations (2020)\\n166. R. You, et al., Advances in Neural Information Processing Systems 32 (2019)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 21, 'page_label': '22', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='165. C. Zhao, et al., International Conference on Learning Representations (2020)\\n166. R. You, et al., Advances in Neural Information Processing Systems 32 (2019)\\n167. X. Fan, et al., Advances in Neural Information Processing Systems 33 (2020)\\n168. G. Brunner, et al., International Conference on Learning Representations (2020)\\n169. A. Dosovitskiy, et al., International Conference on Learning Representations (2021)\\n170. A. Katharopoulos, et al., International Conference on Machine Learning (2020)\\n171. L. Greengard, V. Rokhlin, Journal of Computational Physics 73(2), 325– (1987)\\n172. T. Nguyen, et al., Advances in Neural Information Processing Systems 34 (2021)\\n173. N. Kitaev, L. Kaiser, A. Levskaya, International Conference on Learning Representa-\\ntions (2020)\\n174. J. Lee, et al., International Conference on Machine Learning pp. 3744–3753 (2019)\\n175. A. Roy, et al., Transactions of the Association for Computational Linguistics pp. 53–68\\n(2020)'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 21, 'page_label': '22', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='tions (2020)\\n174. J. Lee, et al., International Conference on Machine Learning pp. 3744–3753 (2019)\\n175. A. Roy, et al., Transactions of the Association for Computational Linguistics pp. 53–68\\n(2020)\\n176. R. Child, et al., arXiv preprint arXiv:1904.10509 (2019)\\n177. G. Correia, V. Niculae, A. Martins, Proceedings of the 2019 Conference on Empirical\\nMethods in Natural Language Processing and the 9th International Joint Conference\\non Natural Language Processing pp. 2174–2184 (2019)\\n178. H. Peng, et al., International Conference on Learning Representations (2021)\\n179. Y. Chen, et al., Advances in Neural Information Processing Systems 34 (2021)\\n180. M. Zaheer, et al., Advances in Neural Information Processing Systems 33 (2020)\\n181. C.Z. Huang, et al., International Conference on Learning Representations (2019)\\n182. J. Lu, et al., Advances in Neural Information Processing Systems 34 (2021)\\n183. Z. Pan, et al., Proceedings of the IEEE/CVF International Conference on Computer'),\n",
       " Document(metadata={'producer': 'pdfTeX-1.40.21', 'creator': 'LaTeX with hyperref', 'creationdate': '2022-04-29T00:26:20+00:00', 'author': '', 'keywords': '', 'moddate': '2022-04-29T00:26:20+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.21 (TeX Live 2020) kpathsea version 6.3.2', 'subject': '', 'title': '', 'trapped': '/False', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 22, 'page': 21, 'page_label': '22', 'source_file': 'attention.pdf', 'file_type': 'pdf'}, page_content='182. J. Lu, et al., Advances in Neural Information Processing Systems 34 (2021)\\n183. Z. Pan, et al., Proceedings of the IEEE/CVF International Conference on Computer\\nVision pp. 377–386 (2021)\\n184. C. Zhu, et al., Advances in Neural Information Processing Systems 34 (2021)\\n185. A. Jaegle, et al., International Conference on Machine Learning pp. 4651–4664 (2021)\\n186. K. Choromanski, et al., International Conference on Learning Representations (2021)\\n187. A. El-Nouby, et al., Advances in Neural Information Processing Systems 34 (2021)\\n188. Q. Yu, et al., Advances in Neural Information Processing Systems 34 (2021)\\n189. Z. Zeng, et al., International Conference on Machine Learning pp. 12,321–12,332 (2021)\\n190. Z. Shen, et al., Proceedings of the IEEE/CVF Winter Conference on Applications of\\nComputer Vision pp. 3531–3539 (2021)\\n191. S. Luo, et al., Advances in Neural Information Processing Systems 34 (2021)\\n192. X. Ma, et al., Advances in Neural Information Processing Systems 34 (2021)'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 0, 'page_label': '1', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nQZhou-Embedding Technical Report\\nPeng Yu, En Xu, Bin Chen, Haibiao Chen, Yinfei Xu\\nKingsoft AI ∗\\nAugust 2025\\nAbstract\\nWe present QZhou-Embedding, a general-purpose contextual text embed-\\nding model with exceptional text representation capabilit ies. Built upon the\\nQwen2.5-7B-Instruct foundation model, we designed a uniﬁe d multi-task frame-\\nwork comprising specialized data transformation and train ing strategies. The\\ndata transformation scheme enables the incorporation of mo re diverse textual\\ntraining datasets, while the task-speciﬁc training strate gies enhance model learn-\\ning eﬃciency. We developed a data synthesis pipeline levera ging LLM API, in-\\ncorporating techniques such as Paraphrasing, Augmentatio n, and Hard negative\\nexample generation to improve the semantic richness and sam ple diﬃculty of\\nthe training set. Additionally, we employ a two-stage train ing strategy, compris-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 0, 'page_label': '1', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='example generation to improve the semantic richness and sam ple diﬃculty of\\nthe training set. Additionally, we employ a two-stage train ing strategy, compris-\\ning initial retrieval-focused pretraining followed by ful l-task ﬁne-tuning, enabling\\nthe embedding model to extend its capabilities based on robu st retrieval perfor-\\nmance. Our model achieves state-of-the-art results on the M TEB and CMTEB\\nbenchmarks, ranking ﬁrst on both leaderboards(August 27, 2 025), simultaneously\\nachieves state-of-the-art performance on tasks including Reranking, Clustering,\\netc. Our ﬁndings demonstrate that higher-quality, more div erse data is crucial for\\nadvancing retrieval model performance, and that leveragin g LLMs’ generative ca-\\npabilities can further optimize data quality for embedding model breakthroughs.\\nOur model weights are released on HuggingFace 1 under Apache 2.0 license. For\\nreproducibility, we provide evaluation code and instructi ons on GitHub 2.\\n1 Introduction'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 0, 'page_label': '1', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Our model weights are released on HuggingFace 1 under Apache 2.0 license. For\\nreproducibility, we provide evaluation code and instructi ons on GitHub 2.\\n1 Introduction\\nText embedding models, which transform natural language text int o mathematical vec-\\ntor representations, play an indispensable role in text mining, quest ion-answering sys-\\ntems, recommendation systems, and retrieval-augmented gener ation. Recently, LLM-\\nbased agent technology has experienced rapid development and wid espread adoption,\\nembedding models, which transform textual or multimodal data into vector represen-\\ntations for knowledge base construction, have signiﬁcantly enhan ced agent systems\\n∗ https://kingsoft.com/\\n1https://huggingface.co/Kingsoft-LLM/QZhou-Embedding\\n2https://github.com/Kingsoft-LLM/QZhou-Embedding\\narXiv:2508.21632v1  [cs.CL]  29 Aug 2025'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 1, 'page_label': '2', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nin terms of real-time performance, long-term memory, data privac y preservation, and\\nknowledge integration capabilities. With the continuous advancemen t of neural net-\\nworks and deep learning, text embeddings have evolved from early s parse representa-\\ntions (e.g., BM25[ 1]) to dense representations based on ﬁne-tuned deep networks s uch\\nas BERT[2] and T5[ 3], leading to signiﬁcant performance improvements[ 4][5][6][7][8]. In\\n2022, the rise of large language models (LLMs), exempliﬁed by ChatG PT[9], ushered in\\na new era of text embeddings based on LLM representations, includ ing models like text-\\nembedding-3-large and RepLLaMA[ 10]. Recent research on optimizing text embedding\\nmodels has explored diverse perspectives and focal points. For ins tance, to address\\nthe limitation of decoder-only architectures—where causal atten tion mechanisms re-\\nstrict token embeddings to unidirectional semantic capture—seve ral approaches have'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 1, 'page_label': '2', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='the limitation of decoder-only architectures—where causal atten tion mechanisms re-\\nstrict token embeddings to unidirectional semantic capture—seve ral approaches have\\nbeen proposed: Echo Embedding[ 11] employs input repetition and instruction design\\nto enable preceding tokens to capture subsequent token semant ics. LLM2Vec[ 12] modi-\\nﬁes attention to bi-directional mechanism to remove backward dep endency constraints.\\nConan-Embedding-v2[13] proposes a novel soft masking mechanism combined with dy-\\nnamic rank reduction. Another widely adopted approach is knowledg e distillation,\\nwhere text embeddings are treated as the ”signal states” repre senting textual seman-\\ntics. By distilling knowledge from high-performing teacher models to s tudent models,\\nthe objective is to optimize the embedding performance. For instan ce, Jasper[ 14] em-\\nploys a multi-stage knowledge distillation framework, combining with mu ltiple carefully'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 1, 'page_label': '2', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='the objective is to optimize the embedding performance. For instan ce, Jasper[ 14] em-\\nploys a multi-stage knowledge distillation framework, combining with mu ltiple carefully\\ndesigned loss functions and ﬁnally achieving superior results. Debat er[16] proposes a\\nstep-by-step thinking mechanism for embedding generation, itera tively optimizing doc-\\nument representations through continuous COT. Distillation is applie d to constrain\\nthe ﬁnal token representation to learn the optimal semantic stat es from these thinking\\nsteps. Additionally, hard negative sampling has emerged as a crucial research direc-\\ntion in text embedding models, serving as a pivotal technique for mod el optimization.\\nANCE[18] identiﬁed that conventional dense retrieval training leads to dimin ishing gra-\\ndient norms during optimization. Thus they developed an asynchron ous Approximate\\nNearest Neighbor (ANN) indexing mechanism that periodically refres hes the negative'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 1, 'page_label': '2', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='dient norms during optimization. Thus they developed an asynchron ous Approximate\\nNearest Neighbor (ANN) indexing mechanism that periodically refres hes the negative\\nsample pool using the current model parameters, thereby ensur ing the maintenance\\nof up-to-date and optimally challenging negative samples. Both Cona n-Embedding[24]\\nand its v2 version incorporated similar dynamic hard negative sampling techniques to\\nenhance model performance. NV-Embed[ 19] implemented an alternative approach by\\nleveraging their previously developed NV-Retriever’s[ 20] positive-aware negative min-\\ning strategy, including TopK-MarginPos and TopKPercPos ﬁltering m echanisms.\\nIn this work, we present QZhou-Embedding, built upon the powerfu l Qwen2.5-7B-\\nInstruct[21] model, which pushes the boundaries of text embedding capabilities. To\\nenhance the model’s semantic understanding, we designed a uniﬁed m ulti-task learn-\\ning framework that not only accommodates more diverse training da ta but also bring'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 1, 'page_label': '2', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='enhance the model’s semantic understanding, we designed a uniﬁed m ulti-task learn-\\ning framework that not only accommodates more diverse training da ta but also bring\\neﬃcient learning across three key tasks: retrieval, natural langu age inference (NLI),\\nand classiﬁcation. Our framework comprises two core components : 1. Data Trans-\\nformation: We carefully adapt data formats to the speciﬁc require ments of retrieval,\\nNLI, and classiﬁcation tasks, enabling eﬀective feature extractio n from heterogeneous\\ndata sources, signiﬁcantly beneﬁting retrieval model training. 2. Training Strategy:\\nWe designed specialized loss functions based on each task’s charact eristics, optimizing\\nmodel training eﬃciency. To further improve the robustness and g eneralization of vec-\\n2'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 2, 'page_label': '3', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\ntor representation, we propose a data synthesis method by emplo ying three techniques\\nto address data scarcity: Paraphrasing & Data augmentation for limited datasets and\\nHard negative generation for negative sample enrichment. Building u pon prior work, we\\ndesigned a strategy named ”Data Grouping Strategy”, enabling ba tch sampling within\\nsingle datasets, inadvertently increasing training diﬃculty through in-batch negative\\nsampling from the same distribution. For model training, we used a tw o-phase train-\\ning approach, through the ﬁrst-stage retrieval training and sec ond-stage full-capability\\ntraining, our model acquires a solid foundation of retrieval capabilit ies, while eﬀectively\\nextending to multiple capability dimensions. Our model achieved state -of-the-art av-\\nerage scores on CMTEB[ 22] and MTEB[ 23] benchmarks, ranking ﬁrst overall on both\\nCMTEB and MTEB leaderboards, demonstrating the eﬀectiveness o f our approach.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 2, 'page_label': '3', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='erage scores on CMTEB[ 22] and MTEB[ 23] benchmarks, ranking ﬁrst overall on both\\nCMTEB and MTEB leaderboards, demonstrating the eﬀectiveness o f our approach.\\nThe contributions of our work are summarized as follows:\\n• We propose a uniﬁed multi-task learning framework that systematic ally coordi-\\nnates both data processing and training pipelines, enhancing divers ity in datasets\\nand eﬃciency in model training ;\\n• We develop advanced data synthesis techniques powered by LLM, in cluding Para-\\nphrasing, Data augmentation, and Hard negative generation. The se methods\\nsigniﬁcantly enhance the quality of training corpora, thereby impro ving model’s\\nrobustness and generalization capabilities;\\n• We emply a two-stage training paradigm: Stage 1 focuses exclusively on retrieval\\ncapability building, establishing strong foundational retrieval perf ormance; and\\nstage 2 implements balanced training with controled retrieval/non-r etrieval task'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 2, 'page_label': '3', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='capability building, establishing strong foundational retrieval perf ormance; and\\nstage 2 implements balanced training with controled retrieval/non-r etrieval task\\nratios, achieving superior performance on classiﬁcation (CLS), pa ir classiﬁcation\\n(PairCLS), and semantic textual similarity (STS) tasks while maintain ing re-\\ntrieval eﬀectiveness;\\n• Our model achieves state-of-the-art performance on both MTE B and CMTEB\\nbenchmarks, which validates the eﬀectiveness of our proposed me thods.\\n2 Related Works\\n2.1 Text Embedding Models\\nText vector representation is a fundamental research area in na tural language processing\\n(NLP) and serves as the cornerstone for language understandin g. Early approaches re-\\nlied on sparse vector representations, such as TF-IDF[\\n25], BM25[26], and LSA[ 27]. With\\nthe advent of pretrained language models, dense contextualized r epresentations based\\non architectures like BERT[ 2] and T5[ 3] became widely studied and applied[ 4][5][6]. In'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 2, 'page_label': '3', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='the advent of pretrained language models, dense contextualized r epresentations based\\non architectures like BERT[ 2] and T5[ 3] became widely studied and applied[ 4][5][6]. In\\nthe era of large language models (LLMs), major advancements hav e led to the devel-\\nopment of LLM-based embedding models, such as text-embedding- 3-small/large (Ope-\\nnAI), E5-Mistral-7B[28], SFR-Embedding-Mistral[29], SFR-Embedding-2R[ 30], GRITLM[31],\\nLLM2Vec[12], RepLLaMA[10], BGE-en-icl[32], NV-Embed[19], gte-Qwen2-7B-Instruct[33],\\nQwen3-Embedding[34], etc. These models beneﬁt from optimized LLM architectures—suc h\\nas RoPE positional encoding[ 35], RMSNorm[ 36], and GeGLU activation[ 37]—combined\\nwith their strong semantic contextualization capabilities acquired th rough large-scale\\n3'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 3, 'page_label': '4', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\npretraining. As a result, LLM-based embeddings achieve superior p erformance in re-\\ntrieval and related tasks.\\n2.2 Embedding Model Training\\nThe mainstream approaches currently involve contrastive learning pretraining on un-\\nsupervised/weakly supervised corpora and supervised contrast ive learning training on\\nhigh-quality labeled positive and negative samples. In unsupervised le arning, early\\nwork like SimCSE[\\n7] proposed feeding continuous inputs of both original and noise-\\naugmented texts while employing contrastive learning to enhance th e model’s dis-\\ncriminative representation capability. For weakly supervised learnin g, gte[ 33] utilized\\nlarge-scale structured data (web search data, title-article pairs , etc.) for pretraining,\\nfollowed by ﬁne-tuning on high-quality open-source retrieval train ing data, achieving\\nperformance comparable to OpenAI embeddings with signiﬁcantly fe wer parameters.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 3, 'page_label': '4', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='followed by ﬁne-tuning on high-quality open-source retrieval train ing data, achieving\\nperformance comparable to OpenAI embeddings with signiﬁcantly fe wer parameters.\\nConan-Embedding[24] and v2 similarly adopted the weakly supervised pretraining &\\nsupervised ﬁne-tuning approach but incorporated techniques like cross-GPU batch loss\\nbalancing, dynamic hard negative mining, and soft masking (v2) to op timize the model.\\nSeed1.6-Embedding[38] employed a phased training strategy combining text and multi-\\nmodal pretraining followed by business-scenario-speciﬁc ﬁne-tun ing, achieving superior\\nrepresentation quality.\\nSubstantial research has also been conducted on modeling diﬀeren t tasks. Piccolo2[\\n39]\\nintroduced multi-task hybrid loss functions for diverse downstrea m tasks, an approach\\nwe also incorporate. SFR-Embedding[ 30] utilized multi-task learning techniques to\\nregularize embeddings, signiﬁcantly enhancing domain data discrimina tion. Xiaobu-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 3, 'page_label': '4', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='we also incorporate. SFR-Embedding[ 30] utilized multi-task learning techniques to\\nregularize embeddings, signiﬁcantly enhancing domain data discrimina tion. Xiaobu-\\nembedding uniﬁed the treatment of major CMTEB problem categorie s from the per-\\nspective of circle loss[ 40], fully leveraging multiple positive examples in original datasets\\nwhile carefully balancing diﬀerent loss weights.\\n2.3 Data Synthesis\\nData quantity and quality are the most critical factors in model opt imization, data\\nsynthesis methods have become a critical research direction due t o the high cost of\\nmanual annotation. Doc2Query[\\n41] and Query2Doc[ 42] employ question-answering\\nmodels to generate pseudo-queries and pseudo-documents resp ectively, enhancing data\\nfor improved RAG performance. Promptagator[ 43] addresses few-shot retrieval sce-\\nnarios by generating queries of diverse intents using few-shot dem onstrations and an-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 3, 'page_label': '4', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='for improved RAG performance. Promptagator[ 43] addresses few-shot retrieval sce-\\nnarios by generating queries of diverse intents using few-shot dem onstrations and an-\\nnotations, eﬀectively improving retrieval capabilities across varyin g intents or distri-\\nbutions. GPL[ 44] utilizes existing T5 encoder-decoder models to generate queries,\\nretrieves similar passages as hard negatives using existing retrieva l models, and em-\\nploys cross-encoders to score each (query, passage) pair. Unn atural Instructions[ 45]\\nleverages prompt and in-context learning (ICL) techniques to gen erate synthetic ex-\\namples through controlled instructions, inputs, and constraints, producing 64k diverse\\ndata entries from several seed examples with promising experiment al results. Qwen3-\\nEmbedding[34] designs a diversiﬁed prompting strategy by assigning document-s peciﬁc\\nroles to simulate potential users querying that document, enabling LLMs to generate'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 3, 'page_label': '4', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Embedding[34] designs a diversiﬁed prompting strategy by assigning document-s peciﬁc\\nroles to simulate potential users querying that document, enabling LLMs to generate\\nstylistically authentic queries that enhance diversity and realism.\\n4'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 4, 'page_label': '5', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\n2.4 Hard Negative Mining Techniques\\nHard negatives serve as essential components in contrastive lear ning for retrieval model\\ntraining. Early work like ANCE[\\n46] proposed an asynchronous ANN indexing mech-\\nanism that periodically updates hard negatives using checkpoint sta tes to maintain\\noptimally challenging samples. Conan-Embedding[ 24] and its v2 version implemented\\na dynamic hard negative sampling strategy by excluding and refresh ing samples when\\ntheir scores fall below a threshold. NV-Retriever[ 47] proposed positive-aware negative\\nmining, introducing TopK-MarginPos and TopKPercPos ﬁltering crite ria to minimize\\nfalse negatives. LGAI-Embedding[ 17] built upon NV-Retriever’s strategy with adap-\\ntive margin-based mining strategies, employing ANNA IR as a teacher retrieval model\\nto identify high-quality hard negatives while using TopKPercPos ﬁlter ing to eliminate\\nfalse negatives.\\n3 Uniﬁed Multi-task Learning Framework'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 4, 'page_label': '5', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='to identify high-quality hard negatives while using TopKPercPos ﬁlter ing to eliminate\\nfalse negatives.\\n3 Uniﬁed Multi-task Learning Framework\\nEmbedding models support numerous downstream tasks including re trieval, reranking,\\nSTS, and classiﬁcation. Given the diversity of these tasks and their associated data\\ncomplexity, we explore a uniﬁed strategy to eﬀectively handle them c ollectively while\\npromoting optimization of the embedding model. Existing research on uniﬁed task pro-\\ncessing includes circle loss[\\n40], which approaches sentence pair similarity from a global\\nperspective by categorizing tasks into class-level labels and pair-w ise labels, Xiaobu-\\nembedding demonstrated signiﬁcant improvements by adopting this approach. Other\\nmodels like Piccolo2[ 39], SFR-Embedding[ 30], NV-Embed[ 47], Conan-Embedding[ 24] ,\\nand Conan-Embedding-v2 have incorporated multi-task learning us ing diverse train-\\ning data with varying label processing methods, some employing task -speciﬁc losses'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 4, 'page_label': '5', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='and Conan-Embedding-v2 have incorporated multi-task learning us ing diverse train-\\ning data with varying label processing methods, some employing task -speciﬁc losses\\n(InfoNCE[48], Cosent[ 49], etc.).\\nOur design principle aims to accommodate more tasks and data types , enabling cross-\\ndomain and cross-task data to eﬀectively enhance embedding capa bilities. We propose\\na uniﬁed multi-task learning framework that categorizes training da ta into three task\\ntypes: retrieval, NLI, and classiﬁcation, with customized data and training solutions\\nfor each, allowing most natural text data to be converted into emb edding training data\\nthrough this framework. The following sections detail the framewo rk’s components and\\nimplementation methods.\\n3.1 Model Architecture\\nEmbedding models based on BERT or T5 [\\n39][15][50][24] exhibit powerful contextual\\nrepresentation capabilities, primarily attributed to their bidirection al attention mech-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 4, 'page_label': '5', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='3.1 Model Architecture\\nEmbedding models based on BERT or T5 [\\n39][15][50][24] exhibit powerful contextual\\nrepresentation capabilities, primarily attributed to their bidirection al attention mech-\\nanisms. However, recent large language models predominantly adop t decoder-only ar-\\nchitectures with unidirectional attention, signiﬁcantly constrainin g tokens’ ability to\\ncapture contextual information. Several studies have address ed this limitation through\\narchitectural modiﬁcations or attention mechanism optimizations[ 12][31][47]. Our work\\nbuilds upon the Qwen2.5-7B-Instruct architecture and checkpoin t due to its exceptional\\nChinese language contextual capabilities. Consequently, we impleme nted the following\\nmodiﬁcations: (1) modifying the original causal attention to bi-dire ctional attention\\n5'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 5, 'page_label': '6', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nFigure 1: QZhou-Embedding Architecture\\nto enable comprehensive context capture, and (2) employing mean pooling with sub-\\nsequent normalization to produce ﬁnal embedding vectors. The mo del architecture is\\nshown in Figure 1\\n3.2 Data Transformation\\n3.2.1 Retrieval-oriented Process\\nWhile open-source datasets such as MS MARCO[\\n64] are readily accessible, they alone\\nare insuﬃcient for further advancing embedding model capabilities, thus we supplement\\nwith data from additional sources, such as news, academic paper a nd QA datasets.\\nGiven the heterogeneous nature of these datasets across doma ins and purposes, we\\ndesign a retrieval-oriented data transformation methodology to c onvert diverse sources\\nand formats into training data suitable for retrieval task. Below we outline selected\\ncategories of training data used for transformation and their pro cessing procedures:\\n• Title-Body/Abstract ”Title-Body/Abstract” type data primarily consists of'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 5, 'page_label': '6', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='categories of training data used for transformation and their pro cessing procedures:\\n• Title-Body/Abstract ”Title-Body/Abstract” type data primarily consists of\\ntitle-body/article pairs typically sourced from online news, articles, documents,\\narXiv publications and Wikipedia. For these data types, the transfo rmation pro-\\ncess involves using the title as the query and the body/abstract as the positive\\nsample. However, since the latter are documents, truncation is ap plied when they\\nexceed the maximum training length.\\n• Claim-Evidence This data type typically presents a claim or statement followed\\nby extracted evidence that either supports or refutes it, commo nly used for multi-\\nhop fact extraction and claim veriﬁcation tasks. Datasets genera lly contain claims\\nand corresponding evidence, with each evidence instance labeled as ”Supports”\\nor ”Refutes”. The transformation process involves: converting the claim portion\\n6'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 6, 'page_label': '7', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\ninto a query sample, for evidence labeled as ”Supports”, the text is treated as a\\npositive sample; for evidence labeled as ”Refutes”, it is converted in to a negative\\nsample.\\n• Question-Answer Question-answering data and conversational Q-A pairs pri-\\nmarily originate from chat platforms and forums. Within the current wave of\\nLLM and reinforcement learning research, such data exhibits rema rkable volume\\nand diversity. Virtually single-turn Q-A datasets(one question pair ed with one\\nanswer) represents the most suitable format for retrieval train ing. For transfor-\\nmation, the ”Question/Query/User” portion is converted into que ries, while the\\n”Answer/Response/Assistant” portion is processed as documen ts.\\n3.2.2 NLI-oriented Process\\nNatural Language Inference (NLI) represents a fundamental capability of NLP models,\\nencompassing tasks such as semantic similarity, textual entailment , and sentiment anal-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 6, 'page_label': '7', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='3.2.2 NLI-oriented Process\\nNatural Language Inference (NLI) represents a fundamental capability of NLP models,\\nencompassing tasks such as semantic similarity, textual entailment , and sentiment anal-\\nysis. This section describes the methodology for transforming and constructing training\\nsets from NLI-style data, using textual semantic similarity (STS) a nd textual entailment\\ntasks as illustrative examples. Our approach distinctively reformula tes NLI tasks into\\ntext\\npair-score formats compatible with Cosent loss[ 49] training strategy, where sample\\npairs are quantitatively scored based on their semantic relationship s. The processing\\nprocedures for each are detailed below:\\n• STS Semantic Textual Similarity (STS) is characterized by its symmetric s e-\\nmantic matching to determine whether two sentences share equiva lent meaning.\\nSTS datasets typically consist of sentence pairs with associated lab els, which may'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 6, 'page_label': '7', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='mantic matching to determine whether two sentences share equiva lent meaning.\\nSTS datasets typically consist of sentence pairs with associated lab els, which may\\nbe binary classiﬁcations (yes/no, true/false) or numerical score s (e.g., 1.2, 3.1,\\n4.8). For binary labels, ”yes”/”true” are mapped to a numerical va lue of 1, while\\n”no”/”false” are converted to 0. The data is then structured int o (query, docu-\\nment, score) triplets. Due to the symmetric nature of STS, each s ingle original\\ndata sample can generate two training triplets by interchanging the query and\\npositive document roles.\\n• Textual Entailment Textual entailment further examines a model’s capabilities\\nin reasoning, typically featuring three-class labels: entailment, neu tral, contradic-\\ntion. Our processing method employs a three-tier scoring system: labels are\\nassigned values of 2, 1, and 0 for entailment, neutral, and contrad iction respec-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 6, 'page_label': '7', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='tion. Our processing method employs a three-tier scoring system: labels are\\nassigned values of 2, 1, and 0 for entailment, neutral, and contrad iction respec-\\ntively. We construct (query, document, score) triplets accordin gly, and similarly\\nleverage symmetry to double the dataset size.\\n3.2.3 CLS-oriented Process\\nClassiﬁcation tasks encompass text categorization and sentiment classiﬁcation scenar-\\nios, it typically follows a (text, label) format, where texts within the s ame category\\nexhibit semantic proximity while distinct boundaries separate diﬀeren t classes. NV-\\nEmbed[\\n47] compared label-based and example-based data construction met hods, with\\nexperimental results demonstrating the superiority of the latter . Adopting the example-\\nbased approach, we process classiﬁcation data (text, label) by us ing the text as query,\\n7'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 7, 'page_label': '8', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nFigure 2: CLS-oriented data transformation\\nsampling other texts sharing the same label as positive examples, an d selecting texts\\nfrom diﬀerent labels as negative examples. Figure 2 provides a detailed schematic\\nillustration of this process.\\n3.3 Training Strategy\\nEach task category—retrieval, NLI, and classiﬁcation—operates within a data construc-\\ntion process respectively, for which we have designed specialized tr aining objectives to\\nto enhance model training eﬃciency. This section elaborates on the design of loss\\nfunctions for retrieval, NLI, and classiﬁcation tasks.\\n3.3.1 Retrieval\\nFor the retrieval task, we adopt the widely used InfoNCE loss[\\n48], but incorporate an\\nimprovement inspired by gte[ 33] by augmenting the original query-negative loss with an\\nadditional query-query loss term. Speciﬁcally, each query within a b atch is treated as a\\nnegative sample for all other queries. The ﬁnal loss formulation is ex plicitly described'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 7, 'page_label': '8', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='additional query-query loss term. Speciﬁcally, each query within a b atch is treated as a\\nnegative sample for all other queries. The ﬁnal loss formulation is ex plicitly described\\nin Equation ( 1).\\nLRetrieval = − 1\\nn\\n∑\\ni\\nlog esim(qi,d +\\ni )/τ\\nesim(qi,d +\\ni )/τ + ∑\\nj esim(qi,d −\\nj )/τ + ∑\\nj̸=i esim(qi,q j )/τ\\n(1)\\n3.3.2 NLI\\nFor NLI tasks, the transformed labels are numerically comparable a nd exhibit ordinal\\nrelationships. We employ Cosent loss[\\n49] to optimize such data, which is designed\\nbased on the principles of Circle loss[ 40]. As a ranking-sensitive loss function, Cosent\\nloss requires only ordinal label information for optimization while demo nstrating faster\\nconvergence. Its mathematical formulation is presented in Equat ion ( 2).\\n8'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 8, 'page_label': '9', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nLNLI = log(1 +\\n∑\\nsim(i,j )>sim(k,l )\\nexp(sim(xk, x l) − sim(xi, x j)\\nτ )) (2)\\n3.3.3 CLS\\nThe classiﬁcation loss also adopts the InfoNCE objective. However , since CLS data is\\nprocessed in an example-based manner, directly applying in-batch n egative sampling\\non classiﬁcation datasets with limited categories may lead to false neg atives from items\\nof diﬀerent classes. Numerous studies have proposed diverse app roaches to address\\nthis issue[\\n51][52][47]. We propose a masking mechanism that appends class labels to\\neach positive and negative sample during preprocessing (recorded as separate variables\\nrather than modifying raw text). During in-batch negative sampling , for each negative\\nsample from other data instances, we check whether its label matc hes the current query’s\\nclass. If matched, the negative loss contribution is masked to zero to prevent erroneous'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 8, 'page_label': '9', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='sample from other data instances, we check whether its label matc hes the current query’s\\nclass. If matched, the negative loss contribution is masked to zero to prevent erroneous\\npenalization; otherwise, it is normally computed. The core loss remain s InfoNCE, with\\nthe CLS loss formulation shown in Equation ( 3). Where Cti denotes the class label of\\nsample ti, and nrepresents the number of negative samples per data instance.\\nLCLS = − 1\\nn\\n∑\\ni\\nlog esim(ti,t +\\ni )/τ\\nZi\\n(3)\\nwhere Zi = esim(ti,t +\\ni )/τ +\\n∑\\nn\\nMASK(ti, t −\\ni,n ) ·esim(ti,t −\\ni,n )/τ +\\n∑\\nj̸=i\\nMASK(ti, t j ) ·esim(ti,t j )/τ +\\n∑\\nj̸=i\\n∑\\nn\\nMASK(ti, t −\\nj,n ) ·esim(ti,t −\\nj,n )/τ\\nand Cti = Ct+\\ni\\nand MASK( ti, t j ) =\\n{\\n0 if Cti = Ctj ,\\n1 otherwise\\n4 Data Synthesis\\nThe production of higher-quality data through data production ha s gained critical im-\\nportance in embedding training. Manual annotation incurs higher co sts and lower\\nproduction eﬃciency, thus developing eﬀective automated data sy nthesis methods has'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 8, 'page_label': '9', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='portance in embedding training. Manual annotation incurs higher co sts and lower\\nproduction eﬃciency, thus developing eﬀective automated data sy nthesis methods has\\nemerged as a key research focus. Recent advancements in large la nguage models (LLMs)\\nhave signiﬁcantly improved their linguistic capabilities, enabling accura te interpretation\\nof human instructions and generation of high-quality outputs. Mult iple existing meth-\\nods have eﬀectively leveraged LLMs to generate high-quality data[\\n28][34], we similarly\\n9'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 9, 'page_label': '10', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nleverages LLM capabilities for data production across three dimens ions: structural di-\\nversity, semantic diversity, and diﬃculty, with dedicated synthesis strategies for each.\\nFor structural diversity, we propose Paraphrasing techniques; for semantic diversity,\\nwe introduce Augmentation methods; and to increase training diﬃcu lty and improve\\nsemantic discriminability, we employ LLMs to generate more challenging hard negative\\nexamples. The following sections detail these methodologies. The co nstraint compo-\\nnents for all data synthesis techniques are speciﬁed in Table 5 of Appendix A.1.\\n4.1 Structural Diversity Enhancement\\nLinguistic structures of text encompass lexical, syntactic, and gr ammatical features,\\nwhich represent relatively surface-level characteristics reﬂect ing word arrangements,\\ncombinations, tenses, voices, and other formal attributes. Emb edding models must'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 9, 'page_label': '10', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='which represent relatively surface-level characteristics reﬂect ing word arrangements,\\ncombinations, tenses, voices, and other formal attributes. Emb edding models must\\naccurately capture underlying semantics despite variations in surf ace form, ensuring\\nrobustness to external structural changes. For example, the following two sentences,\\ndespite structural diﬀerences, should be recognized as semantic ally equivalent:\\n• The cat chased the mouse.\\n• The mouse was chased by the cat.\\nTo eﬀectively train an embedding model that remains invariant to str uctural variations\\nwhile accurately capturing semantic information, we propose a Para phrasing strategy.\\nFor each training sample containing a query and a positive document, we apply LLM-\\nbased paraphrasing to both contents, generating augmented ins tances that preserve\\nsemantic equivalence while introducing structural divergence. The prompt constraints\\nand workﬂow are illustrated in Figure\\n3.\\nFigure 3: LLM-based Paraphrasing Workﬂow'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 9, 'page_label': '10', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='semantic equivalence while introducing structural divergence. The prompt constraints\\nand workﬂow are illustrated in Figure\\n3.\\nFigure 3: LLM-based Paraphrasing Workﬂow\\n4.2 Semantic Diversity Enhancement\\nMerely augmenting data through superﬁcial structural modiﬁcat ions yields negligible\\nimprovements in model capabilities, as generalization relies not only on structural dis-\\nentanglement but also on diverse topics and content to ensure unif orm vector rep-\\nresentations in the spatial domain. Therefore, beyond paraphra sing, we propose an\\naugmentation method using LLM to diversify semantics. The core co ncept is: given a\\n10'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 10, 'page_label': '11', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\ncomplete (query, positive) pair, the model must comprehend the d omain and perspec-\\ntive discussed and learn to expand into diﬀerent topics, aspects, a nd viewpoints while\\nremaining contextually anchored. This process is governed via prom pt constraints. The\\nAugmentation framework is illustrated in Figure 4.\\nFigure 4: Semantic Augmentation Workﬂow\\nFigure 5: Hard Negative Synthesis Workﬂow\\n4.3 More challenging embeddings\\nHard negative examples are crucial for enhancing the performanc e of text embedding\\nmodels, often requiring substantial eﬀort to acquire. Leveraging the linguistic capabili-\\nties of large language models, we design an automated hard negative synthesis method\\ntailored for retrieval datasets. Our domain-speciﬁc experiments demonstrate that large\\nlanguage models can generate examples that are indistinguishable, t he framework is\\nillustrated in Figure\\n5.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 10, 'page_label': '11', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='tailored for retrieval datasets. Our domain-speciﬁc experiments demonstrate that large\\nlanguage models can generate examples that are indistinguishable, t he framework is\\nillustrated in Figure\\n5.\\nDuring Data paraphrasing and Augmentation, we implement task-sp eciﬁc strategies:\\nfor retrieval tasks, we rewrite/expand (query, positive) pairs a nd add them to the orig-\\ninal dataset; for NLI tasks, we rewrite individual sentences by ra ndomly duplicating\\nexisting entries containing the original sentences and replacing the m with rewritten\\nversions to achieve data expansion—without applying augmentation to prevent ambi-\\nguity; for classiﬁcation tasks, we rewrite sentences while retaining their original labels,\\nexample-based processing was applied using the rewritten results, again without em-\\nploying augmentation. We provide several data synthesis examples in Appendix A.3\\nfor reference.\\n11'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 11, 'page_label': '12', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nFigure 6: Training pipeline\\n5 Training Optimization\\n5.1 Data Grouping Strategy\\nPrior works like Linq-Embedding[\\n52] and SFR-Embedding-Mistral[ 30] adopted task-\\nhomogeneous batching, partitioning data by task rather than mixin g them, and sam-\\npling tasks based on weighted randomness during training. Building on this, we propose\\na reﬁned Data Grouping Strategy, extending the granularity from task-level to dataset-\\nlevel partitioning. We posit that dataset-level grouping captures more domain-speciﬁc\\nclustering patterns—samples within the same dataset often exhibit inherent domain\\nsimilarities, while such consistency may not hold across datasets.\\nOur approach partitions training data into subsets by name. During training, only\\nsamples from a single dataset are sampled per batch, with ﬁle pointer s recorded to\\nenable sequential reading in subsequent iterations. For sampling we ights, we adopt\\nthe data sampling strategy from gte['),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 11, 'page_label': '12', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='enable sequential reading in subsequent iterations. For sampling we ights, we adopt\\nthe data sampling strategy from gte[\\n33] and mgte[ 50], scaling weights by dataset size\\nfollowed by normalization. For dataset i with size li, its sampling weight is computed\\nas Equation ( 4)\\npi = lα\\ni∑ m\\nj=1 lα\\nj\\n(4)\\n5.2 Two-Stage Training\\nInspired by NV-Embed’s[\\n47] two-stage contrastive learning instruction tuning tech-\\nnique, we adopt a similar training approach: the ﬁrst stage exclusive ly uses retrieval-\\noriented training data, while the second stage integrates both ret rieval and non-retrieval\\ntasks, the overall training framework is illustrated in the ﬁgure 6. Two key distinctions\\nare incorporated: ﬁrst, we integrate the previously described Da ta Grouping Strat-\\negy; second, we implement global control over the sampling ratio of retrieval training\\ndatasets, since our ﬁndings indicate that naively incorporating add itional data signiﬁ-\\ncantly degrades retrieval performance.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 11, 'page_label': '12', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='datasets, since our ﬁndings indicate that naively incorporating add itional data signiﬁ-\\ncantly degrades retrieval performance.\\nFor global control of sampling ratio, a hyperparameter η is introduced into the sampling\\n12'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 12, 'page_label': '13', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nfunction to control the proportion of retrieval training, ensurin g that throughout the\\nsecond training stage, the computational contribution of retriev al data accounts for η,\\nwhile non-retrieval data constitutes 1 − η. The following set of equations formalizes the\\ncomputational process from partitioned datasets to sampling rat io determination. Let\\nthe training data D = [ d1, d 2, ..., d N ] , where each di represents a distinct dataset (e.g.,\\nMSMARCO passage, SQUAD), with corresponding sizes L = [ l1, l 2, ..., l N ]. Following\\nthe aforementioned strategy, we ﬁrst apply an exponential scalin g factor α , a mask fac-\\ntor M is then applied to ﬁlter retrieval and non-retrieval training sets fo r summation.\\nThe equations are as follows:\\nSret =\\n∑\\ni\\nMi ·lα\\ni\\nSnon ret =\\n∑\\ni\\n(1 − Mi) ·lα\\ni\\nwhere M i =\\n{\\n0 if di ∈ RET,\\n1 else\\nwhere RET denotes the set of retrieval training datasets. The re trieval ratio is then'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 12, 'page_label': '13', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Sret =\\n∑\\ni\\nMi ·lα\\ni\\nSnon ret =\\n∑\\ni\\n(1 − Mi) ·lα\\ni\\nwhere M i =\\n{\\n0 if di ∈ RET,\\n1 else\\nwhere RET denotes the set of retrieval training datasets. The re trieval ratio is then\\nscaled using η to derive the ﬁnal normalized sampling ratios for the training sets:\\nLsamp = [ lsamp\\n1 , l samp\\n2 , ...l samp\\nN ]\\nwhere l samp\\ni =\\n{ ηRET ·lα\\ni\\nSret\\nif di ∈ RET,\\n(1−ηRET )·lα\\ni\\nSnon ret\\nelse\\n6 Experiments\\n6.1 Training Dataset\\nPrimary data sources include bge-en-icl, bge-m3-data, and bge-m ultilingual-gemma2-\\ndata\\n3 . The E5 dataset (approximately 1.5M samples) 4, utilized in E5-Mistral-7B[ 28],\\nEcho Embedding[ 11], and LLM2Vec[ 12], is also incorporated. The aforementioned\\ndatasets include commonly used retrieval training corpora such as MS MARCO (both\\npassage and document versions)[ 64], Natural Questions (NQ)[ 65], ELI5[66], HotpotQA[ 67],\\nMIRACL[68], SQuAD[ 69], FEVER[70], Quora Question Pairs(QQP), and DuReader[ 71],'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 12, 'page_label': '13', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='passage and document versions)[ 64], Natural Questions (NQ)[ 65], ELI5[66], HotpotQA[ 67],\\nMIRACL[68], SQuAD[ 69], FEVER[70], Quora Question Pairs(QQP), and DuReader[ 71],\\netc. Previous researchers have already systematically collected a nd organized these\\ndatasets, making them readily usable, we solely utilized the proposed method to update\\nharder negative samples. Stella’s[ 53] retrieval data llm 5 provides high-quality (query,\\npositive, negative) triplets, while zpoint leverages datasets such a s Huatuo medical QA 6,\\nall above data has been incorporated. Additional data from huggin gface’s sentence-\\ntransformers7 repository includes reddit, hover[ 72], mr-tydi[ 73], law-gpt, and s2orc[ 74].\\n3https://github.com/FlagOpen/FlagEmbedding/tree/master/dataset\\n4https://drive.google.com/ﬁle/d/1YqgaJIzmBIH37XBxpRPCVzV CLh6aOI4/view\\n5https://huggingface.co/datasets/infgrad/retrieval data llm\\n6https://huggingface.co/iampanda/zpoint large embedding zh'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 12, 'page_label': '13', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='4https://drive.google.com/ﬁle/d/1YqgaJIzmBIH37XBxpRPCVzV CLh6aOI4/view\\n5https://huggingface.co/datasets/infgrad/retrieval data llm\\n6https://huggingface.co/iampanda/zpoint large embedding zh\\n7https://huggingface.co/sentence-transformers\\n13'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 13, 'page_label': '14', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nOther sources encompass web questions, BioASQ[ 54], cmrc[ 55], CSL 8, nli for simcse\\n(used in SimCSE[ 7] and GTE[ 33]), MLDR 9, GLUE Benchmark[ 56], Yelp Reviews[ 57]\\nand Weibo Sentiment 10 training sets.\\nWe further integrate MTEB evaluation-related datasets like Imdb- Classiﬁcation[58],\\nMassiveIntent-Classiﬁcation[59], MassiveScenario-Classiﬁcation[59], STS12[60], LCQMC[61],\\nPAWSX[62], and STSB[ 63], we utilized the training split from these datasets with con-\\ntamination exclusion applied to remove samples highly similar to test set s.\\nFor data requiring format conversion, we apply the methodologies d escribed in Sen-\\ntion 3.2. Datasets with limited samples (e.g., subsets of bge and e5 series, Im db-\\nClassiﬁcation, STS12, LCQMC) are augmented via Paraphrasing and Augmentation\\n(typically applied to datasets with fewer than 60k samples), we ultima tely obtained ap-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 13, 'page_label': '14', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Classiﬁcation, STS12, LCQMC) are augmented via Paraphrasing and Augmentation\\n(typically applied to datasets with fewer than 60k samples), we ultima tely obtained ap-\\nproximately 5M high-quality training samples through API interfaces . We deduplicate\\nall training sets and ﬁlter out samples with low query-pos scores usin g GTE-Qwen2-7B-\\nInstruct 11. For retrieval data lacking hard negatives, we employ synthetic ha rd negative\\ngeneration. Due to API cost constraints, only 30% of hard negativ es are synthetically\\ngenerated; the remainder are produced using stella-large-zh-v3 -1792d[53], with top-10\\nto top-30 ranked results selected as hard negatives. The ﬁnal tr aining dataset contains\\n11M quadruples (query, pos, neg, instruction) in total.\\n6.2 Trainset Instructions\\nFor most training data containing instruction formats, we retain th eir original con-\\ntents. For the MTEB training set, we adopt instructions correspo nding to its evalu-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 13, 'page_label': '14', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='6.2 Trainset Instructions\\nFor most training data containing instruction formats, we retain th eir original con-\\ntents. For the MTEB training set, we adopt instructions correspo nding to its evalu-\\nation(consistent with Qwen3-Embedding runtime). For external d ata lacking instruc-\\ntions (e.g., Huatuo, Reddit, Law-GPT, GLUE), we design task-spec iﬁc and domain-\\nadaptive instructions. Partial instruction templates are provided in Appendix\\nA.2.\\n6.3 Training Details\\nAs previously mentioned, we adopt a two-stage training approach. For the ﬁrst-stage\\nretrieval training, we train on all retrieval datasets, with a warm- up step of 300 and\\na learning rate of 3e-5, the total step of training is 32k. In the sec ond stage, we use\\nall training data, set the learning rate to 2e-5, and train for 8k ste ps, keeping all other\\nconﬁgurations the same as in the ﬁrst stage. We employ a batch size of 256 for all data\\nusing the InfoNCE loss (i.e., retrieval and classiﬁcation), considerin g data using the'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 13, 'page_label': '14', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='conﬁgurations the same as in the ﬁrst stage. We employ a batch size of 256 for all data\\nusing the InfoNCE loss (i.e., retrieval and classiﬁcation), considerin g data using the\\ncosent loss (i.e., NLI), due to lower memory consumption from the ab sence of forward\\ncomputation for negative samples, the batch size is set to 768. Acr oss all stages, we\\nemploy bﬂoat16 precision, with 4 hard negative samples and a cosine t emperature of\\n0.02, using Adam optimizer with a weight decay of 0.01. The Data Group ing Strategy\\nremains unchanged between the two stages, except that the sec ond stage incorporates\\nall data with a global retrieval ratio ηRET of 0.72. Unlike existing works that commonly\\n8https://github.com/ydli-ai/CSL?tab=readme-ov-ﬁle\\n9https://huggingface.co/datasets/Shitao/MLDR\\n10https://github.com/SophonPlus/ChineseNlpCorpus?tab=readme-ov-ﬁle\\n11https://huggingface.co/Alibaba-NLP/gte-Qwen2-7B-instruct\\n14'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 14, 'page_label': '15', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nuse LoRA ﬁne-tuning, we employ full-parameter ﬁne-tuning at all st ages to ensure\\nmaximum performance improvement. The query and passage length s are set to 256\\nand 1536 respectively. However, in practice, the model can handle sequences up to 8k\\nin length due to the strong length extrapolation capability of the RoP E[35] positional\\nencoding used in most LLMs. The hyperparameter conﬁgurations f or all training stages\\nare provided in the table 1.\\nTable 1: Training Hyperparameter Speciﬁcations\\nItem Stage1 Stage2\\nWarm-up 300\\nSteps 3e-5 2e-5\\nLR 32k 8k\\nBatch Size InfoNCE 256\\nBatch Size Cosent - 768\\nPrecision bﬂoat16\\nTemperature 0.02\\nOptimizer Adam\\nQuery Length 256\\nPassage Length 1536\\n6.4 Compared Methods\\nWe selected the top-10 ranked models(August 27, 2025) on the MT EB/CMTEB leader-\\nboards prior to the release of QZhou-Embedding as baselines. For M TEB, the compar-\\native models include LGAI-Embedding-Preview['),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 14, 'page_label': '15', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='boards prior to the release of QZhou-Embedding as baselines. For M TEB, the compar-\\native models include LGAI-Embedding-Preview[\\n17], the Seed series (v1.5[ 75] , v1.6[ 38]),\\nQwen series (8B, 4B)[ 34], ritrieve zh v1, xiaobu-embedding-v2, gemini-embedding-001[ 76],\\njasper en vision language v1[14], Linq-Embed-Mistral[52], SFR-Embedding-Mistral[ 30],\\nand NV-Embed-v2[ 47]. For CMTEB, the baseline models comprise the Seed series (as\\nabove), Qwen series (as above), Conan series (v1[ 24], v2[13]), zpoint large embedding zh,\\nand piccolo-large-zh-v2[ 39].\\n6.5 Main Results\\nThis section presents the evaluation results of Qzhou-embedding o n MTEB/CMTEB\\nbenchmarks, alongside comparative scores from the top 10 ranke d models. As detailed\\nin Table\\n2, Table 3, Qzhou-embedding achieves state-of-the-art performance ac ross\\nboth task-level and task-type average metrics, demonstrating the eﬀectiveness of our\\napproach. Furthermore, under MTEB’s oﬃcial ranking protocol, Q zhou-embedding'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 14, 'page_label': '15', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='both task-level and task-type average metrics, demonstrating the eﬀectiveness of our\\napproach. Furthermore, under MTEB’s oﬃcial ranking protocol, Q zhou-embedding\\nsecured the top position on both leaderboards. ( Note: Highlighted maximum values\\nin certain columns may reﬂect the best performance among the liste d models rather\\nthan the overall leaderboard maximum, as exempliﬁed by the MTEB/c lassiﬁcation\\nbenchmark where the top score does not appear in the top 10 mode ls.)\\n15'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 15, 'page_label': '16', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nTable 2: Performance on MTEB(eng, v2)\\nModel Class. Clust. Pair Class. Rerank. STS Retr. Summ. Mean(Task) Mean(TaskType)\\nLGAI-Embedding-Preview 89.97 59.25 88.67 49.13 66.18 86.69 38.93 74.12 68.4\\nSeed1.5-Embedding 89.88 60.83 87.39 50.67 67.45 87.23 36.44 74.76 68.56\\nQwen3-Embedding-8B 90.43 58.57 87.52 51.56 69.44 88.58 34.83 75.22 68.71\\nQwen3-Embedding-4B 89.84 57.51 87.01 50.76 68.46 88.72 34.39 74.6 68.1\\nSeed1.6-embedding 92.42 59.22 85.07 50.28 64.9 86.87 37.1 74.07 67.98\\ngemini-embedding-001 90.05 59.39 87.7 48.59 64.35 85.29 38.28 73.3 67.67\\njasper en vision language v1 90.27 60.52 88.14 50 56.05 84.37 37.19 71.41 66.65\\nLinq-Embed-Mistral 83 54.07 88.44 49.44 60.14 84.69 37.26 69.8 65.29\\nSFR-Embedding-Mistral 80.47 54.93 88.59 50.15 59.33 84.77 36.32 69.31 64.94\\nNV-Embed-v2 87.19 47.66 88.69 49.61 62.84 83.82 35.21 69.81 65\\nQZhou-Embedding(Ours) 88.97 61.65 92.43 51.77 67.12 91.65 33.05 75.97 69.52'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 15, 'page_label': '16', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='NV-Embed-v2 87.19 47.66 88.69 49.61 62.84 83.82 35.21 69.81 65\\nQZhou-Embedding(Ours) 88.97 61.65 92.43 51.77 67.12 91.65 33.05 75.97 69.52\\nTable 3: Performance on CMTEB(cmn, v1)\\nModel Class. Clust. Pair Class. Rerank. STS Retr. Mean(Task) Mean(TaskType)\\nSeed1.6-embedding 77.98 73.11 88.71 71.65 79.69 68.94 75.63 76.68\\nSeed1.5-Embedding 79.37 71.11 89.57 70.14 79.33 66.56 74.87 76.01\\nritrieve zh v1 76.88 66.5 85.98 72.86 76.97 63.92 72.71 73.85\\nConan-embedding-v2 76.47 68.84 92.44 74.41 78.31 65.48 74.24 75.99\\nxiaobu-embedding-v2 76.53 65.17 85.94 72.58 76.49 64.18 72.36 73.48\\nQwen3-Embedding-8B 76.97 80.08 84.23 66.99 78.21 63.53 73.84 75\\nConan-embedding-v1 76.77 66.33 85.68 72.76 76.67 63.67 72.5 73.65\\nzpoint large embedding zh 76.4 62.23 85.75 72.33 76.36 63.86 71.81 72.82\\npiccolo-large-zh-v2 76.42 62.16 85.22 70 74.36 63.46 70.86 71.94\\nQwen3-Embedding-4B 75.46 77.89 83.34 66.05 77.03 61.26 72.27 73.51\\nQZhou-Embedding(Ours) 79.99 70.91 95.07 74.85 78.80 71.89 76.99 78.58'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 15, 'page_label': '16', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Qwen3-Embedding-4B 75.46 77.89 83.34 66.05 77.03 61.26 72.27 73.51\\nQZhou-Embedding(Ours) 79.99 70.91 95.07 74.85 78.80 71.89 76.99 78.58\\n7 Conclusion\\nIn this technical report, we present QZhou-Embedding, a genera l-purpose contextual\\ntext embedding model with exceptional text representation capa bilities. We designed a\\nuniﬁed multi-task framework comprising specialized data transform ation and training\\nstrategies, eﬀectively enhanced the diversity of training data. To further improve the\\nquality of training data and the model’s generalization capabilities, we d eveloped a data\\nsynthesis pipeline leveraging LLM API, incorporating techniques suc h as Paraphrasing,\\nAugmentation, and Hard negative example generation. We employ a t wo-stage training\\nstrategy comprising initial retrieval-focused training followed by fu ll-task ﬁne-tuning,\\nenabling the embedding model to extend its capabilities based on robu st retrieval per-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 15, 'page_label': '16', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='strategy comprising initial retrieval-focused training followed by fu ll-task ﬁne-tuning,\\nenabling the embedding model to extend its capabilities based on robu st retrieval per-\\nformance. The model achieves state-of-the-art results on the MTEB and CMTEB\\nbenchmarks, ranking ﬁrst on both leaderboards. Our ﬁndings est ablish that data qual-\\nity and diversity are pivotal for improving embedding model capabilitie s. In the future,\\nwe will focus on developing multimodal and multilingual embedding models , as well\\nas exploring eﬀective applications of embedding models in agent syste ms, aiming to\\nintegrate cutting-edge technologies to optimize this classical modu le.\\nReferences\\n[1] Robertson, Stephen E., and Steve Walker. ”Some simple eﬀective approximations to\\nthe 2-poisson model for probabilistic weighted retrieval.” In SIGIR’9 4: Proceedings\\n16'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 16, 'page_label': '17', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nof the Seventeenth Annual International ACM-SIGIR Conferen ce on Research and\\nDevelopment in Information Retrieval, organised by Dublin City Univer sity, pp.\\n232-241. London: Springer London, 1994.\\n[2] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutano va. Bert: Pre-\\ntraining of deep bidirectional transformers for language underst anding. arXiv\\npreprint arXiv:1810.04805, 2018.\\n[3] Colin Raﬀel, Noam Shazeer, Adam Roberts, Katherine Lee, Shara n Narang, Michael\\nMatena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of tr ansfer learn-\\ning with a uniﬁed text-to-text transformer. Journal of machine le arning research,\\n21(140):1–67, 2020.\\n[4] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, D axin Jiang,\\nRangan Majumder, and Furu Wei. Text embeddings by weakly-super vised con-\\ntrastive pre-training. arXiv preprint arXiv:2212.03533, 2022.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 16, 'page_label': '17', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Rangan Majumder, and Furu Wei. Text embeddings by weakly-super vised con-\\ntrastive pre-training. arXiv preprint arXiv:2212.03533, 2022.\\n[5] Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Ried el, Piotr Bo-\\njanowski, Armand Joulin, and Edouard Grave. Unsupervised dense information\\nretrieval with contrastive learning. arXiv preprint arXiv:2112.0911 8, 2021.\\n[6] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence em beddings using\\nsiamese bert-networks. arXiv preprint arXiv:1908.10084, 2019.\\n[7] Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple contrastive\\nlearning of sentence embeddings. In Proceedings of the 2021 Conf erence on Empir-\\nical Methods in Natural Language Processing, pages 6894–6910, Online and Punta\\nCana, Dominican Republic. Association for Computational Linguistics .\\n[8] Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hern´ andez ´Abrego, Ji Ma,\\nVincent Y Zhao, Yi Luan, Keith B Hall, Ming-Wei Chang, et al. Large du al encoders'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 16, 'page_label': '17', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='[8] Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hern´ andez ´Abrego, Ji Ma,\\nVincent Y Zhao, Yi Luan, Keith B Hall, Ming-Wei Chang, et al. Large du al encoders\\nare generalizable retrievers. arXiv preprint arXiv:2112.07899, 202 1.\\n[9] Brown, Tom, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D . Kaplan, Pra-\\nfulla Dhariwal, Arvind Neelakantan et al. ”Language models are few-s hot learners.”\\nAdvances in neural information processing systems 33 (2020): 18 77-1901.\\n[10] Ma, Xueguang, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin. ”F ine-tuning\\nllama for multi-stage text retrieval.” In Proceedings of the 47th Int ernational ACM\\nSIGIR Conference on Research and Development in Information Re trieval, pp. 2421-\\n2425. 2024.\\n[11] Springer, Jacob Mitchell, Suhas Kotha, Daniel Fried, Graham Ne ubig, and Aditi\\nRaghunathan. ”Repetition improves language model embeddings.” a rXiv preprint\\narXiv:2402.15449 (2024).\\n[12] BehnamGhader, Parishad, Vaibhav Adlakha, Marius Mosbach, D zmitry Bah-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 16, 'page_label': '17', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Raghunathan. ”Repetition improves language model embeddings.” a rXiv preprint\\narXiv:2402.15449 (2024).\\n[12] BehnamGhader, Parishad, Vaibhav Adlakha, Marius Mosbach, D zmitry Bah-\\ndanau, Nicolas Chapados, and Siva Reddy. ”Llm2vec: Large languag e models are\\nsecretly powerful text encoders.” arXiv preprint arXiv:2404.0596 1 (2024).\\n[13] https://cloud.tencent.com/developer/news/2461911\\n17'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 17, 'page_label': '18', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\n[14] Zhang, Dun, Jiacheng Li, Ziyang Zeng, and Fulong Wang. ”Jaspe r and stella:\\ndistillation of sota embedding models.” arXiv preprint arXiv:2412.19048 (2024).\\n[15] Chen, Jianlv, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng\\nLiu. ”Bge m3-embedding: Multi-lingual, multi-functionality, multi-gran ularity text\\nembeddings through self-knowledge distillation.” arXiv preprint arXiv :2402.03216\\n(2024).\\n[16] Ji, Yifan, Zhipeng Xu, Zhenghao Liu, Yukun Yan, Shi Yu, Yishan L i, Zhiyuan\\nLiu, Yu Gu, Ge Yu, and Maosong Sun. ”Learning more eﬀective repre senta-\\ntions for dense retrieval through deliberate thinking before sear ch.” arXiv preprint\\narXiv:2502.12974 (2025).\\n[17] Choi J, Kim H, Jang H, et al. LG-ANNA-Embedding technical repo rt[J]. arXiv\\npreprint arXiv:2506.07438, 2025.\\n[18] Xiong, Lee, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Pau l Bennett,\\nJunaid Ahmed, and Arnold Overwijk. ”Approximate nearest neighbo r negative con-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 17, 'page_label': '18', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='preprint arXiv:2506.07438, 2025.\\n[18] Xiong, Lee, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Pau l Bennett,\\nJunaid Ahmed, and Arnold Overwijk. ”Approximate nearest neighbo r negative con-\\ntrastive learning for dense text retrieval.” arXiv preprint arXiv:200 7.00808 (2020).\\n[19] Lee, Chankyu, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad\\nShoeybi, Bryan Catanzaro, and Wei Ping. ”Nv-embed: Improved t echniques for\\ntraining llms as generalist embedding models.” arXiv preprint arXiv:2405 .17428\\n(2024).\\n[20] Moreira, Gabriel de Souza P., Radek Osmulski, Mengyao Xu, Rona y Ak, Benedikt\\nSchiﬀerer, and Even Oldridge. ”NV-Retriever: Improving text emb edding models\\nwith eﬀective hard-negative mining.” arXiv preprint arXiv:2407.15831 (2024).\\n[21] Team, Qwen. ”Qwen2 technical report.” arXiv preprint arXiv:24 07.10671 (2024).\\n[22] Xiao, Shitao, Zheng Liu, Peitian Zhang, Niklas Muennighoﬀ, Defu L ian, and Jian-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 17, 'page_label': '18', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='[21] Team, Qwen. ”Qwen2 technical report.” arXiv preprint arXiv:24 07.10671 (2024).\\n[22] Xiao, Shitao, Zheng Liu, Peitian Zhang, Niklas Muennighoﬀ, Defu L ian, and Jian-\\nYun Nie. ”C-pack: Packed resources for general chinese embedd ings.” In Proceedings\\nof the 47th international ACM SIGIR conference on research and development in\\ninformation retrieval, pp. 641-649. 2024. Team, Qwen.\\n[23] Muennighoﬀ, Niklas, Nouamane Tazi, Lo¨ ıc Magne, and Nils Reimers . ”Mteb: Mas-\\nsive text embedding benchmark.” arXiv preprint arXiv:2210.07316 (2 022).\\n[24] Li, Shiyu, Yang Tang, Shizhe Chen, and Xi Chen. ”Conan-embed ding: Gen-\\neral text embedding with more and better negative samples.” arXiv p reprint\\narXiv:2408.15710 (2024).\\n[25] Aizawa, Akiko. ”An information-theoretic perspective of tf–id f measures.” Infor-\\nmation Processing & Management 39, no. 1 (2003): 45-65.\\n[26] Robertson, Stephen E., and Steve Walker. ”Some simple eﬀectiv e approximations'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 17, 'page_label': '18', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='mation Processing & Management 39, no. 1 (2003): 45-65.\\n[26] Robertson, Stephen E., and Steve Walker. ”Some simple eﬀectiv e approximations\\nto the 2-poisson model for probabilistic weighted retrieval.” In SIGI R’94: Proceed-\\nings of the Seventeenth Annual International ACM-SIGIR Confe rence on Research\\nand Development in Information Retrieval, organised by Dublin City Un iversity,\\npp. 232-241. London: Springer London, 1994.\\n18'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 18, 'page_label': '19', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\n[27] Deerwester, Scott, Susan T. Dumais, George W. Furnas, Tho mas K. Landauer, and\\nRichard Harshman. ”Indexing by latent semantic analysis.” Journal of the American\\nsociety for information science 41, no. 6 (1990): 391-407.\\n[28] Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Maj umder, and\\nFuru Wei. Improving text embeddings with large language models. arX iv preprint\\narXiv:2401.00368, 2023b.\\n[29] Meng, Rui, Ye Liu, Shaﬁq Rayhan Joty, Caiming Xiong, Yingbo Zhou , and Semih\\nYavuz. ”Sfrembedding-mistral: enhance text retrieval with tran sfer learning.” Sales-\\nforce AI Research Blog 3 (2024): 6.\\n[30] Meng R, Liu Y, Joty S R, et al. Sfr-embedding-2: Advanced text embedding with\\nmulti-stage training, 2024[J].\\n[31] Muennighoﬀ, Niklas, S. U. Hongjin, Liang Wang, Nan Yang, Furu W ei, Tao Yu,\\nAmanpreet Singh, and Douwe Kiela. ”Generative representational instruction tun-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 18, 'page_label': '19', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='multi-stage training, 2024[J].\\n[31] Muennighoﬀ, Niklas, S. U. Hongjin, Liang Wang, Nan Yang, Furu W ei, Tao Yu,\\nAmanpreet Singh, and Douwe Kiela. ”Generative representational instruction tun-\\ning.” In The Thirteenth International Conference on Learning Rep resentations.\\n2024.\\n[32] Chaofan Li, MingHao Qin, Shitao Xiao, Jianlyu Chen, Kun Luo, Yingx ia Shao,\\nDefu Lian, and Zheng Liu. Making text embedders few-shot learner s. arXiv preprint\\narXiv:2409.15700, 2024.\\n[33] Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie , and Meis-\\nhan Zhang. Towards general text embeddings with multi-stage con trastive learning,\\n2023. URL https://arxiv.org/abs/2308.03281.\\n[34] Zhang, Yanzhao, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, B aosong Yang,\\nPengjun Xie et al. ”Qwen3 Embedding: Advancing Text Embedding and Reranking\\nThrough Foundation Models.” arXiv preprint arXiv:2506.05176 (2025 ).\\n[35] Su, Jianlin, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, an d Yunfeng Liu.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 18, 'page_label': '19', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Through Foundation Models.” arXiv preprint arXiv:2506.05176 (2025 ).\\n[35] Su, Jianlin, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, an d Yunfeng Liu.\\n”Roformer: Enhanced transformer with rotary position embeddin g.” Neurocomput-\\ning 568 (2024): 127063.\\n[36] Zhang, Biao, and Rico Sennrich. ”Root mean square layer norma lization.” Ad-\\nvances in neural information processing systems 32 (2019).\\n[37] Shazeer, Noam. ”Glu variants improve transformer.” arXiv pre print\\narXiv:2002.05202 (2020).\\n[38] https://seed1-6-embedding.github.io/\\n[39] Huang, Junqin, Zhongjie Hu, Zihao Jing, Mengya Gao, and Yichao Wu. ”Pic-\\ncolo2: General text embedding with multi-task hybrid loss training.” a rXiv preprint\\narXiv:2405.06932 (2024).\\n[40] Sun, Yifan, Changmao Cheng, Yuhan Zhang, Chi Zhang, Liang Z heng, Zhongdao\\nWang, and Yichen Wei. ”Circle loss: A uniﬁed perspective of pair similarit y op-\\ntimization.” In Proceedings of the IEEE/CVF conference on comput er vision and'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 18, 'page_label': '19', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Wang, and Yichen Wei. ”Circle loss: A uniﬁed perspective of pair similarit y op-\\ntimization.” In Proceedings of the IEEE/CVF conference on comput er vision and\\npattern recognition, pp. 6398-6407. 2020.\\n19'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 19, 'page_label': '20', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\n[41] Rodrigo Nogueira, Wei Yang, Jimmy Lin, and Kyunghyun Cho. 201 9. Document\\nexpansion by query prediction. ArXiv preprint, abs/1904.08375.\\n[42] Liang Wang, Nan Yang, and Furu Wei. 2023. Query2doc: Query e xpansion with\\nlarge language models. In Proceedings of the 2023 Conference on E mpirical Meth-\\nods in Natural Language Processing, pages 9414–9423, Singapor e. Association for\\nComputational Linguistics.\\n[43] Zhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, An ton Bakalov,\\nKelvin Guu, Keith Hall, and Ming-Wei Chang. 2022. Promptagator: Fe wshot dense\\nretrieval from 8 examples. In The Eleventh International Confer ence on Learning\\nRepresentations.\\n[44] Kexin Wang, Nandan Thakur, Nils Reimers, and Iryna Gurevych. 2022a. GPL:\\nGenerative pseudo labeling for unsupervised domain adaptation of d ense retrieval.\\nIn Proceedings of the 2022 Conference of the North American Cha pter of the'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 19, 'page_label': '20', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Generative pseudo labeling for unsupervised domain adaptation of d ense retrieval.\\nIn Proceedings of the 2022 Conference of the North American Cha pter of the\\nAssociation for Computational Linguistics: Human Language Techn ologies, pages\\n2345–2360, Seattle, United States. Association for Computation al Linguistics.\\n[45] Honovich, Or, Thomas Scialom, Omer Levy, and Timo Schick. ”Unn atural in-\\nstructions: Tuning language models with (almost) no human labor.” ar Xiv preprint\\narXiv:2212.09689 (2022).\\n[46] Xiong, Lee, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Pau l Bennett,\\nJunaid Ahmed, and Arnold Overwijk. ”Approximate nearest neighbo r negative con-\\ntrastive learning for dense text retrieval.” arXiv preprint arXiv:200 7.00808 (2020).\\n[47] Moreira, Gabriel de Souza P., Radek Osmulski, Mengyao Xu, Rona y Ak, Benedikt\\nSchiﬀerer, and Even Oldridge. ”NV-Retriever: Improving text emb edding models\\nwith eﬀective hard-negative mining.” arXiv preprint arXiv:2407.15831 (2024).'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 19, 'page_label': '20', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Schiﬀerer, and Even Oldridge. ”NV-Retriever: Improving text emb edding models\\nwith eﬀective hard-negative mining.” arXiv preprint arXiv:2407.15831 (2024).\\n[48] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representatio n learning with\\ncontrastive predictive coding. arXiv preprint arXiv:1807.03748, 20 18.\\n[49] https://www.kexue.fm/archives/8847\\n[50] Xin Zhang, Yanzhao Zhang, Dingkun Long, Wen Xie, Ziqi Dai, Jialon g Tang, Huan\\nLin, Baosong Yang, Pengjun Xie, Fei Huang, Meishan Zhang, Wenjie Li, and Min\\nZhang. mgte: Generalized long-context text representation and reranking models\\nfor multilingual text retrieval, 2024.\\n[51] Lee, Jinhyuk, Zhuyun Dai, Xiaoqi Ren, Blair Chen, Daniel Cer, Je remy R. Cole,\\nKai Hui et al. ”Gecko: Versatile text embeddings distilled from large la nguage\\nmodels, 2024.” URL https://arxiv. org/abs/2403.20327.\\n[52] Junseong Kim, Seolhwa Lee, Jihoon Kwon, Sangmo Gu, Yejin Kim, M inkyung'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 19, 'page_label': '20', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='models, 2024.” URL https://arxiv. org/abs/2403.20327.\\n[52] Junseong Kim, Seolhwa Lee, Jihoon Kwon, Sangmo Gu, Yejin Kim, M inkyung\\nCho, Jy yong Sohn, and Chanyeol Choi. Linq-embed-mistral: Elevat ing text re-\\ntrieval with improved gpt data through task-speciﬁc control and quality reﬁnement.\\nlinq ai research blog, 2024.\\n[53] https://huggingface.co/dunzhang/stella-large-zh-v3-1792d\\n20'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 20, 'page_label': '21', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\n[54] Tsatsaronis G, Balikas G, Malakasiotis P, et al. An overview of the BIOASQ large-\\nscale biomedical semantic indexing and question answering competitio n[J]. BMC\\nbioinformatics, 2015, 16(1): 138.\\n[55] Cui Y, Liu T, Che W, et al. A span-extraction dataset for Chines e machine reading\\ncomprehension[J]. arXiv preprint arXiv:1810.07366, 2018.\\n[56] Wang A, Singh A, Michael J, et al. GLUE: A multi-task benchmark a nd analysis\\nplatform for natural language understanding[J]. arXiv preprint ar Xiv:1804.07461,\\n2018.\\n[57] Yelp Dataset. Yelp Inc., [Year]. Available: https://www.yelp.com/dataset\\n[58] Maas A, Daly R E, Pham P T, et al. Learning word vectors for sent iment analy-\\nsis[C]//Proceedings of the 49th annual meeting of the association f or computational\\nlinguistics: Human language technologies. 2011: 142-150.\\n[59] Jack FitzGerald, Christopher Hench, Charith Peris, Scott Mac kie, Kay Rottmann,'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 20, 'page_label': '21', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='linguistics: Human language technologies. 2011: 142-150.\\n[59] Jack FitzGerald, Christopher Hench, Charith Peris, Scott Mac kie, Kay Rottmann,\\nAna Sanchez, Aaron Nash, Liam Urbach, Vishesh Kakarala, Richa Sin gh, Swetha\\nRanganath, Laurie Crist, Misha Britan, Wouter Leeuwis, Gokhan Tu r, and Prem\\nNatarajan. 2022. Massive: A 1m-example multilingual natural langu age understand-\\ning dataset with 51 typologically-diverse languages.\\n[60] Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre . 2012. Semeval-\\n2012 task 6: A pilot on semantic textual similarity. In * SEM 2012: The First\\nJoint Conference on Lexical and Computational Semantics–Volume 1: Proceedings\\nof the main conference and the shared task, and Volume 2: Procee dings of the Sixth\\nInternational Workshop on Semantic Evaluation (SemEval 2012), pages 385–393.\\n[61] Liu, Xin, Qingcai Chen, Chong Deng, Huajun Zeng, Jing Chen, Do ngfang Li,\\nand Buzhou Tang. ”Lcqmc: A large-scale chinese question matching corpus.” In'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 20, 'page_label': '21', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='[61] Liu, Xin, Qingcai Chen, Chong Deng, Huajun Zeng, Jing Chen, Do ngfang Li,\\nand Buzhou Tang. ”Lcqmc: A large-scale chinese question matching corpus.” In\\nProceedings of the 27th international conference on computatio nal linguistics, pp.\\n1952-1962. 2018.\\n[62] Yang, Yinfei, Yuan Zhang, Chris Tar, and Jason Baldridge. ”PAW S-X: A\\ncross-lingual adversarial dataset for paraphrase identiﬁcation .” arXiv preprint\\narXiv:1908.11828 (2019).\\n[63] Cer, Daniel, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and L ucia Specia.\\n”Semeval-2017 task 1: Semantic textual similarity-multilingual and c ross-lingual\\nfocused evaluation.” arXiv preprint arXiv:1708.00055 (2017).\\n[64] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh T iwary, Rangan\\nMajumder, and Li Deng. 2016. MS MARCO: A human generated mach ine read-\\ning comprehension dataset. In Proceedings of the Workshop on Co gnitive Com-\\nputation: Integrating neural and symbolic approaches 2016 co-lo cated with the'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 20, 'page_label': '21', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='ing comprehension dataset. In Proceedings of the Workshop on Co gnitive Com-\\nputation: Integrating neural and symbolic approaches 2016 co-lo cated with the\\n30th Annual Conference on Neural Information Processing Syst ems (NIPS 2016),\\nBarcelona, Spain, December 9, 2016, volume 1773 of CEUR Worksho p Proceedings.\\nCEUR-WS.org.\\n21'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 21, 'page_label': '22', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\n[65] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redﬁeld, Michael Collins , Ankur\\nParikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Ke nton Lee,\\net al. Natural questions: a benchmark for question answering res earch. Transactions\\nof the Association for Computational Linguistics, 7:453–466, 2019 .\\n[66] Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jaso n Weston, and\\nMichael Auli. 2019. ELI5: Long Form Question Answering. In Procee dings of\\nthe 57th Annual Meeting of the Association for Computational Ling uistics, pages\\n3558–3567, Florence, Italy. Association for Computational Lingu istics.\\n[67] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan\\nSalakhutdinov, and Christopher D. Manning. HotpotQA: A dataset for diverse,\\nexplainable multi-hop question answering. In Proceedings of the 201 8 Conference\\non Empirical Methods in Natural Language Processing, pp. 2369–2 380, Brussels,'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 21, 'page_label': '22', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='explainable multi-hop question answering. In Proceedings of the 201 8 Conference\\non Empirical Methods in Natural Language Processing, pp. 2369–2 380, Brussels,\\nBelgium, October-November 2018. Association for Computational Linguistics. doi:\\n10.18653/v1/D18-1259. URL https://aclanthology.org/D18-125 9.\\n[68] Xinyu Zhang, Nandan Thakur, Odunayo Ogundepo, Ehsan Kama lloo, David\\nAlfonso-Hermelo, Xiaoguang Li, Qun Liu, Mehdi Rezagholizadeh, and Jimmy Lin.\\nMiracl: A multilingual retrieval dataset covering 18 diverse language s. Transactions\\nof the Association for Computational Linguistics, 11:1114–1131, 2 023.\\n[69] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Per cy Liang.\\nSquad: 100,000+ questions for machine comprehension of text. ar Xiv preprint\\narXiv:1606.05250, 2016.\\n[70] James Thorne, Andreas Vlachos, Christos Christodoulopoulos , and Arpit Mit-\\ntal. Fever: a large-scale dataset for fact extraction and veriﬁca tion. arXiv preprint\\narXiv:1803.05355, 2018.'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 21, 'page_label': '22', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='[70] James Thorne, Andreas Vlachos, Christos Christodoulopoulos , and Arpit Mit-\\ntal. Fever: a large-scale dataset for fact extraction and veriﬁca tion. arXiv preprint\\narXiv:1803.05355, 2018.\\n[71] Wei He, Kai Liu, Jing Liu, Yajuan Lyu, Shiqi Zhao, Xinyan Xiao, Yu an Liu,\\nYizhong Wang, Hua Wu, Qiaoqiao She, Xuan Liu, Tian Wu, and Haifeng Wa ng.\\n2018. DuReader: a Chinese Machine Reading Comprehension Datase t from Real-\\nworld Applications. In Proceedings of the Workshop on Machine Read ing for Ques-\\ntion Answering, pages 37–46, Melbourne, Australia. Association fo r Computational\\nLinguistics.\\n[72] Yichen Jiang, Shikha Bordia, Zheng Zhong, Charles Dognin, Mane esh Singh, and\\nMohit Bansal. 2020. HoVer: A Dataset for Many-Hop Fact Extract ion And Claim\\nVeriﬁcation. In Findings of the Association for Computational Lingu istics: EMNLP\\n2020, pages 3441–3460, Online. Association for Computational Lin guistics.\\n[73] Zhang X, Ma X, Shi P, et al. Mr. TyDi: A multi-lingual benchmark fo r dense'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 21, 'page_label': '22', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='2020, pages 3441–3460, Online. Association for Computational Lin guistics.\\n[73] Zhang X, Ma X, Shi P, et al. Mr. TyDi: A multi-lingual benchmark fo r dense\\nretrieval[J]. arXiv preprint arXiv:2108.08787, 2021.\\n[74] Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Danie l Weld. 2020.\\nS2ORC: The Semantic Scholar Open Research Corpus. In Proceedin gs of the 58th\\nAnnual Meeting of the Association for Computational Linguistics, p ages 4969–4983,\\nOnline. Association for Computational Linguistics.\\n22'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 22, 'page_label': '23', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\n[75] https://huggingface.co/spaces/mteb/leaderboard\\n[76] Jinhyuk Lee, Feiyang Chen, Sahil Dua, Daniel Cer, Madhuri Sha nbhogue, Iftekhar\\nNaim, Gustavo Hernandez /acute.ts1Abrego, Zhe Li, Kaifeng Chen, Henrique Schechter\\nVera, et al. Gemini embedding: Generalizable embeddings from gemini. arXiv\\npreprint arXiv:2503.07891, 2025b.\\nA Appendix\\nA.1 Framework Constraints\\nTable 4: Speciﬁcations of framework constraints\\nItem Explanation\\nKeep core semantics Preserving the core semantic content, which is the\\nmost critical requirement.\\nDiversity in morphology,\\nsyntax, grammar, tense,\\nrhetoric, etc\\nVariations in lexical composition, syntactic struc-\\nture, grammatical rules, and tense usage are per-\\nmitted.\\nLength within ±15% The length deviation from the original sentence\\nshould not exceed 15%.\\nKeep language The language used must be consistent with the\\noriginal sentence.\\nClose in ﬁeld The content must remain strictly aligned with the'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 22, 'page_label': '23', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='should not exceed 15%.\\nKeep language The language used must be consistent with the\\noriginal sentence.\\nClose in ﬁeld The content must remain strictly aligned with the\\ndomain of the given sentence.\\nTopic transfer, expansion,\\nextension, prohibiting pure\\nrewriting\\nTopic shifting, extension, or elaboration is permit-\\nted, but purely paraphrased content (identical to\\nthe original topic) is prohibited.\\nPOS is the perfect\\nanswer(necessary &\\nsuﬃcient)\\nPositive examples must be unambiguous and pre-\\ncisely address the query (necessity condition) while\\ncontaining exclusively relevant content without ex-\\ntraneous information (suﬃciency condition).\\nHard NEG: Worse than\\nPOS:\\n- Semantic deviation\\n(inadequate)\\n- Including irrelevant\\ninformation(unnecessary)\\n- Diﬀerent aspects of the\\nsame topic\\nHard negative examples must exhibit inferior qual-\\nity compared to positive instances, with noise in-\\ntroduced through three strategies: 1) semantic de-\\nviation (failing to accurately address the query),'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 22, 'page_label': '23', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='ity compared to positive instances, with noise in-\\ntroduced through three strategies: 1) semantic de-\\nviation (failing to accurately address the query),\\n2) incorporation of irrelevant information, or 3)\\nmaintaining the same topic but diverging in as-\\npects.\\nImitation: syntax, sentence\\nstructure, structural\\nGenerating hard negative examples by emulating\\nthe structural and syntactic patterns of the given\\npositive instance is a critical step to maximize dis-\\ncriminative challenge for the model.\\n23'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 23, 'page_label': '24', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nA.2 Instruction Examples\\nTable 5: Instruction for partial training data\\nDataset Instruction\\nHuatuo Given a medical question, retrieve user replies that\\nbest answer the question\\nReddit Retrieve the paragraph most semantically similar\\nto the given statement\\nLaw-GPT Retrieve relevant legal provisions or interpreta-\\ntions for the given case\\nMNLI/SNLI Retrieve semantically similar text\\nYelp Classify the customer review of businesses\\nWeibo Classify the sentiment of Weibo comments\\nA.3 Data Synthesis Examples\\nNote: The text highlighted in yellow represents the original sentence, fo llowed by the\\nsynthetically generated sentence.\\nTable 6: Paraphrasing Example (1)\\nquery pos\\nWhat is the best credit\\ncard for someone with no\\ncredit history?\\nIf you’ve never had a credit card before a likely\\nreason can be due to lack of credit history. You\\ncan apply for a department store card.\\nWhat’s the ideal credit\\ncard for a person without\\nany credit history?'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 23, 'page_label': '24', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='reason can be due to lack of credit history. You\\ncan apply for a department store card.\\nWhat’s the ideal credit\\ncard for a person without\\nany credit history?\\nIf you’ve never had a credit card, it’s probably\\nbecause you don’t have a credit history. A depart-\\nment store card could be a good option to apply\\nfor.\\nWhat’s the top credit card\\nchoice for someone who has\\nno credit history?\\nIf you’ve never owned a credit card, it’s probably\\nbecause you don’t have a credit history. A depart-\\nment store card might be a good option to con-\\nsider.\\n24'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 24, 'page_label': '25', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nTable 7: Paraphrasing Example (2)\\nquery pos\\nWhich English Poet\\nLaureate wrote ’The Faerie\\nQueene’?\\nEnglish Renaissance to begin, shakily, in the 1520s,\\nand it continued until perhaps 1620. England had\\na strong tradition of literature in the English ver-\\nnacular, which gradually increased as English use\\nof the printing press became common during the\\nmid 16th century. By the time of Elizabethan liter-\\nature a vigorous literary culture in both drama and\\npoetry included poets such as Edmund Spenser,\\nwhose verse epic ’The Faerie Queene’ had a strong\\ninﬂuence on English literature but was eventu-\\nally overshadowed by the lyrics of William Shake-\\nspeare, Thomas Wyatt and others. Typically, the\\nworks of these playwrights and poets circulated in\\nmanuscript form.\\nWho was the English Poet\\nLaureate that penned ’The\\nFaerie Queene’?\\nThe English Renaissance began, albeit slowly, in\\nthe 1520s and likely lasted until around 1620. Eng-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 24, 'page_label': '25', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='manuscript form.\\nWho was the English Poet\\nLaureate that penned ’The\\nFaerie Queene’?\\nThe English Renaissance began, albeit slowly, in\\nthe 1520s and likely lasted until around 1620. Eng-\\nland boasted a robust literary tradition in the En-\\nglish language, which expanded as the printing\\npress became more widely adopted during the mid-\\n16th century. By the time of Elizabethan litera-\\nture, a dynamic literary scene in both drama and\\npoetry emerged, featuring writers like Edmund\\nSpenser, whose epic poem The Faerie Queene sig-\\nniﬁcantly inﬂuenced English literature, although\\nit was eventually overshadowed by the works of\\nWilliam Shakespeare, Thomas Wyatt, and others.\\nGenerally, the writings of these poets and play-\\nwrights circulated in manuscript form.\\nWhich English Poet\\nLaureate authored ’The\\nFaerie Queene’?\\nThe English Renaissance began somewhat uncer-\\ntainly in the 1520s, continuing through to around\\n1620. England had a rich literary heritage in its'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 24, 'page_label': '25', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='Laureate authored ’The\\nFaerie Queene’?\\nThe English Renaissance began somewhat uncer-\\ntainly in the 1520s, continuing through to around\\n1620. England had a rich literary heritage in its\\nnative tongue, which expanded as the printing\\npress became more widely used in the mid-16th\\ncentury. By the Elizabethan era, a thriving lit-\\nerary culture in both drama and poetry emerged,\\nwith poets like Edmund Spenser, whose epic The\\nFaerie Queene profoundly impacted English liter-\\nature, though it was eventually eclipsed by the\\nworks of William Shakespeare, Thomas Wyatt,\\nand others. Generally, the writings of these play-\\nwrights and poets were circulated in manuscript\\nform.\\n25'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 25, 'page_label': '26', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nTable 8: Augmentation Example\\nquery pos neg\\nWhat is the best\\ncredit card for\\nsomeone with no\\ncredit history?\\nIf you’ve never had a credit\\ncard before a likely reason\\ncan be due to lack of credit\\nhistory. You can apply for a\\ndepartment store card.\\n-\\nWhich credit card is\\neasiest to get\\napproved for with bad\\ncredit?\\nFor those with poor credit\\nscores, secured credit cards\\nfrom Capital One or Dis-\\ncover are often the most ac-\\ncessible options since they\\nrequire a refundable de-\\nposit.\\nSome premium travel cards\\nlike Chase Sapphire have\\ngreat rewards but usually\\nneed excellent credit. Store\\ncards may accept lower\\nscores but have high inter-\\nest rates.\\nWhat credit cards\\ncan I get as a college\\nstudent with no\\ncredit?\\nStudents without credit his-\\ntory often qualify for starter\\ncards like Discover it Stu-\\ndent or Capital One Jour-\\nney, which are designed to\\nhelp build credit.\\nPremium cards like Amer-\\nican Express Platinum re-'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 25, 'page_label': '26', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='tory often qualify for starter\\ncards like Discover it Stu-\\ndent or Capital One Jour-\\nney, which are designed to\\nhelp build credit.\\nPremium cards like Amer-\\nican Express Platinum re-\\nquire good credit history,\\nthough some banks oﬀer\\nstudent accounts with debit\\ncards.\\nWhich English Poet\\nLaureate wrote ’The\\nFaerie Queene’?\\n...By the time of Eliz-\\nabethan literature a vig-\\norous literary culture in\\nboth drama and poetry in-\\ncluded poets such as Ed-\\nmund Spenser, whose verse\\nepic ’The Faerie Queene’\\nhad a strong inﬂuence on\\nEnglish literature but was\\neventually overshadowed by\\nthe lyrics of William ...\\n-\\nWhat major epic\\npoem did Edmund\\nSpenser write during\\nQueen Elizabeth’s\\nreign?\\nEdmund Spenser composed\\n’The Faerie Queene’, an\\nallegorical epic poem that\\nbecame one of the most\\nsigniﬁcant works of Eliz-\\nabethan literature though\\nlater eclipsed by Shake-\\nspeare’s popularity.\\nChristopher Marlowe’s\\n’Hero and Leander’ was an-\\nother notable Elizabethan\\npoem, but unlike Spenser’s'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 25, 'page_label': '26', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='signiﬁcant works of Eliz-\\nabethan literature though\\nlater eclipsed by Shake-\\nspeare’s popularity.\\nChristopher Marlowe’s\\n’Hero and Leander’ was an-\\nother notable Elizabethan\\npoem, but unlike Spenser’s\\nwork it wasn’t an epic\\nallegory.\\nWhich poet created\\n’Paradise Lost’ during\\nthe English\\nRenaissance?\\nJohn Milton authored the\\nepic poem ’Paradise Lost’\\nin the 17th century, a mon-\\numental work that explored\\nbiblical themes through\\nblank verse and became\\na cornerstone of English\\nliterature.\\nWilliam Blake’s ’The Mar-\\nriage of Heaven and Hell’\\nalso dealt with religious\\nthemes, though it was more\\nprophetic than epic in style\\ncompared to Milton’s mas-\\nterpiece.\\n26'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 26, 'page_label': '27', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='QZhou-Embedding Technical Report\\n Kingsoft AI\\nTable 9: Hard-Negative Generation Example\\nquery pos neg\\nWhat territory was\\nKing Hussein afraid\\nIsrael would obtain?\\n...Hussein was nonetheless\\nwary that an Egyptian-\\nIsraeli war would risk the\\nWest Bank’s occupation by\\nIsrael...\\n-\\nWhat territory was\\nKing Hussein afraid\\nIsrael would obtain?\\n...Hussein was nonetheless\\nwary that an Egyptian-\\nIsraeli war would risk the\\nWest Bank’s occupation by\\nIsrael...\\nKing Hussein expressed\\nconcerns about potential\\nIsraeli expansion during\\nthe Arab-Israeli conﬂicts,\\nthough his warnings to\\nNasser were delayed and\\ninitially dismissed, while\\nother Arab leaders focused\\nmore on direct military\\npreparations against Israel.\\nWhat territory was\\nKing Hussein afraid\\nIsrael would obtain?\\n...Hussein was nonetheless\\nwary that an Egyptian-\\nIsraeli war would risk the\\nWest Bank’s occupation by\\nIsrael...\\nKing Hussein expressed\\nconcerns about potential\\nIsraeli territorial expansion\\nduring the 1967 tensions,'),\n",
       " Document(metadata={'producer': 'pikepdf 8.15.1', 'creator': 'arXiv GenPDF (tex2pdf:)', 'creationdate': '2025-09-01T00:50:53+00:00', 'author': 'Peng Yu; En Xu; Bin Chen; Haibiao Chen; Yinfei Xu', 'doi': 'https://doi.org/10.48550/arXiv.2508.21632', 'keywords': '', 'license': 'http://creativecommons.org/licenses/by/4.0/', 'moddate': '2025-09-01T00:50:53+00:00', 'title': 'QZhou-Embedding Technical Report', 'arxivid': 'https://arxiv.org/abs/2508.21632v1', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 27, 'page': 26, 'page_label': '27', 'source_file': 'embeddings.pdf', 'file_type': 'pdf'}, page_content='wary that an Egyptian-\\nIsraeli war would risk the\\nWest Bank’s occupation by\\nIsrael...\\nKing Hussein expressed\\nconcerns about potential\\nIsraeli territorial expansion\\nduring the 1967 tensions,\\nthough his warnings were\\ndelayed in reaching Nasser\\nand mixed with broader\\nregional tensions, while\\nEgyptian military move-\\nments in Sinai were already\\nunderway under Amer’s\\norders.\\n27'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Title: A Survey of Object Detection: From Region Proposals to End-to-End \\nTransformers \\nAuthor: [Your Name/Institution] Date: October 2025 \\n \\nAbstract \\nObject detection, a fundamental task in computer vision, involves identifying and localizing \\ninstances of objects within an image or video. It goes beyond simple image classification by not only \\ndetermining the class of an object but also providing a bounding box that precisely outlines its \\nlocation. This paper provides a comprehensive survey of the evolution of object detection \\nmethodologies, primarily focusing on the deep learning era. We begin by contextualizing the \\nproblem with a brief overview of traditional computer vision techniques. The core of the review is \\ndedicated to the two dominant paradigms in deep learning-based detectors: two-stage detectors, \\nexemplified by the R-CNN family (R-CNN, Fast R-CNN, Faster R-CNN), which prioritize accuracy'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'page': 0, 'page_label': '1', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='dedicated to the two dominant paradigms in deep learning-based detectors: two-stage detectors, \\nexemplified by the R-CNN family (R-CNN, Fast R-CNN, Faster R-CNN), which prioritize accuracy \\nthrough a region proposal mechanism; and single-stage detectors, such as YOLO and SSD, which \\noptimize for speed by performing detection in a single pass. We then explore key architectural \\ncomponents like backbone networks, anchor boxes, and non-maximum suppression. The survey \\nculminates with a discussion of modern architectures, including the paradigm-shifting DETR \\n(DEtection TRansformer), which reframes object detection as an end-to-end set prediction problem. \\nFinally, we cover standard evaluation metrics, common datasets, real-world applications, and the \\nongoing challenges and future directions that are shaping the field. \\nKeywords: Object Detection, Computer Vision, Deep Learning, R-CNN, YOLO, SSD, Transformer, \\nDETR, Bounding Box, mAP.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Table of Contents \\n1. Introduction 1.1. Defining Object Detection: Classification and Localization 1.2. Distinction \\nfrom Other Vision Tasks 1.3. The Importance of Object Detection 1.4. Paper Structure \\n2. Background and Foundational Concepts 2.1. Traditional Computer Vision Approaches \\n(Viola-Jones, HOG) 2.2. The Sliding Window Method 2.3. The Deep Learning Revolution \\n3. Two-Stage Object Detectors: A Focus on Accuracy 3.1. The \"Propose, then Classify\" \\nParadigm 3.2. R-CNN: Regions with CNN Features 3.3. Fast R-CNN: Sharing Computation \\n3.4. Faster R-CNN: The Region Proposal Network (RPN) \\n4. Single-Stage Object Detectors: A Focus on Speed 4.1. The \"Single Pass\" Paradigm 4.2. \\nYOLO: You Only Look Once 4.3. SSD: Single Shot MultiBox Detector \\n5. Key Architectural Components and Innovations 5.1. Backbone Networks: The Feature \\nExtractors 5.2. Anchor Boxes: The Priors for Prediction 5.3. Non-Maximum Suppression \\n(NMS): Pruning Redundant Detections'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'page': 1, 'page_label': '2', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Extractors 5.2. Anchor Boxes: The Priors for Prediction 5.3. Non-Maximum Suppression \\n(NMS): Pruning Redundant Detections \\n6. Modern Architectures and the Rise of Transformers 6.1. Balancing Speed and Accuracy: \\nEfficientDet 6.2. DETR: End-to-End Object Detection with Transformers \\n7. Evaluation Metrics and Datasets 7.1. Intersection over Union (IoU) 7.2. Average Precision \\n(AP) and mean Average Precision (mAP) 7.3. Landmark Datasets (PASCAL VOC, COCO) \\n8. Applications and Real-World Use Cases 8.1. Autonomous Vehicles 8.2. Medical Imaging \\n8.3. Retail and Inventory Management 8.4. Security and Surveillance \\n9. Challenges and Future Directions 9.1. Detecting Small and Occluded Objects 9.2. The \\nSpeed vs. Accuracy Trade-off 9.3. Domain Adaptation and Generalization 9.4. Few-Shot and \\nZero-Shot Detection \\n10. Conclusion \\n11. References'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='1. Introduction \\n1.1. Defining Object Detection: Classification and Localization \\nObject detection is a core computer vision task concerned with answering two fundamental questions \\nabout an image: \"What objects are in this image?\" and \"Where are they located?\". The first \\nquestion is a classification task, assigning a class label (e.g., \"cat,\" \"car,\" \"person\") to an object. The \\nsecond is a localization task, providing a tight-fitting bounding box (typically defined by x/y \\ncoordinates and width/height) around each identified object. \\n1.2. Distinction from Other Vision Tasks \\nIt is crucial to distinguish object detection from related tasks: \\n• Image Classification: Simply assigns one label to an entire image (e.g., \"this is a picture of a \\ncat\"). \\n• Semantic Segmentation: Assigns a class label to every pixel in the image but does not \\ndistinguish between different instances of the same object (e.g., all pixels belonging to any \\nperson are labeled \"person\").'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='distinguish between different instances of the same object (e.g., all pixels belonging to any \\nperson are labeled \"person\"). \\n• Instance Segmentation: Assigns a class label to every pixel and differentiates between \\nobject instances (e.g., \"person 1,\" \"person 2,\" \"person 3\"). Object detection can be seen as a \\nprecursor to this more complex task. \\n1.3. The Importance of Object Detection \\nThe ability to detect and locate objects is foundational to how machines perceive and interact with \\nthe physical world. It is the technology that enables self-driving cars to see pedestrians and other \\nvehicles, allows doctors to identify tumors in medical scans, and helps robots navigate complex \\nenvironments. Its broad applicability has made it one of the most actively researched areas in \\nartificial intelligence. \\n1.4. Paper Structure \\nThis paper will trace the evolution of object detection methods, beginning with a brief look at pre-'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'page': 2, 'page_label': '3', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='artificial intelligence. \\n1.4. Paper Structure \\nThis paper will trace the evolution of object detection methods, beginning with a brief look at pre-\\ndeep learning techniques. We will then delve into the two primary families of deep learning'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='detectors: two-stage and single-stage. We will discuss their core components, modern architectures \\nincluding Transformers, and conclude with evaluation metrics, applications, and future challenges. \\n \\n2. Background and Foundational Concepts \\n2.1. Traditional Computer Vision Approaches \\nBefore deep learning, object detection relied on hand-crafted features. Methods like the Viola-Jones \\nframework (famous for real-time face detection) used simple Haar-like features and a cascade of \\nclassifiers. Other approaches used more complex feature descriptors like HOG (Histogram of \\nOriented Gradients), often paired with a classifier like a Support Vector Machine (SVM), to \\nidentify objects. These methods were effective for specific tasks but were brittle and did not \\ngeneralize well. \\n2.2. The Sliding Window Method \\nA common technique was the sliding window approach. A window of a fixed size would be slid'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'page': 3, 'page_label': '4', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='generalize well. \\n2.2. The Sliding Window Method \\nA common technique was the sliding window approach. A window of a fixed size would be slid \\nacross all possible locations and scales of an image. For each window, a feature descriptor would be \\ncomputed and fed to a classifier. This method was computationally exhaustive and prone to errors. \\n2.3. The Deep Learning Revolution \\nThe success of AlexNet in the 2012 ImageNet classification challenge marked a turning point. \\nResearchers quickly realized that the rich, hierarchical features learned automatically by \\nConvolutional Neural Networks (CNNs) were far more powerful than any hand-crafted features. \\nThis discovery paved the way for the modern era of object detection. \\n \\n3. Two-Stage Object Detectors: A Focus on Accuracy \\nTwo-stage detectors break the object detection problem into two distinct steps, a paradigm that \\ngenerally leads to higher accuracy at the cost of speed. \\n3.1. The \"Propose, then Classify\" Paradigm'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content=\"The core idea is to first generate a sparse set of region proposals—areas of the image that are likely \\nto contain an object. In the second stage, a classifier is run only on these proposed regions to \\ndetermine the object's class and refine the bounding box. \\n3.2. R-CNN: Regions with CNN Features \\nR-CNN was the first major breakthrough in applying deep learning to this paradigm. However, its \\nprocess was slow and cumbersome: \\n1. Generate ~2000 region proposals using an external algorithm like Selective Search. \\n2. Warp/resize each proposed region to a fixed size. \\n3. Pass each warped region independently through a pre-trained CNN to extract features. \\n4. Use a set of SVMs to classify the object in each region. \\n3.3. Fast R-CNN: Sharing Computation \\nFast R-CNN made a significant improvement. Instead of running the CNN 2000 times, it passes the \\nentire image through the CNN just once to generate a feature map. The region proposals are then\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Fast R-CNN made a significant improvement. Instead of running the CNN 2000 times, it passes the \\nentire image through the CNN just once to generate a feature map. The region proposals are then \\nprojected onto this feature map. A novel RoI (Region of Interest) Pooling layer extracts a fixed-size \\nfeature vector from each proposed region, which is then fed into a classifier. This shared computation \\nmade the process much faster. \\n3.4. Faster R-CNN: The Region Proposal Network (RPN) \\nThe bottleneck in Fast R-CNN was the external Selective Search algorithm for proposing regions. \\nFaster R-CNN introduced the Region Proposal Network (RPN), a small neural network that learns \\nto generate high-quality region proposals directly from the CNN features. By integrating the RPN, \\nFaster R-CNN became the first truly end-to-end, unified deep learning object detector, setting a new \\nstandard for accuracy. \\n \\n4. Single-Stage Object Detectors: A Focus on Speed'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'page': 4, 'page_label': '5', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Faster R-CNN became the first truly end-to-end, unified deep learning object detector, setting a new \\nstandard for accuracy. \\n \\n4. Single-Stage Object Detectors: A Focus on Speed \\nSingle-stage detectors remove the region proposal step and instead perform localization and \\nclassification in a single forward pass of the network, making them extremely fast and suitable for \\nreal-time applications.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='4.1. The \"Single Pass\" Paradigm \\nThese models treat object detection as a regression problem. They look at the image once and \\ndirectly predict a set of bounding boxes and their corresponding class probabilities. \\n4.2. YOLO: You Only Look Once \\nThe YOLO family of models is renowned for its speed. YOLO divides the input image into a grid. \\nFor each grid cell, the model simultaneously predicts: \\n• Several bounding boxes. \\n• A \"confidence\" score for each box, indicating how likely it is to contain an object. \\n• Class probabilities for the object within the box. This unified architecture allows for end-to-\\nend training and blazingly fast inference speeds, making it ideal for video processing. \\n4.3. SSD: Single Shot MultiBox Detector \\nSSD aimed to find a middle ground between the speed of YOLO and the accuracy of Faster R-CNN. \\nIts key innovation is using feature maps from multiple layers of the backbone network to make'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='SSD aimed to find a middle ground between the speed of YOLO and the accuracy of Faster R-CNN. \\nIts key innovation is using feature maps from multiple layers of the backbone network to make \\npredictions. By making predictions at different scales, SSD is much better at detecting objects of \\nvarious sizes, particularly small ones, compared to the original YOLO. \\n \\n5. Key Architectural Components and Innovations \\nModern detectors, whether two-stage or single-stage, share several common components. \\n5.1. Backbone Networks: The Feature Extractors \\nThe backbone is a deep CNN (like ResNet, VGG, or MobileNet) pre-trained on a large image \\nclassification dataset (e.g., ImageNet). Its role is to act as a powerful feature extractor, converting the \\nraw pixel data of an image into rich, hierarchical feature maps that can be used for detection. \\n5.2. Anchor Boxes: The Priors for Prediction \\nInstead of predicting bounding boxes from scratch, most detectors predict offsets relative to a set of'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'page': 5, 'page_label': '6', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='5.2. Anchor Boxes: The Priors for Prediction \\nInstead of predicting bounding boxes from scratch, most detectors predict offsets relative to a set of \\npre-defined default boxes called anchor boxes. These anchors have various sizes and aspect ratios'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='and are tiled across the image at different locations. Using anchors reframes the problem from \\npredicting absolute coordinates to refining a well-placed prior, which makes learning easier for the \\nnetwork. \\n5.3. Non-Maximum Suppression (NMS): Pruning Redundant Detections \\nA detector will often output multiple, highly overlapping bounding boxes for the same object. NMS \\nis a crucial post-processing step that cleans up these redundant detections. It sorts all boxes by their \\nconfidence scores, keeps the box with the highest score, and suppresses (discards) any other boxes \\nthat have a high overlap with it. \\n \\n6. Modern Architectures and the Rise of Transformers \\n6.1. Balancing Speed and Accuracy: EfficientDet \\nThe EfficientDet family of models introduced a systematic way to scale detectors for different \\nresource constraints. It uses a highly efficient backbone (EfficientNet) and a novel feature fusion'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'page': 6, 'page_label': '7', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='The EfficientDet family of models introduced a systematic way to scale detectors for different \\nresource constraints. It uses a highly efficient backbone (EfficientNet) and a novel feature fusion \\nmechanism (BiFPN) to achieve state-of-the-art efficiency, balancing high accuracy with low \\ncomputational cost. \\n6.2. DETR: End-to-End Object Detection with Transformers \\nDETR (DEtection TRansformer) represents a major paradigm shift. It completely eliminates the need \\nfor hand-crafted components like anchor boxes and NMS. DETR uses a standard Transformer \\nencoder-decoder architecture, similar to those used in NLP. It treats object detection as a direct set \\nprediction problem: the model ingests image features and directly outputs the final set of unique \\nobject detections. This simplifies the detection pipeline significantly and has opened up a new and \\nexciting research direction. \\n \\n7. Evaluation Metrics and Datasets \\n7.1. Intersection over Union (IoU)'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='IoU is the fundamental metric used to measure the \"correctness\" of a predicted bounding box. It is \\ncalculated as the area of overlap between the predicted box and the ground-truth box, divided by the \\narea of their union. A detection is typically considered a \"true positive\" if its IoU with a ground-truth \\nbox is above a certain threshold (e.g., 0.5). \\n7.2. Average Precision (AP) and mean Average Precision (mAP) \\nAverage Precision (AP) is the primary metric for evaluating the performance of a detector on a \\nsingle object class. It is calculated from the precision-recall curve and effectively measures the \\ndetector\\'s accuracy across all confidence levels. Mean Average Precision (mAP) is the average of \\nthe AP values across all object classes and is the standard metric for comparing different object \\ndetection models. \\n7.3. Landmark Datasets \\nThe field has been driven by large-scale, publicly available datasets, most notably PASCAL VOC'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'page': 7, 'page_label': '8', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='detection models. \\n7.3. Landmark Datasets \\nThe field has been driven by large-scale, publicly available datasets, most notably PASCAL VOC \\nand Microsoft COCO (Common Objects in Context). The COCO dataset, with its large number of \\nobject categories and instances per image, is the current benchmark for modern object detectors. \\n \\n8. Applications and Real-World Use Cases \\nObject detection is a deployed and impactful technology across numerous industries. \\n• Autonomous Vehicles: Detecting cars, pedestrians, cyclists, and traffic signals is essential \\nfor safe navigation. \\n• Medical Imaging: Assisting radiologists by automatically locating tumors, lesions, or other \\nanomalies in X-rays, CT scans, and MRIs. \\n• Retail: Powering cashier-less stores, monitoring shelf inventory, and analyzing customer foot \\ntraffic. \\n• Security and Surveillance: Automatically detecting intruders, unattended baggage, or \\nmonitoring crowd density. \\n \\n9. Challenges and Future Directions'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'page': 8, 'page_label': '9', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='Despite immense progress, several challenges remain. \\n• Detecting Small and Occluded Objects: Models still struggle to reliably detect objects that \\nare very small, far away, or partially hidden. \\n• The Speed vs. Accuracy Trade-off: While models are becoming more efficient, the \\nfundamental trade-off between real-time speed and maximum accuracy remains a key design \\nconsideration. \\n• Domain Adaptation and Generalization: A model trained on daytime, sunny weather data \\nmay fail when deployed at night or in the rain. Improving robustness to new environments is \\na major challenge. \\n• Few-Shot and Zero-Shot Detection: Training models to detect new object categories with \\nvery few (or zero) labeled examples is an active and important area of research.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content=\"10. Conclusion \\nObject detection has undergone a remarkable transformation, moving from slow, brittle systems \\nbased on hand-crafted features to highly accurate and efficient end-to-end deep learning models. The \\nevolution from the methodical two-stage R-CNN family to the rapid single-stage YOLO and SSD \\ndetectors, and now to the elegant, anchor-free Transformer-based models like DETR, showcases the \\nfield's rapid pace of innovation. As a core enabling technology for machine perception, object \\ndetection continues to solve critical real-world problems and will undoubtedly remain a central focus \\nof AI research for years to come. \\n \\n11. References \\n[Viola & Jones, 2001] Viola, P., & Jones, M. (2001). Rapid object detection using a boosted cascade \\nof simple features. Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision \\nand Pattern Recognition. \\n[Girshick et al., 2014] Girshick, R., Donahue, J., Darrell, T., & Malik, J. (2014). Rich feature\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='and Pattern Recognition. \\n[Girshick et al., 2014] Girshick, R., Donahue, J., Darrell, T., & Malik, J. (2014). Rich feature \\nhierarchies for accurate object detection and semantic segmentation. Proceedings of the IEEE \\nconference on computer vision and pattern recognition. \\n[Ren et al., 2015] Ren, S., He, K., Girshick, R., & Sun, J. (2015). Faster R-CNN: Towards real-time \\nobject detection with region proposal networks. Advances in neural information processing systems. \\n[Redmon et al., 2016] Redmon, J., Divvala, S., Girshick, R., & Farhadi, A. (2016). You only look \\nonce: Unified, real-time object detection. Proceedings of the IEEE conference on computer vision \\nand pattern recognition. \\n[Liu et al., 2016] Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C. Y., & Berg, A. C. \\n(2016). SSD: Single shot multibox detector. European conference on computer vision. \\n[Carion et al., 2020] Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., & Zagoruyko, S.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'page': 9, 'page_label': '10', 'source_file': 'objectdetection.pdf', 'file_type': 'pdf'}, page_content='(2016). SSD: Single shot multibox detector. European conference on computer vision. \\n[Carion et al., 2020] Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., & Zagoruyko, S. \\n(2020). End-to-end object detection with transformers. European conference on computer vision.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:54:40+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:54:40+01:00', 'source': '..\\\\data\\\\pdf\\\\proposal.pdf', 'total_pages': 8, 'page': 0, 'page_label': '1', 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='Title: Self-Training with Uncertainty-Aware Style Transfer for Cross-Domain \\nObject Detection \\nPrincipal Investigator: [Your Name] Affiliation: [Your Institution/Research Group] Date: October \\n15, 2025 \\n \\nAbstract \\nModern object detection models achieve remarkable performance but suffer a significant drop in \\naccuracy when deployed in environments (target domains) that differ from their training data (source \\ndomain). This problem of domain shift is a major obstacle to the real-world application of \\ntechnologies like autonomous driving, where a vehicle must operate reliably in diverse weather, \\nlighting, and geographic conditions. This proposal outlines a research project to develop a novel \\nframework for unsupervised domain adaptation in object detection. We propose a method that \\ncombines generative style transfer with a robust self-training mechanism. Specifically, we will use a \\nCycle-Consistent Generative Adversarial Network (CycleGAN) to translate images between'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:54:40+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:54:40+01:00', 'source': '..\\\\data\\\\pdf\\\\proposal.pdf', 'total_pages': 8, 'page': 0, 'page_label': '1', 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='combines generative style transfer with a robust self-training mechanism. Specifically, we will use a \\nCycle-Consistent Generative Adversarial Network (CycleGAN) to translate images between \\ndomains, artificially augmenting the training data. More importantly, we will enhance a self-training \\npipeline by incorporating uncertainty estimation. By using techniques like Monte Carlo Dropout, \\nour model will only leverage pseudo-labels from the target domain in which it has high confidence, \\npreventing the accumulation of errors from incorrect labels. We hypothesize that this uncertainty-\\naware approach will make the self-training process more stable and effective, leading to a significant \\nimprovement in object detection performance in unseen target domains. The proposed research will \\nbe evaluated on benchmark datasets like Cityscapes and Foggy Cityscapes, with the goal of creating \\nmore robust and reliable perception systems.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:54:40+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:54:40+01:00', 'source': '..\\\\data\\\\pdf\\\\proposal.pdf', 'total_pages': 8, 'page': 0, 'page_label': '1', 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='be evaluated on benchmark datasets like Cityscapes and Foggy Cityscapes, with the goal of creating \\nmore robust and reliable perception systems. \\nKeywords: Research Proposal, Object Detection, Domain Adaptation, Self-Training, Uncertainty \\nEstimation, Style Transfer, Autonomous Vehicles.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:54:40+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:54:40+01:00', 'source': '..\\\\data\\\\pdf\\\\proposal.pdf', 'total_pages': 8, 'page': 1, 'page_label': '2', 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='Table of Contents \\n1. Introduction and Problem Statement 1.1. The Success and Brittleness of Modern Detectors \\n1.2. The Challenge of Domain Shift 1.3. Research Questions and Objectives 1.4. Proposed \\nContribution \\n2. Literature Review 2.1. State-of-the-Art Object Detection Models 2.2. Unsupervised Domain \\nAdaptation (UDA) 2.3. UDA Techniques in Object Detection 2.3.1. Adversarial Training \\nMethods 2.3.2. Style Transfer and Image-to-Image Translation 2.3.3. Self-Training and \\nPseudo-Labeling \\n3. Proposed Methodology 3.1. Overall Framework Architecture 3.2. Module 1: Cross-Domain \\nStyle Transfer 3.3. Module 2: Self-Training with Pseudo-Labeling 3.4. The Core Innovation: \\nUncertainty-Aware Label Filtering \\n4. Experimental Setup and Evaluation 4.1. Datasets and Benchmarks 4.2. Baseline Models \\nfor Comparison 4.3. Evaluation Metrics 4.4. Implementation Details \\n5. Expected Results and Broader Impact 5.1. Hypothesized Performance Gains 5.2. Impact'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:54:40+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:54:40+01:00', 'source': '..\\\\data\\\\pdf\\\\proposal.pdf', 'total_pages': 8, 'page': 1, 'page_label': '2', 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='for Comparison 4.3. Evaluation Metrics 4.4. Implementation Details \\n5. Expected Results and Broader Impact 5.1. Hypothesized Performance Gains 5.2. Impact \\non Autonomous Systems and Robotics 5.3. Contribution to the Field of AI \\n6. Plan of Work and Timeline 6.1. Phase 1: Literature Review and Environment Setup \\n(Months 1-2) 6.2. Phase 2: Implementation of Core Modules (Months 3-6) 6.3. Phase 3: \\nExperimentation and Analysis (Months 7-10) 6.4. Phase 4: Dissemination of Results (Months \\n11-12) \\n7. Ethical Considerations \\n8. Conclusion \\n9. References'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:54:40+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:54:40+01:00', 'source': '..\\\\data\\\\pdf\\\\proposal.pdf', 'total_pages': 8, 'page': 2, 'page_label': '3', 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='1. Introduction and Problem Statement \\n1.1. The Success and Brittleness of Modern Detectors \\nDeep learning-based object detectors like Faster R-CNN and YOLO have become incredibly \\naccurate, forming the perception backbone for many emerging technologies. However, their success \\nis predicated on the assumption that the training and testing data are drawn from the same statistical \\ndistribution. \\n1.2. The Challenge of Domain Shift \\nIn the real world, this assumption is frequently violated. A model trained exclusively on clear, sunny \\nday driving data may fail catastrophically when deployed at night, in the rain, or in a city with \\ndifferent architecture. This phenomenon is known as domain shift. Manually annotating data for \\nevery possible domain is prohibitively expensive and unscalable. Therefore, Unsupervised Domain \\nAdaptation (UDA), which aims to adapt a model trained on a labeled source domain to an unlabeled \\ntarget domain, is a critical area of research.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:54:40+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:54:40+01:00', 'source': '..\\\\data\\\\pdf\\\\proposal.pdf', 'total_pages': 8, 'page': 2, 'page_label': '3', 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='Adaptation (UDA), which aims to adapt a model trained on a labeled source domain to an unlabeled \\ntarget domain, is a critical area of research. \\n1.3. Research Questions and Objectives \\nThis research aims to answer the following question: How can we make an object detector robust to \\ndomain shift without requiring any labeled data from the new domain? Our primary objectives are: \\n1. To design a framework that leverages both image-level style translation and model-level self-\\ntraining. \\n2. To develop a novel mechanism to filter noisy pseudo-labels generated during self-training by \\nestimating model uncertainty. \\n3. To empirically validate the proposed framework and demonstrate its superiority over existing \\nUDA methods. \\n1.4. Proposed Contribution \\nThe main contribution of this work will be an uncertainty-aware self-training framework. While \\nself-training is a common technique, it is often unstable because the model can reinforce its own'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:54:40+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:54:40+01:00', 'source': '..\\\\data\\\\pdf\\\\proposal.pdf', 'total_pages': 8, 'page': 2, 'page_label': '3', 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='The main contribution of this work will be an uncertainty-aware self-training framework. While \\nself-training is a common technique, it is often unstable because the model can reinforce its own \\nmistakes by trusting incorrect \"pseudo-labels.\" By introducing a principled mechanism for the model'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:54:40+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:54:40+01:00', 'source': '..\\\\data\\\\pdf\\\\proposal.pdf', 'total_pages': 8, 'page': 3, 'page_label': '4', 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='to gauge its own uncertainty, we can select only the most reliable pseudo-labels, leading to more \\nstable and effective adaptation. \\n \\n2. Literature Review \\n2.1. State-of-the-Art Object Detection Models \\nOur work will build upon established object detection architectures. We will consider both a two-\\nstage detector (e.g., Faster R-CNN) and a Transformer-based detector (e.g., DETR) as the base \\nmodels for our adaptation framework. \\n2.2. Unsupervised Domain Adaptation (UDA) \\nUDA is a well-established field in machine learning. The central goal is to leverage a label-rich \\nsource domain to learn a task in a label-scarce target domain. \\n2.3. UDA Techniques in Object Detection \\n• Adversarial Training: These methods use a \"domain discriminator\" network that tries to \\ndistinguish between features from the source and target domains. The main network is then \\ntrained to produce features that can \"fool\" this discriminator, thereby learning domain-\\ninvariant features.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:54:40+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:54:40+01:00', 'source': '..\\\\data\\\\pdf\\\\proposal.pdf', 'total_pages': 8, 'page': 3, 'page_label': '4', 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='trained to produce features that can \"fool\" this discriminator, thereby learning domain-\\ninvariant features. \\n• Style Transfer: Generative models like GANs are used to translate images from the source \\nstyle to the target style (e.g., making a sunny image look foggy). This creates a synthetic \\nlabeled dataset in the target style. \\n• Self-Training: This involves using an initial model to make predictions on the unlabeled \\ntarget data. The most confident predictions are then treated as \"pseudo-labels\" and are used to \\nretrain the model. This approach is powerful but risks error accumulation if the pseudo-labels \\nare noisy. Our proposed work directly addresses this key limitation. \\n \\n3. Proposed Methodology \\n3.1. Overall Framework Architecture'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:54:40+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:54:40+01:00', 'source': '..\\\\data\\\\pdf\\\\proposal.pdf', 'total_pages': 8, 'page': 4, 'page_label': '5', 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='The proposed system will consist of three interconnected modules operating on a base object \\ndetector. The model will be trained on labeled source data (e.g., sunny images) and unlabeled target \\ndata (e.g., rainy images). \\n3.2. Module 1: Cross-Domain Style Transfer \\nWe will first train a CycleGAN model to learn the mappings between the source and target domains. \\nThis will allow us to translate a source image into a \"fake\" target image (e.g., sunny -> rainy) and \\nvice-versa. This provides a basic form of data augmentation, allowing the detector to see labeled \\nimages that look like they are from the target domain. \\n3.3. Module 2: Self-Training with Pseudo-Labeling \\nIn parallel, we will use the model trained on the source data to generate predictions (bounding boxes \\nand classes) for the unlabeled target domain images. These predictions will serve as initial pseudo-\\nlabels. \\n3.4. The Core Innovation: Uncertainty-Aware Label Filtering'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:54:40+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:54:40+01:00', 'source': '..\\\\data\\\\pdf\\\\proposal.pdf', 'total_pages': 8, 'page': 4, 'page_label': '5', 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content=\"and classes) for the unlabeled target domain images. These predictions will serve as initial pseudo-\\nlabels. \\n3.4. The Core Innovation: Uncertainty-Aware Label Filtering \\nThis is the central component of our proposal. Instead of naively trusting all pseudo-labels above a \\nsimple confidence threshold, we will estimate the model's uncertainty for each prediction. We will \\nuse Monte Carlo Dropout, a technique where dropout is applied at inference time over multiple \\nforward passes. The variance in the resulting predictions serves as a strong indicator of model \\nuncertainty. We will then filter the pseudo-labels using a combined score of confidence and low \\nuncertainty. Only the most certain and confident pseudo-labels will be added to a replay buffer used \\nto fine-tune the detector, making the adaptation process robust to noise. \\n \\n4. Experimental Setup and Evaluation \\n4.1. Datasets and Benchmarks\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:54:40+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:54:40+01:00', 'source': '..\\\\data\\\\pdf\\\\proposal.pdf', 'total_pages': 8, 'page': 4, 'page_label': '5', 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='to fine-tune the detector, making the adaptation process robust to noise. \\n \\n4. Experimental Setup and Evaluation \\n4.1. Datasets and Benchmarks \\nWe will focus on autonomous driving scenarios. The primary experiment will be adapting from the \\nCityscapes dataset (clear weather) to the Foggy Cityscapes dataset. We will also evaluate on other \\ncommon shifts, such as adapting from synthetic data (Sim10k) to real-world data (KITTI).'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:54:40+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:54:40+01:00', 'source': '..\\\\data\\\\pdf\\\\proposal.pdf', 'total_pages': 8, 'page': 5, 'page_label': '6', 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='4.2. Baseline Models for Comparison \\nWe will compare our method against three baselines: \\n1. A \"Lower Bound\" model trained only on source data. \\n2. A state-of-the-art adversarial training method for UDA. \\n3. A standard self-training method without uncertainty awareness. \\n4.3. Evaluation Metrics \\nPerformance will be measured using the standard object detection metric, mean Average Precision \\n(mAP), calculated on the labeled validation set of the target domain. \\n \\n5. Expected Results and Broader Impact \\n5.1. Hypothesized Performance Gains \\nWe expect our uncertainty-aware framework to significantly outperform the baselines. We \\nhypothesize that by reducing the noise in the pseudo-labeling process, our model will adapt more \\neffectively, resulting in a 5-10% absolute improvement in mAP on the target domain compared to \\nstandard self-training methods. \\n5.2. Impact on Autonomous Systems and Robotics \\nA more robust perception system directly translates to increased safety and reliability for'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:54:40+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:54:40+01:00', 'source': '..\\\\data\\\\pdf\\\\proposal.pdf', 'total_pages': 8, 'page': 5, 'page_label': '6', 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='standard self-training methods. \\n5.2. Impact on Autonomous Systems and Robotics \\nA more robust perception system directly translates to increased safety and reliability for \\nautonomous vehicles, drones, and industrial robots. This research could help bridge the gap between \\ndevelopment and real-world deployment of these technologies. \\n \\n6. Plan of Work and Timeline \\nThe project is planned for a 12-month period. \\n• Phase 1 (Months 1-2): In-depth literature review; setting up the computational environment \\nand baseline models.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:54:40+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:54:40+01:00', 'source': '..\\\\data\\\\pdf\\\\proposal.pdf', 'total_pages': 8, 'page': 6, 'page_label': '7', 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='• Phase 2 (Months 3-6): Implementation of the style transfer module and the uncertainty-\\naware self-training loop. \\n• Phase 3 (Months 7-10): Conducting extensive experiments on benchmark datasets, \\nanalyzing results, and performing ablation studies. \\n• Phase 4 (Months 11-12): Writing a research paper for submission to a top-tier computer \\nvision conference (e.g., CVPR, ICCV) and finalizing the project report. \\n \\n7. Ethical Considerations \\nThe primary application of this research is to enhance safety in autonomous systems. However, \\nobject detection technology can also be used for surveillance. Our research will be conducted \\ntransparently, and we will focus our evaluation on publicly available datasets related to driving. We \\nwill not use private or personally identifiable data. The code and models will be made publicly \\navailable to ensure reproducibility and encourage positive use. \\n \\n8. Conclusion'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:54:40+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:54:40+01:00', 'source': '..\\\\data\\\\pdf\\\\proposal.pdf', 'total_pages': 8, 'page': 6, 'page_label': '7', 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='will not use private or personally identifiable data. The code and models will be made publicly \\navailable to ensure reproducibility and encourage positive use. \\n \\n8. Conclusion \\nThis research proposal addresses the critical problem of domain shift in object detection. By \\nproposing a novel framework that integrates style transfer with a more robust, uncertainty-aware \\nself-training mechanism, we aim to significantly advance the state of the art in unsupervised domain \\nadaptation. The successful completion of this project will produce more reliable perception models, \\nthereby accelerating the safe and responsible deployment of AI in real-world applications. \\n \\n9. References \\n[Hoffman et al., 2018] Hoffman, J., Tzeng, E., Park, T., Zhu, J. Y., Isola, P., Saenko, K., ... & \\nDarrell, T. (2018). CyCADA: Cycle-consistent adversarial domain adaptation. International \\nconference on machine learning.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:54:40+01:00', 'author': 'ISMAIL LAMRANI', 'moddate': '2025-10-15T15:54:40+01:00', 'source': '..\\\\data\\\\pdf\\\\proposal.pdf', 'total_pages': 8, 'page': 7, 'page_label': '8', 'source_file': 'proposal.pdf', 'file_type': 'pdf'}, page_content='[Saito et al., 2017] Saito, K., Watanabe, K., Ushiku, Y., & Harada, T. (2017). Asymmetric tri-\\ntraining for unsupervised domain adaptation. International conference on machine learning. \\n[Gal & Ghahramani, 2016] Gal, Y., & Ghahramani, Z. (2016). Dropout as a Bayesian approximation: \\nRepresenting model uncertainty in deep learning. International conference on machine learning. \\n[Zou et al., 2018] Zou, Y., Yu, Z., Kumar, B., & Wang, J. (2018). Unsupervised domain adaptation \\nfor semantic segmentation via class-balanced self-training. European conference on computer vision.')]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3ae00695",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Attention Mechanism in Neural Networks:\\nWhere it Comes and Where it Goes\\nDerya Soydaner\\nReceived: 22 July 2021 / Accepted: 27 April 2022\\nAbstract A long time ago in the machine learning literature, the idea of\\nincorporating a mechanism inspired by the human visual system into neural\\nnetworks was introduced. This idea is named the attention mechanism, and it\\nhas gone through a long development period. Today, many works have been\\ndevoted to this idea in a variety of tasks. Remarkable performance has re-\\ncently been demonstrated. The goal of this paper is to provide an overview\\nfrom the early work on searching for ways to implement attention idea with\\nneural networks until the recent trends. This review emphasizes the impor-\\ntant milestones during this progress regarding diﬀerent tasks. By this way,\\nthis study aims to provide a road map for researchers to explore the current\\ndevelopment and get inspired for novel approaches beyond the attention.',\n",
       " 'this study aims to provide a road map for researchers to explore the current\\ndevelopment and get inspired for novel approaches beyond the attention.\\nKeywords Attention mechanism ·Neural networks ·Deep learning ·Survey\\n1 Introduction\\nHuman eye sees the world in an interesting way. We suppose as if we see the\\nentire scene at once, but this is an illusion created by the subconscious part\\nof our brain [1]. According to the Scanpath theory [2,3], when the human eye\\nlooks at an image, it can see only a small patch in high resolution. This small\\npatch is called thefovea. It can see the rest of the image in low resolution which\\nis called the periphery. To recognize the entire scene, the eye performs feature\\nextraction based on the fovea. The eye is moved to diﬀerent parts of the image\\nuntil the information obtained from the fovea is suﬃcient for recognition [4].\\nThese eye movements are called saccades. The eye makes successive ﬁxations\\nDerya Soydaner',\n",
       " 'until the information obtained from the fovea is suﬃcient for recognition [4].\\nThese eye movements are called saccades. The eye makes successive ﬁxations\\nDerya Soydaner\\nDepartment of Brain and Cognition, University of Leuven (KU Leuven), Leuven, Belgium\\nTel.: +32-16710471\\nE-mail: derya.soydaner@kuleuven.be\\narXiv:2204.13154v1  [cs.LG]  27 Apr 2022',\n",
       " '2 Derya Soydaner\\nuntil the recognition task is complete. This sequential process happens so\\nquickly that we feel as if it happens all at once.\\nBiologically, this is called visual attention system . Visual attention is de-\\nﬁned as the ability to dynamically restrict processing to a subset of the visual\\nﬁeld [5]. It seeks answers for two main questions: What and where to look?\\nVisual attention has been extensively studied in psychology and neuroscience;\\nfor reviews see [6,7,8,9,10]. Besides, there is a large amount of literature on\\nmodeling eye movements [11,12,13,14]. These studies have been a source of\\ninspiration for many artiﬁcial intelligence tasks. It has been discovered that\\nthe attention idea is useful from image recognition to machine translation.\\nTherefore, diﬀerent types of attention mechanisms inspired from the human\\nvisual system have been developed for years. Since the success of deep neural\\nnetworks has been at the forefront for these artiﬁcial intelligence tasks, these',\n",
       " 'visual system have been developed for years. Since the success of deep neural\\nnetworks has been at the forefront for these artiﬁcial intelligence tasks, these\\nmechanisms have been integrated into neural networks for a long time.\\nThis survey is about the journey of attention mechanisms used with neu-\\nral networks. Researchers have been investigating ways to strengthen neural\\nnetwork architectures with attention mechanisms for many years. The pri-\\nmary aim of these studies is to reduce computational burden and to improve\\nthe model performance as well. Previous work reviewed the attention mecha-\\nnisms from diﬀerent perspectives [15], or examined them in context of natural\\nlanguage processing (NLP) [16,17]. However, in this study, we examine the\\ndevelopment of attention mechanisms over the years, and recent trends. We\\nbegin with the ﬁrst attempts to integrate the visual attention idea to neural\\nnetworks, and continue until the most modern neural networks armed with at-',\n",
       " 'begin with the ﬁrst attempts to integrate the visual attention idea to neural\\nnetworks, and continue until the most modern neural networks armed with at-\\ntention mechanisms. One of them is the Transformer, which is used for many\\nstudies including the GPT-3 language model [18], goes beyond convolutions\\nand recurrence by replacing them with only attention layers [19]. Finally, we\\ndiscuss how much more can we move forward, and what’s next?\\n2 From the Late 1980s to Early 2010s: The Attention Awakens\\nThe ﬁrst attempts at adapting attention mechanisms to neural networks go\\nback to the late 1980s. One of the early studies is the improved version of\\nthe Neocognitron [20] with selective attention [21]. This study is then mod-\\niﬁed to recognize and segment connected characters in cursive handwriting\\n[22]. Another study describes VISIT, a novel model that concentrates on its\\nrelationship to a number of visual areas of the brain [5]. Also, a novel archi-',\n",
       " '[22]. Another study describes VISIT, a novel model that concentrates on its\\nrelationship to a number of visual areas of the brain [5]. Also, a novel archi-\\ntecture named Signal Channelling Attentional Network (SCAN) is presented\\nfor attentional scanning [23].\\nEarly work on improving the attention idea for neural networks includes\\na variety of tasks such as target detection [24]. In another study, a visual at-\\ntention system extracts regions of interest by combining the bottom-up and\\ntop-down information from the image [25]. A recognition model based on se-\\nlective attention which analyses only a small part of the image at each step,\\nand combines results in time is described [4]. Besides, a model based on the',\n",
       " 'Attention Mechanism in Neural Networks: 3\\nconcept of selective tuning is proposed [26]. As the years go by, several studies\\nthat use the attention idea in diﬀerent ways have been presented for visual\\nperception and recognition [27,28,29,30].\\nBy the 2000s, the studies on making attention mechanisms more useful for\\nneural networks continued. In the early years, a model that integrates an at-\\ntentional orienting where pathway and an object recognition what pathway is\\npresented [31]. A computational model of human eye movements is proposed\\nfor an object class detection task [32]. A serial model is presented for visual pat-\\ntern recognition gathering Markov models and neural networks with selective\\nattention on the handwritten digit recognition and face recognition problems\\n[33]. In that study, a neural network analyses image parts and generates pos-\\nterior probabilities as observations to the Markov model. Also, attention idea',\n",
       " '[33]. In that study, a neural network analyses image parts and generates pos-\\nterior probabilities as observations to the Markov model. Also, attention idea\\nis used for object recognition [34], and the analysis of a scene [35]. An inter-\\nesting study proposes to learn sequential attention in real-world visual object\\nrecognition using a Q-learner [36]. Besides, a computational model of visual\\nselective attention is described to automatically detect the most relevant parts\\nof a color picture displayed on a television screen [37]. The attention idea is\\nalso used for identifying and tracking objects in multi-resolution digital video\\nof partially cluttered environments [38].\\nIn 2010, the ﬁrst implemented system inspired by the fovea of human retina\\nwas presented for image classiﬁcation [39]. This system jointly trains a re-\\nstricted Boltzmann machine (RBM) and an attentional component called the\\nﬁxation controller. Similarly, a novel attentional model is implemented for si-',\n",
       " 'stricted Boltzmann machine (RBM) and an attentional component called the\\nﬁxation controller. Similarly, a novel attentional model is implemented for si-\\nmultaneous object tracking and recognition that is driven by gaze data [40].\\nBy taking advantage of reinforcement learning, a novel recurrent neural net-\\nwork (RNN) is described for image classiﬁcation [41]. Deep Attention Selective\\nNetwork (DasNet), a deep neural network with feedback connections that are\\nlearned through reinforcement learning to direct selective attention to certain\\nfeatures extracted from images, is presented [42]. Additionally, a deep learning\\nbased framework using attention has been proposed for generative modeling\\n[43].\\n3 2015: The Rise of Attention\\nIt can be said that 2015 is the golden year of attention mechanisms. Because\\nthe number of attention studies has grown like an avalanche after three main\\nstudies presented in that year. The ﬁrst one proposed a novel approach for',\n",
       " 'the number of attention studies has grown like an avalanche after three main\\nstudies presented in that year. The ﬁrst one proposed a novel approach for\\nneural machine translation (NMT) [44]. As it is known, most of the NMT\\nmodels belong to a family of encoder-decoders [45,46], with an encoder and a\\ndecoder for each language. However, compressing all the necessary information\\nof a source sentence into a ﬁxed-length vector is an important disadvantage of\\nthis encoder-decoder approach. This usually makes it diﬃcult for the neural\\nnetwork to capture all the semantic details of a very long sentence [1].\\nThe idea that [44] introduced is an extension to the conventional NMT\\nmodels. This extension is composed of an encoder and decoder as shown in',\n",
       " '4 Derya Soydaner\\nFig. 1 The extension to the conventional NMT models that is proposed by [44]. It generates\\nthe t-th target word yt given a source sentence ( x1,x2,...,x T ).\\nFig 1. The ﬁrst part, encoder, is a bidirectional RNN (BiRNN) [47] that takes\\nword vectors as input. The forward and backward states of BiRNN are com-\\nputed. Then, an annotation aj for each word xj is obtained by concatenating\\nthese forward and backward hidden states. Thus, the encoder maps the input\\nsentence to a sequence of annotations ( a1,...,a Tx ). By using a BiRNN rather\\nthan conventional RNN, the annotation of each word can summarize both\\nthe preceding words and the following words. Besides, the annotation aj can\\nfocus on the words around xj because of the inherent nature of RNNs that\\nrepresenting recent inputs better.\\nIn decoder, a weight αij of each annotation aj is obtained by using its\\nassociated energy eij that is computed by a feedforward neural network f as',\n",
       " 'representing recent inputs better.\\nIn decoder, a weight αij of each annotation aj is obtained by using its\\nassociated energy eij that is computed by a feedforward neural network f as\\nin Eq. (1). This neural network f is deﬁned as an alignment model that can\\nbe jointly trained with the proposed architecture. In order to reduce compu-\\ntational burden, a multilayer perceptron (MLP) with a single hidden layer is\\nproposed as f. This alignment model tells us about the relation between the\\ninputs around position j and the output at position i. By this way, the decoder\\napplies an attention mechanism. As it is seen in Eq. (2), the αij is the output\\nof softmax function:\\neij = f(hi−1,aj) (1)\\nαij = exp(eij)∑Tx\\nk=1 exp(eik)\\n(2)',\n",
       " 'Attention Mechanism in Neural Networks: 5\\nHere, the probability αij determines the importance of annotation aj with\\nrespect to the previous hidden state hi−1. Finally, the context vector ci is\\ncomputed as a weighted sum of these annotations as follows [44]:\\nci =\\nTx∑\\nj=1\\nαijaj (3)\\nBased on the decoder state, the context and the last generated word, the\\ntarget word yt is predicted. In order to generate a word in a translation, the\\nmodel searches for the most relevant information in the source sentence to\\nconcentrate. When it ﬁnds the appropriate source positions, it makes the pre-\\ndiction. By this way, the input sentence is encoded into a sequence of vectors\\nand a subset of these vectors is selected adaptively by the decoder that is rel-\\nevant to predicting the target [44]. Thus, it is no longer necessary to compress\\nall the information of a source sentence into a ﬁxed-length vector.\\nThe second study is the ﬁrst visual attention model in image captioning',\n",
       " 'all the information of a source sentence into a ﬁxed-length vector.\\nThe second study is the ﬁrst visual attention model in image captioning\\n[48]. Diﬀerent from the previous study [44], it uses a deep convolutional neural\\nnetwork (CNN) as an encoder. This architecture is an extension of the neural\\nnetwork [49] that encodes an image into a compact representation, followed by\\nan RNN that generates a corresponding sentence. Here, the annotation vectors\\nai ∈RD are extracted from a lower convolutional layer, each of which is a D-\\ndimensional representation corresponding to a part of the image. Thus, the\\ndecoder selectively focuses on certain parts of an image by weighting a subset\\nof all the feature vectors [48]. This extended architecture uses attention for\\nsalient features to dynamically come to the forefront instead of compressing\\nthe entire image into a static representation.\\nThe context vector ct represents the relevant part of the input image at',\n",
       " 'salient features to dynamically come to the forefront instead of compressing\\nthe entire image into a static representation.\\nThe context vector ct represents the relevant part of the input image at\\ntime t. The weight αi of each annotation vector is computed similar to Eq. (2),\\nwhereas its associated energy is computed similar to Eq. (1) by using an MLP\\nconditioned on the previous hidden state ht−1. The remarkable point of this\\nstudy is a new mechanism φ that computes ct from the annotation vectors ai\\ncorresponding to the features extracted at diﬀerent image locations:\\nct = φ(\\n{\\nai\\n}\\n,\\n{\\nαi\\n}\\n) (4)\\nThe deﬁnition of the φ function causes two variants of attention mecha-\\nnisms: The hard (stochastic) attention mechanism is trainable by maximizing\\nan approximate variational lower bound, i.e., by REINFORCE [50]. On the\\nother side, the soft (deterministic) attention mechanism is trainable by stan-\\ndard backpropagation methods. The hard attention deﬁnes a location variable',\n",
       " 'other side, the soft (deterministic) attention mechanism is trainable by stan-\\ndard backpropagation methods. The hard attention deﬁnes a location variable\\nst, and uses it to decide where to focus attention when generating the t-th\\nword. When the hard attention is applied, the attention locations are con-\\nsidered as intermediate latent variables. It assigns a multinoulli distribution\\nparametrized by αi, and ct becomes a random variable. Here, st,i is deﬁned\\nas a one-hot variable which is set to 1 if the i-th location is used to extract\\nvisual features [48]:',\n",
       " '6 Derya Soydaner\\np(st,i = 1|sj<t,a) = αt,i (5)\\nct =\\n∑\\ni\\nst,iai (6)\\nWhereas learning hard attention requires sampling the attention location\\nst each time, the soft attention mechanism computes a weighted annotation\\nvector similar to [44] and takes the expectation of the context vectorct directly:\\nEp(st|α)[ct] =\\nL∑\\ni=1\\nαt,iai (7)\\nFurthermore, in training the deterministic version of the model, an alterna-\\ntive method namely doubly stochastic attention, is proposed with an additional\\nconstraint added to the training objective to encourage the model to pay equal\\nattention to all parts of the image.\\nThe third study should be emphasized presents two classes of attention\\nmechanisms for NMT: the global attention that always attends to all source\\nwords, and the local attention that only looks at a subset of source words at\\na time [51]. These mechanisms derive the context vector ct in diﬀerent ways:\\nWhereas the global attention considers all the hidden states of the encoder, the',\n",
       " 'a time [51]. These mechanisms derive the context vector ct in diﬀerent ways:\\nWhereas the global attention considers all the hidden states of the encoder, the\\nlocal one selectively focuses on a small window of context. In global attention, a\\nvariable-length alignment vector is derived similar to Eq. (2). Here, the current\\ntarget hidden state ht is compared with each source hidden state ¯hs by using a\\nscore function instead of the associated energy eij. Thus, the alignment vector\\nwhose size equals the number of time steps on the source side is derived.\\nGiven the alignment vector as weights, the context vector ct is computed as\\nthe weighted average over all the source hidden states. Here, score is referred\\nas a content-based function, and three diﬀerent alternatives are considered [51].\\nOn the other side, the local attention is diﬀerentiable. Firstly, an aligned\\nposition pt is generated for each target word at a time t. Then, a window',\n",
       " 'On the other side, the local attention is diﬀerentiable. Firstly, an aligned\\nposition pt is generated for each target word at a time t. Then, a window\\ncentered around the source position pt is used to compute the context vector\\nas a weighted average of the source hidden states within the window. The\\nlocal attention selectively focuses on a small window of context, and obtains\\nthe alignment vector from the current target state ht and the source states ¯hs\\nin the window [51].\\nThe introduction of these novel mechanisms in 2015 triggered the rise of\\nattention for neural networks. Based on the proposed attention mechanisms,\\nsigniﬁcant research has been conducted in a variety of tasks. In order to imag-\\nine the attention idea in neural networks better, two visual examples are shown\\nin Fig. 2. A neural image caption generation task is seen in the top row that\\nimplements an attention mechanism [48]. Then, the second example shows',\n",
       " 'in Fig. 2. A neural image caption generation task is seen in the top row that\\nimplements an attention mechanism [48]. Then, the second example shows\\nhow the attention mechanisms can be used for visual question answering [52].\\nBoth examples demonstrate how attention mechanisms focus on parts of input\\nimages.',\n",
       " 'Attention Mechanism in Neural Networks: 7\\nFig. 2 Examples of the attention mechanism in visual. (Top) Attending to the correct\\nobject in neural image caption generation [48]. (Bottom) Visualization of original image\\nand question pairs, and co-attention maps namely word-level, phrase-level and question-\\nlevel, respectively [52].\\n4 2015-2016: Attack of the Attention\\nDuring two years from 2015, the attention mechanisms were used for diﬀerent\\ntasks, and novel neural network architectures were presented applying these\\nmechanisms. After the memory networks [53] that require a supervision signal\\ninstructing them how to use their memory cells, the introduction of the neural\\nTuring machine [54] allows end-to-end training without this supervision signal,\\nvia the use of a content-based soft attention mechanism [1]. Then, end-to-end\\nmemory network [55] that is a form of memory network based on a recurrent\\nattention mechanism is proposed.',\n",
       " 'via the use of a content-based soft attention mechanism [1]. Then, end-to-end\\nmemory network [55] that is a form of memory network based on a recurrent\\nattention mechanism is proposed.\\nIn these years, an attention mechanism called self-attention, sometimes\\ncalled intra-attention, was successfully implemented within a neural network\\narchitecture namely Long Short-Term Memory-Networks (LSTMN) [56]. It\\nmodiﬁes the standard LSTM structure by replacing the memory cell with a\\nmemory network [53]. This is because memory networks have a set of key\\nvectors and a set of value vectors, whereas LSTMs maintain a hidden vector\\nand a memory vector [56]. In contrast to attention idea in [44], memory and\\nattention are added within a sequence encoder in LSTMN. In order to compute\\na representation of a sequence, self-attention is described as relating diﬀerent\\npositions of it [19]. One of the ﬁrst approaches of self-attention is applied for\\nnatural language inference [57].',\n",
       " 'a representation of a sequence, self-attention is described as relating diﬀerent\\npositions of it [19]. One of the ﬁrst approaches of self-attention is applied for\\nnatural language inference [57].\\nMany attention-based models have been proposed for neural image cap-\\ntioning [58], abstractive sentence summarization [59], speech recognition [60,\\n61], automatic video captioning [62], neural machine translation [63], and rec-\\nognizing textual entailment [64]. Diﬀerent attention-based models perform vi-\\nsual question answering [65,66,67]. An attention-based CNN is presented for\\nmodeling sentence pairs [68]. A recurrent soft attention based model learns to\\nfocus selectively on parts of the video frames and classiﬁes videos [69].\\nOn the other side, several neural network architectures have been pre-\\nsented in a variety of tasks. For instance, Stacked Attention Network (SAN)',\n",
       " '8 Derya Soydaner\\nis described for image question answering [70]. Deep Attention Recurrent Q-\\nNetwork (DARQN) integrates soft and hard attention mechanisms into the\\nstructure of Deep Q-Network (DQN) [71]. Wake-Sleep Recurrent Attention\\nModel (WS-RAM) speeds up the training time for image classiﬁcation and\\ncaption generation tasks [72]. alignDRAW model, an extension of the Deep\\nRecurrent Attention Writer (DRAW) [73], is a generative model of images\\nfrom captions using a soft attention mechanism [74]. Generative Adversarial\\nWhat-Where Network (GAWWN) synthesizes images given instructions de-\\nscribing what content to draw in which location [75].\\n5 The Transformer: Return of the Attention\\nAfter the proposed attention mechanisms in 2015, researchers published stud-\\nies that mostly modifying or implementing them to diﬀerent tasks. However,\\nin 2017, a novel neural network architecture, namely the Transformer, based\\nentirely on self-attention was presented [19]. The Transformer achieved great',\n",
       " 'in 2017, a novel neural network architecture, namely the Transformer, based\\nentirely on self-attention was presented [19]. The Transformer achieved great\\nresults on two machine translation tasks in addition to English constituency\\nparsing. The most impressive point about this architecture is that it contains\\nneither recurrence nor convolution. The Transformer performs well by replac-\\ning the conventional recurrent layers in encoder-decoder architecture used for\\nNMT with self-attention.\\nThe Transformer is composed of encoder-decoder stacks each of which has\\nsix identical layers within itself. In Fig. 3, one encoder-decoder stack is shown\\nto illustrate the model [19]. Each stack includes only attention mechanisms\\nand feedforward neural networks. As this architecture does not include any\\nrecurrent or convolutional layer, information about the relative or absolute\\npositions in the input sequence is given at the beginning of both encoder and\\ndecoder using positional encodings.',\n",
       " 'recurrent or convolutional layer, information about the relative or absolute\\npositions in the input sequence is given at the beginning of both encoder and\\ndecoder using positional encodings.\\nThe calculations of self-attention are slightly diﬀerent from the mechanisms\\ndescribed so far in this paper. It uses three vectors namely query, key and\\nvalue for each word. These vectors are computed by multiplying the input with\\nweight matrices Wq, Wk and Wv which are learned during training. In general,\\neach value is weighted by a function of the query with the corresponding key.\\nThe output is computed as a weighted sum of the values. Based on this idea,\\ntwo attention mechanisms are proposed: In the ﬁrst one, called scaled dot-\\nproduct attention, the dot products of the query with all keys are computed\\nas given in the right side of Fig. 3. Each result is divided to the square root of\\nthe dimension of the keys to have more stable gradients. They pass into the',\n",
       " 'as given in the right side of Fig. 3. Each result is divided to the square root of\\nthe dimension of the keys to have more stable gradients. They pass into the\\nsoftmax function, thus the weights for the values are obtained. Finally each\\nsoftmax score is multiplied with the value as given in Eq. (8). The authors\\npropose computing the attention on a set of queries simultaneously by taking\\nqueries and keys of dimension dk, and values of dimension dv as inputs. The\\nkeys, queries and values are packed together into matricesK, Q and V. Finally,\\nthe output matrix is obtained as follows [19]:',\n",
       " 'Attention Mechanism in Neural Networks: 9\\nFig. 3 The Transformer architecture and the attention mechanisms it uses in detail [19].\\n(Left) The Transformer with one encoder-decoder stack. (Center) Multi-head attention.\\n(Right) Scaled dot-product attention.\\nAttention(Q,K,V ) = softmax(QKT\\n√dk\\n)V (8)\\nThis calculation is performed by every word against the other words. This\\nleads to having values of each word relative to each other. For instance, if\\nthe word x2 is not relevant for the word x1, then the softmax score gives low\\nprobability scores. As a result, the corresponding value is decreased. This leads\\nto an increase in the value of relevant words, and those of others decrease. In\\nthe end, every word obtains a new value for itself.\\nAs seen from Fig. 3, the Transformer model does not directly use scaled\\ndot-product attention. But the attention mechanism it uses is based on these\\ncalculations. The second mechanism proposed, called the multi-head attention,',\n",
       " 'dot-product attention. But the attention mechanism it uses is based on these\\ncalculations. The second mechanism proposed, called the multi-head attention,\\nlinearly projects the queries, keys and values h times with diﬀerent, learned\\nlinear projections to dq, dk and dv dimensions, respectively [19]. The attention\\nfunction is performed in parallel on each of these projected versions of queries,\\nkeys and values, i.e., heads. By this way, dv-dimensional output values are\\nobtained. In order to get the ﬁnal values, they are concatenated and projected\\none last time as shown in the center of Fig. 3. By this way, the self-attention is\\ncalculated multiple times using diﬀerent sets of query, key and value vectors.\\nThus, the model can jointly attend to information at diﬀerent positions [19]:\\nMultiHead(Q,K,V ) = Concat(head1,...,head h)WO (9)\\nwherehead i = Attention(QWQ\\ni ,KW K\\ni ,VW V\\ni )\\nIn the decoder part of the Transformer, masked multi-head attention is',\n",
       " 'MultiHead(Q,K,V ) = Concat(head1,...,head h)WO (9)\\nwherehead i = Attention(QWQ\\ni ,KW K\\ni ,VW V\\ni )\\nIn the decoder part of the Transformer, masked multi-head attention is\\napplied ﬁrst to ensure that only previous word embeddings are used when\\ntrying to predict the next word in the sentence. Therefore, the embeddings\\nthat shouldn’t be seen by the decoder are masked by multiplying with zero.',\n",
       " '10 Derya Soydaner\\nAn interesting study examines the contribution made by individual atten-\\ntion heads in the encoder [76]. Also, there is an evaluation of the eﬀects of\\nself-attention on gradient propagation in recurrent networks [77]. For a deeper\\nanalysis of multi-head self-attention mechanism from a theoretical perspective\\nsee [78].\\nSelf-attention has been used successfully in a variety of tasks including\\nsentence embedding [79] and abstractive summarization [80]. It is shown that\\nself-attention can lead to improvements to discriminative constituency parser\\n[81], and speech recognition as well [82,83]. Also, the listen-attend-spell model\\n[84] has been improved with the self-attention for acoustic modeling [85].\\nAs soon as these self-attention mechanisms were proposed, they have been\\nincorporated with deep neural networks for a wide range of tasks. For instance,\\na deep learning model learned a number of large-scale tasks from multiple do-',\n",
       " 'incorporated with deep neural networks for a wide range of tasks. For instance,\\na deep learning model learned a number of large-scale tasks from multiple do-\\nmains with the aid of self-attention mechanism [86]. Novel self-attention neural\\nmodels are proposed for cross-target stance classiﬁcation [87] and NMT [88].\\nAnother study points out that a fully self-attentional model can reach com-\\npetitive predictive performance on ImageNet classiﬁcation and COCO object\\ndetection tasks [89]. Besides, developing novel attention mechanisms has been\\ncarried out such as area attention, a novel mechanism that can be used along\\nmulti-head attention [90]. It attends to areas in the memory by deﬁning the\\nkey of an area as the mean vector of the key of each item, and deﬁning the\\nvalue as the sum of all value vectors in the area.\\nWhen a novel mechanism is proposed, it is inevitable to incorporate it\\ninto the GAN framework [91]. Self-Attention Generative Adversarial Networks',\n",
       " 'value as the sum of all value vectors in the area.\\nWhen a novel mechanism is proposed, it is inevitable to incorporate it\\ninto the GAN framework [91]. Self-Attention Generative Adversarial Networks\\n(SAGANs) [92] introduce a self-attention mechanism into convolutional GANs.\\nDiﬀerent from the traditional convolutional GANs, SAGAN generates high-\\nresolution details using cues from all feature locations. Similarly, Attentional\\nGenerative Adversarial Network (AttnGAN) is presented for text to image\\ngeneration [93]. On the other side, a machine reading and question answering\\narchitecture called QANet [94] is proposed without any recurrent networks. It\\nuses self-attention to learn the global interaction between each pair of words\\nwhereas convolution captures the local structure of the text. In another study,\\nGated Attention Network (GaAN) controls the importance of each attention\\nhead’s output by introducing gates [95]. Another interesting study introduces',\n",
       " 'Gated Attention Network (GaAN) controls the importance of each attention\\nhead’s output by introducing gates [95]. Another interesting study introduces\\nattentive group convolutions with a generalization of visual self-attention [96].\\nA deep transformer model is implemented for language modeling over long\\nsequences [97].\\n5.1 Self-attention variants\\nIn recent years, self-attention has become an important research direction\\nwithin the deep learning community. Self-attention idea has been examined\\nin diﬀerent aspects. For example, self-attention is handled in a multi-instance\\nlearning framework [98]. The idea of Sparse Adaptive Connection (SAC) is\\npresented for accelerating and structuring self-attention [99]. The research on',\n",
       " 'Attention Mechanism in Neural Networks: 11\\nTable 1 Summary of Notation\\nSymbol Deﬁnition\\na annotation\\nc context vector\\nα weight\\ne energy\\nf feedforward neural network\\nh hidden state\\nφ hard (stochastic) / soft (deterministic) attention\\ns location variable\\np source position\\nK, Q, V keys, queries and values matrices, respectively\\nWq, Wk, Wv weight matrices for queries, keys and values, respectively\\nimproving self-attention continues as well [100,101,102]. Besides, based on the\\nself-attention mechanisms proposed in the Transformer, important studies that\\nmodify the self-attention have been presented. Some of the most recent and\\nprominent studies are summarized below.\\nRelation-aware self-attention It extends the self-attention mechanism by\\nregarding representations of the relative positions, or distances between se-\\nquence elements [103]. Thus, it can consider the pairwise relationships between\\ninput elements. This type of attention mechanism deﬁnes vectors to represent',\n",
       " 'quence elements [103]. Thus, it can consider the pairwise relationships between\\ninput elements. This type of attention mechanism deﬁnes vectors to represent\\nthe edge between two inputs. It provides learning two distinct edge represen-\\ntations that can be shared across attention heads without requiring additional\\nlinear transformations.\\nDirectional self-attention (DiSA) A novel neural network architecture\\nfor learning sentence embedding named Directional Self-Attention Network\\n(DiSAN) [104] uses directional self-attention followed by a multi-dimensional\\nattention mechanism. Instead of computing a single importance score for each\\nword based on the word embedding, multi-dimensional attention computes a\\nfeature-wise score vector for each token. To extend this mechanism to the self-\\nattention, two variants are presented: The ﬁrst one, called multi-dimensional\\n‘token2token’ self-attention generates context-aware coding for each element.',\n",
       " 'attention, two variants are presented: The ﬁrst one, called multi-dimensional\\n‘token2token’ self-attention generates context-aware coding for each element.\\nThe second one, called multi-dimensional ‘source2token’ self-attention com-\\npresses the sequence into a vector [104]. On the other side, directional self-\\nattention produces context-aware representations with temporal information\\nencoded by using positional masks. By this way, directional information is en-\\ncoded. First, the input sequence is transformed to a sequence of hidden states\\nby a fully connected layer. Then, multi-dimensional token2token self-attention\\nis applied to these hidden states. Hence, context-aware vector representations\\nare generated for all elements from the input sequence.\\nReinforced self-attention (ReSA)A sentence-encoding model named Re-\\ninforced Self-Attention Network (ReSAN)uses reinforced self-attention (ReSA)\\nthat integrates soft and hard attention mechanisms into a single model. ReSA',\n",
       " '12 Derya Soydaner\\nselects a subset of head tokens, and relates each head token to a small sub-\\nset of dependent tokens to generate their context-aware representations [105].\\nFor this purpose, a novel hard attention mechanism called reinforced sequence\\nsampling (RSS) , which selects tokens from an input sequence in parallel and\\ntrained via policy gradient, is proposed. Given an input sequence, RSS gener-\\nates an equal-length sequence of binary random variables that indicates both\\nthe selected and discarded ones. On the other side, the soft attention provides\\nreward signals back for training the hard attention. The proposed RSS pro-\\nvides a sparse mask to self-attention. ReSA uses two RSS modules to extract\\nthe sparse dependencies between each pair of selected tokens.\\nOuter product attention (OPA)Self-Attentive Associative Memory (SAM)\\nis a novel operator based upon outer product attention (OPA) [106]. This at-\\ntention mechanism is an extension of dot-product attention [19]. OPA diﬀers',\n",
       " 'is a novel operator based upon outer product attention (OPA) [106]. This at-\\ntention mechanism is an extension of dot-product attention [19]. OPA diﬀers\\nusing element-wise multiplication, outer product, and tanh function instead of\\nsoftmax.\\nBidirectional block self-attention (Bi-BloSA)Another mechanism, bidi-\\nrectional block self-attention (Bi-BloSA) which is simply a masked block self-\\nattention (mBloSA) with forward and backward masks to encode the tempo-\\nral order information is presented [107]. Here, mBloSA is composed of three\\nparts from its bottom to top namely intra-block self-attention , inter-block\\nself-attention and the context fusion . It splits a sequence into several length-\\nequal blocks, and applies an intra-block self-attention to each block indepen-\\ndently. Then, inter-block self-attention processes the outputs for all blocks.\\nThis stacked self-attention model results a reduction in the amount of mem-',\n",
       " 'dently. Then, inter-block self-attention processes the outputs for all blocks.\\nThis stacked self-attention model results a reduction in the amount of mem-\\nory compared to a single one applied to the whole sequence. Finally, a feature\\nfusion gate combines the outputs of intra-block and inter-block self-attention\\nwith the original input, to produce the ﬁnal context-aware representations of\\nall tokens.\\nFixed multi-head attentionThe ﬁxed multi-head attention proposes ﬁxing\\nthe head size of the Transformer in the aim of improving the representation\\npower [108]. This study emphasizes its importance by setting the head size of\\nattention units to input sequence length.\\nSparse sinkhorn attentionIt is based on the idea of diﬀerentiable sorting\\nof internal representations within the self-attention module [109]. Instead of\\nallowing tokens to only attend to tokens within the same block, it operates\\non block sorted sequences. Each token attends to tokens in the sorted block.',\n",
       " 'allowing tokens to only attend to tokens within the same block, it operates\\non block sorted sequences. Each token attends to tokens in the sorted block.\\nThus, tokens that may be far apart in the unsorted sequence can be considered.\\nAdditionally, a variant of this mechanism named SortCut sinkhorn attention\\napplies a post-sorting truncation of the input sequence.',\n",
       " 'Attention Mechanism in Neural Networks: 13\\nAdaptive attention spanAdaptive attention span is proposed as an alter-\\nnative to self-attention [110]. It learns the attention span of each head inde-\\npendently. To this end, a masking function inspired by [111] is used to control\\nthe attention span for each head. The purpose of this novel mechanism is to\\nreduce the computational burden of the Transformer. Additionally, dynamic\\nattention span approach is presented to dynamically change the attention span\\nbased on the current input as an extension [51,112].\\n5.2 Transformer variants\\nDiﬀerent from developing novel self-attention mechanisms, several studies have\\nbeen published in the aim of improving the performance of the Transformer.\\nThese studies mostly modify the model architecture. For instance, an addi-\\ntional recurrence encoder is preferred to model recurrence for Transformer di-\\nrectly [113]. In another study, a new weight initialization scheme is applied to',\n",
       " 'tional recurrence encoder is preferred to model recurrence for Transformer di-\\nrectly [113]. In another study, a new weight initialization scheme is applied to\\nimprove Transformer optimization [114]. A novel positional encoding scheme\\nis used to extend the Transformer to tree-structured data [115]. Investigating\\nmodel size by handling Transformer width and depth for eﬃcient training is\\nalso an active research area [116]. Transformer is used in reinforcement learn-\\ning settings [117,118,119] and for time series forecasting in adversarial training\\nsetting [120].\\nBesides, many Transformer variants have been presented in the recent past.\\nCOMmonsEnse Transformer (COMET) is introduced for automatic construc-\\ntion of commonsense knowledge bases [121]. Evolved Transformer applies neu-\\nral architecture search for a better Transformer model [122]. Transformer Au-\\ntoencoder is a sequential autoencoder for conditional music generation [123].',\n",
       " 'ral architecture search for a better Transformer model [122]. Transformer Au-\\ntoencoder is a sequential autoencoder for conditional music generation [123].\\nCrossTransformer takes a small number of labeled images and an unlabeled\\nquery, and computes distances between spatially-corresponding features to in-\\nfer class membership [124]. DEtection TRansformer (DETR) is a new design\\nfor object detection systems [125], and Deformable DETR is an improved ver-\\nsion that achieves better performance in less time [126]. FLOw-bAsed Trans-\\nformER (FLOATER) emphasizes the importance of position encoding in the\\nTransformer, and models the position information via a continuous dynamical\\nmodel [127]. Disentangled Context (DisCo) Transformer simultaneously gener-\\nates all tokens given diﬀerent contexts by predicting every word in a sentence\\nconditioned on an arbitrary subset of the rest of the words [128]. Genera-\\ntive Adversarial Transformer (GANsformer) is presented for visual generative',\n",
       " 'conditioned on an arbitrary subset of the rest of the words [128]. Genera-\\ntive Adversarial Transformer (GANsformer) is presented for visual generative\\nmodeling [129].\\nRecent work has demonstrated signiﬁcant performance on NLP tasks. In\\nOpenAI GPT, there is a left-to-right architecture, where every token can only\\nattend to previous tokens in the self-attention layers of the Transformer [130].\\nGPT-2 [131] and GPT-3 [18] models have improved the progress. In addition\\nto these variants, some prominent Transformer-based models are summarized\\nbelow.',\n",
       " '14 Derya Soydaner\\nUniversal Transformer A generalization of the Transformer model named\\nthe Universal Transformer [132] iteratively computes representations Ht at\\nstep t for all positions in the sequence in parallel. To this end, it uses the\\nscaled dot-product attention in Eq. (8) where d is the number of columns\\nof Q, K and V. In the Universal Transformer, the multi-head self-attention\\nwith k heads is used. The representations Ht is mapped to queries, keys and\\nvalues with aﬃne projections using learned parameter matrices WQ ∈ℜd×d/k,\\nWK ∈ℜd×d/k, WV ∈ℜd×d/k and WO ∈ℜd×d [132]:\\nMultiHead(Ht) = Concat(head1,...,head k)WO (10)\\nwherehead i = Attention(HtWQ\\ni ,HtWK\\ni ,HtWV\\ni )\\nImage TransformerImage Transformer [133] demonstrates that self-attention\\nbased models can also be well-suited for images instead of text. This Trans-\\nformer type restricts the self-attention mechanism to attend to local neigh-\\nborhoods. Thus, the size of images that the model can process is increased.',\n",
       " 'former type restricts the self-attention mechanism to attend to local neigh-\\nborhoods. Thus, the size of images that the model can process is increased.\\nIts larger receptive ﬁelds allow the Image Transformer to signiﬁcantly improve\\nthe model performance on image generation as well as image super-resolution.\\nTransformer-XL This study aims to improve the ﬁxed-length context of the\\nTransformer [19] for language modeling. Transformer-XL [134] makes model-\\ning very long-term dependency possible by reusing the hidden states obtained\\nin previous segments. Hence, information can be propagated through the recur-\\nrent connections. In order to reuse the hidden states without causing temporal\\nconfusion, Transformer-XL uses relative positional encodings. Based on this\\narchitecture, a modiﬁed version named the Gated Transformer-XL (GTrXL)\\nis presented in the reinforcement learning setting [135].\\nTensorized TransformerTensorized Transformer[136] compresses the multi-',\n",
       " 'is presented in the reinforcement learning setting [135].\\nTensorized TransformerTensorized Transformer[136] compresses the multi-\\nhead attention in Transformer. To this end, it uses a novel self-attention model\\nmulti-linear attention with Block-Term Tensor Decomposition (BTD) [137]. It\\nbuilds a single-block attention based on the Tucker decomposition [138]. Then,\\nit uses a multi-linear attention constructed by a BTD to compress the multi-\\nhead attention mechanism. In Tensorized Transformer, the factor matrices are\\nshared across multiple blocks.\\nBERT The Bidirectional Encoder Representations fromTransformers (BERT)\\naims to pre-train deep bidirectional representations from unlabeled text [139].\\nBERT uses a multilayer bidirectional Transformer as the encoder. Besides,\\ninspired by the Cloze task [140], it has a masked language model pre-training\\nobjective. BERT randomly masks some of the tokens from the input, and pre-',\n",
       " 'inspired by the Cloze task [140], it has a masked language model pre-training\\nobjective. BERT randomly masks some of the tokens from the input, and pre-\\ndicts the original vocabulary id of the masked word based only on its context.\\nThis model can pre-train a deep bidirectional Transformer. In all layers, the',\n",
       " 'Attention Mechanism in Neural Networks: 15\\npre-training is carried out by jointly conditioning on both left and right con-\\ntext. BERT diﬀers from the left-to-right language model pre-training from this\\naspect.\\nRecently, BERT model has been examined in detail. For instance, the be-\\nhaviour of attention heads are analysed [141]. Various methods have been\\ninvestigated for compressing [142,143], pruning [144], and quantization [145].\\nAlso, BERT model has been considered for diﬀerent tasks such as coreference\\nresolution [146]. A novel method is proposed in order to accelerate BERT\\ntraining [147].\\nFurthermore, various BERT variants have been presented. ALBERT aims\\nto increase the training speed of BERT, and presents two parameter reduction\\ntechniques [148]. Similarly, PoWER-BERT [149] is developed to improve the\\ninference time of BERT. This scheme is also used to accelerate ALBERT. Also,\\nTinyBERT is proposed to accelerate inference and reduce model size while',\n",
       " 'inference time of BERT. This scheme is also used to accelerate ALBERT. Also,\\nTinyBERT is proposed to accelerate inference and reduce model size while\\nmaintaining accuracy [150]. In order to obtain better representations, Span-\\nBERT is proposed as a pre-training method [151]. As a robustly optimized\\nBERT approach, RoBERTa shows that BERT was signiﬁcantly undertrained\\n[152]. Also, DeBERTa improves RoBERTa using the disentangled attention\\nmechanism [153]. On the other side, DistilBERT shows that it is possible to\\nreach similar performances using much smaller language models pre-trained\\nwith knowledge distillation [154]. StructBERT proposes two novel lineariza-\\ntion strategies [155]. Q-BERT is introduced for quantizing BERT models [156],\\nBioBERT is for biomedical text mining [157], and RareBERT is for rare dis-\\nease diagnosis [158].\\nSince 2017 when the Transformer was presented, research directions have\\ngenerally focused on novel self-attention mechanisms, adapting the Trans-',\n",
       " 'ease diagnosis [158].\\nSince 2017 when the Transformer was presented, research directions have\\ngenerally focused on novel self-attention mechanisms, adapting the Trans-\\nformer for various tasks, or making them more understandable. In one of the\\nmost recent studies, NLP becomes possible in the mobile setting with Lite\\nTransformer. It applies long-short range attention where some heads specialize\\nin the local context modeling while the others specialize in the long-distance\\nrelationship modeling [159]. A deep and light-weight Transformer DeLighT\\n[160] and a hypernetwork-based model namely HyperGrid Transformers [161]\\nperform with fewer parameters. Graph Transformer Network is introduced\\nfor learning node representations on heterogeneous graphs [162] and diﬀerent\\napplications are performed for molecular data [163] or textual graph represen-\\ntation [164]. Also, Transformer-XH applies eXtra Hop attention for structured',\n",
       " 'applications are performed for molecular data [163] or textual graph represen-\\ntation [164]. Also, Transformer-XH applies eXtra Hop attention for structured\\ntext data [165]. AttentionXML is a tree-based model for extreme multi-label\\ntext classiﬁcation [166]. Besides, attention mechanism is handled in a Bayesian\\nframework [167]. For a better understanding of Transformers, an identiﬁabil-\\nity analysis of self-attention weights is conducted in addition to presenting\\neﬀective attention to improve explanatory interpretations [168]. Lastly,Vision\\nTransformer (ViT) processes an image using a standard Transformer encoder\\nas used in NLP by interpreting it as a sequence of patches, and performs well\\non image classiﬁcation tasks [169].',\n",
       " '16 Derya Soydaner\\n5.3 What about complexity?\\nAll these aforementioned studies undoubtedly demonstrate signiﬁcant success.\\nBut success not make one great. The Transformer also brings a very high\\ncomputational complexity and memory cost. The necessity of storing atten-\\ntion matrix to compute the gradients with respect to queries, keys and val-\\nues causes a non-negligible quadratic computation and memory requirements.\\nTraining the Transformer is a slow process for very long sequences because\\nof its quadratic complexity. There is also time complexity which is quadratic\\nwith respect to the sequence length. In order to improve the Transformer in\\nthis respect, recent studies have been conducted to improve this issue. One\\nof them is Linear Transformer which expresses the self-attention as a linear\\ndot-product of kernel feature maps [170]. Linear Transformer reduces both\\nmemory and time complexity by changing the self-attention from the softmax',\n",
       " 'dot-product of kernel feature maps [170]. Linear Transformer reduces both\\nmemory and time complexity by changing the self-attention from the softmax\\nfunction in Eq. (8) to a feature map based dot-product attention. Its per-\\nformance is competitive with the vanilla Transformer architecture on image\\ngeneration and automatic speech recognition tasks while being faster during\\ninference. On the other side, FMMformers which use the idea of thefast multi-\\npole method (FMM) [171] outperform the linear Transformer by decomposing\\nthe attention matrix into near-ﬁeld and far-ﬁeld attention with linear time and\\nmemory complexity [172].\\nAnother suggestion made in response to the Transformer’s quadratic na-\\nture is The Reformer that replaces dot-product attention by one that uses\\nlocality-sensitive hashing [173]. It reduces the complexity but one limitation\\nof the Reformer is its requirement for the queries and keys to be identical. Set',\n",
       " 'locality-sensitive hashing [173]. It reduces the complexity but one limitation\\nof the Reformer is its requirement for the queries and keys to be identical. Set\\nTransformer aims to reduce computation time of self-attention from quadratic\\nto linear by using an attention mechanism based on sparse Gaussian process\\nliterature [174]. Routing Transformer aims to reduce the overall complexity\\nof attention by learning dynamic sparse attention patterns by using routing\\nattention with clustering [175]. It applies k-means clustering to model sparse\\nattention matrices. At ﬁrst, queries and keys are assigned to clusters. The at-\\ntention scheme is determined by considering only queries and keys from the\\nsame cluster. Thus, queries are routed to keys belonging to the same cluster\\n[175].\\nSparse Transformer introduces sparse factorizations of the attention ma-\\ntrix by using factorized self-attention, and avoids the quadratic growth of com-',\n",
       " '[175].\\nSparse Transformer introduces sparse factorizations of the attention ma-\\ntrix by using factorized self-attention, and avoids the quadratic growth of com-\\nputational burden [176]. It also shows the possibility of modeling sequences\\nof length one million or more by using self-attention in theory. In the Trans-\\nformer, all the attention heads with the softmax attention assign a non-zero\\nweight to all context words. Adaptively Sparse Transformer replaces softmax\\nwith α-entmax which is a diﬀerentiable generalization of softmax allowing\\nlow-scoring words to receive precisely zero weight [177]. By means of context-\\ndependent sparsity patterns, the attention heads become ﬂexible in the Adap-\\ntively Sparse Transformer. Random feature attention approximates softmax\\nattention with random feature methods [178]. Skyformer replaces softmax\\nwith a Gaussian kernel and adapts Nystr¨ om method [179]. A sparse atten-',\n",
       " 'Attention Mechanism in Neural Networks: 17\\ntion mechanism named BIGBIRD aims to reduce the quadratic dependency\\nof Transformer-based models to linear [180]. Diﬀerent from the similar stud-\\nies, BIGBIRD performs well for genomics data alongside NLP tasks such as\\nquestion answering.\\nMusic Transformer [181] shows that self-attention can also be useful for\\nmodeling music. This study emphasizes the infeasibility of the relative po-\\nsition representations introduced by [103] for long sequences because of the\\nquadratic intermediate relative information in the sequence length. Therefore,\\nthis study presents an extended version of relative attention named relative\\nlocal attention that improves the relative attention for longer musical com-\\npositions by reducing its intermediate memory requirement to linear in the\\nsequence length. A softmax-free Transformer (SOFT ) is presented to improve\\nthe computational eﬃciency of ViT. It uses Gaussian kernel function instead',\n",
       " 'sequence length. A softmax-free Transformer (SOFT ) is presented to improve\\nthe computational eﬃciency of ViT. It uses Gaussian kernel function instead\\nof the dot-product similarity [182].\\nAdditionally, various approaches have been presented in Hierarchical Vi-\\nsual Transformer [183], Long-Short Transformer (Transformer-LS)[184], Per-\\nceiver [185], and Performer [186]. Image Transformer based on the cross-\\ncovariance matrix between keys and queries is applied [187], and a new vi-\\nsion Transformer is proposed [188]. Furthermore, a Bernoulli sampling atten-\\ntion mechanism decreases the quadratic complexity to linear [189]. A novel\\nlinearized attention mechanism performs well on object detection, instance\\nsegmentation, and stereo depth estimation [190]. A study shows that kernel-\\nized attention with relative positional encoding can be calculated using Fast\\nFourier Transform and it leads to get rid of the quadratic complexity for long',\n",
       " 'ized attention with relative positional encoding can be calculated using Fast\\nFourier Transform and it leads to get rid of the quadratic complexity for long\\nsequences [191]. A linear uniﬁed nested attention mechanism namely Luna\\nuses two nested attention functions to approximate the softmax attention in\\nTransformer to achieve linear time and space complexity [192].\\n6 Concluding Remarks: A New Hope\\nInspired by the human visual system, the attention mechanisms in neural net-\\nworks have been developing for a long time. In this study, we examine this\\nduration beginning with its roots up to the present time. Some mechanisms\\nhave been modiﬁed, or novel mechanisms have emerged in this period. Today,\\nthis journey has reached a very important stage. The idea of incorporating\\nattention mechanisms into deep neural networks has led to state-of-the-art re-\\nsults for a large variety of tasks. Self-attention mechanisms and GPT-n family',\n",
       " 'attention mechanisms into deep neural networks has led to state-of-the-art re-\\nsults for a large variety of tasks. Self-attention mechanisms and GPT-n family\\nmodels have become a new hope for more advanced models. These promising\\nprogress bring the questions whether the attention could help further devel-\\nopment, replace the popular neural network layers, or could be a better idea\\nthan the existing attention mechanisms? It is still an active research area and\\nmuch to learn we still have, but it is obvious that more powerful systems are\\nawaiting when neural networks and attention mechanisms join forces.',\n",
       " '18 Derya Soydaner\\nConﬂict of interest\\nThe author declares that she has no conﬂict of interest.\\nReferences\\n1. I. Goodfellow, Y. Bengio, A. Courville, The MIT Press (2016)\\n2. D. Noton, L. Stark, Scientiﬁc American 224(6), 34 (1971)\\n3. D. Noton, L. Stark, Vision Research 11, 929 (1971)\\n4. E. Alpaydın, Advances in Neural Information Processing Systems 8 pp. 771–777 (1995)\\n5. S. Ahmad, Advances in Neural Information Processing Systems 4 pp. 420–427 (1991)\\n6. M. Posner, S. Petersen, Annual Review of Neuroscience 13(1), 25 (1990)\\n7. C. Bundesen, Psychological Review 97(4), 523 (1990)\\n8. R. Desimone, J. Duncan, Annual Review of Neuroscience 18(1), 193 (1995)\\n9. M. Corbetta, G. Shulman, Nature Reviews Neuroscience 3(3), 201 (2002)\\n10. S. Petersen, M. Posner, Annual Review of Neuroscience 35, 73 (2012)\\n11. R. Rimey, C. Brown, Technical Report, University of Rochester (1990)\\n12. B. Sheliga, L. Riggio, G. Rizzolatti, Experimental Brain Research 98(3), 507 (1994)',\n",
       " '11. R. Rimey, C. Brown, Technical Report, University of Rochester (1990)\\n12. B. Sheliga, L. Riggio, G. Rizzolatti, Experimental Brain Research 98(3), 507 (1994)\\n13. B. Sheliga, L. Riggio, G. Rizzolatti, Experimental Brain Research 105(2), 261 (1995)\\n14. J. Hoﬀman, B. Subramaniam, Perception and Psychophysics 57(6), 787 (1995)\\n15. S. Chaudhari, et al., ACM Transactions on Intelligent Systems and Technology (TIST)\\npp. 1–32 (2021)\\n16. A. Galassi, et al., IEEE Transactions on Neural Networks and Learning Systems (2020)\\n17. J. Lee, et al., ACM Transactions on Knowledge Discovery from Data (TKDD) 13(6),\\n1 (2019)\\n18. T. Brown, et al., Advances in Neural Information Processing Systems 33 pp. 1877–1901\\n(2020)\\n19. A. Vaswani, et al., Advances in Neural Information Processing Systems 30 pp. 5998–\\n6008 (2017)\\n20. K. Fukushima, Biological Cybernetics 36, 193 (1980)\\n21. K. Fukushima, Applied Optics 26(23), 4985 (1987)\\n22. K. Fukushima, T. Imagawa, Neural Networks 6(1), 33 (1993)',\n",
       " '6008 (2017)\\n20. K. Fukushima, Biological Cybernetics 36, 193 (1980)\\n21. K. Fukushima, Applied Optics 26(23), 4985 (1987)\\n22. K. Fukushima, T. Imagawa, Neural Networks 6(1), 33 (1993)\\n23. E. Postma, H.V. den Herik, P. Hudson, Neural Networks 10(6), 993 (1997)\\n24. J. Schmidhuber, R. Huber, International Journal of Neural Systems pp. 125–134 (1991)\\n25. R. Milanese, et al., IEEE Computer Society Conference on Computer Vision and Pat-\\ntern Recoginition, Seattle, WA, USA pp. 781–785 (1994)\\n26. J. Tsotsos, et al., Artiﬁcial Intelligence 78(1-2), 507 (1995)\\n27. S. Culhane, J. Tsotsos, Proceedings of the 11th IAPR International Conference on\\nPattern Recognition, The Hague, Netherlands pp. 36–40 (1992)\\n28. D. Reisfeld, H. Wolfson, Y. Yeshurun, International Journal of Computer Vision14(2),\\n119 (1995)\\n29. I. Rybak, et al., Vision Research 38(15-16), 2387 (1998)\\n30. J. Keller, et al., Pattern Analysis and Applications 2(3) (1999)',\n",
       " '119 (1995)\\n29. I. Rybak, et al., Vision Research 38(15-16), 2387 (1998)\\n30. J. Keller, et al., Pattern Analysis and Applications 2(3) (1999)\\n31. F. Miau, L. Itti, Proceedings of the 23rd Annual International Conference of the IEEE\\nEngineering in Medicine and Biology Society, Istanbul, Turkey pp. 789–792 (2001)\\n32. W. Zhang, et al., Advances in Neural Information Processing Systems 19 pp. 1609–1616\\n(2006)\\n33. A. Salah, E. Alpaydın, L. Akarun, IEEE Transactions on Pattern Analysis and Machine\\nIntelligence 24(3), 420 (2002)\\n34. D. Walther, et al., International Workshop on Biologically Motivated Computer Vision,\\nSpringer, Berlin, Heidelberg pp. 472–479 (2002)\\n35. K. Schill, et al., Journal of Electronic Imaging 10(1), 152 (2001)\\n36. L. Paletta, G. Fritz, C. Seifert, International Conference on Machine Learning (2005)\\n37. O.L. Meur, et al., IEEE Transactions on Pattern Analysis and Machine Intelligence\\n28(5), 802– (2006)',\n",
       " 'Attention Mechanism in Neural Networks: 19\\n38. S. Gould, et al., International Joint Conference on Artiﬁcial Intelligence (IJCAI) pp.\\n2115–2121 (2007)\\n39. H. Larochelle, G. Hinton, Advances in Neural Information Processing Systems 23 pp.\\n1243–1251 (2010)\\n40. L. Bazzani, et al., International Conference on Machine Learning (2011)\\n41. V. Mnih, et al., Advances in Neural Information Processing Systems 27 pp. 2204–2212\\n(2014)\\n42. M. Stollenga, et al., Advances in Neural Information Processing Systems 27 pp. 3545–\\n3553 (2014)\\n43. Y. Tang, N. Srivastava, R. Salakhutdinov, Advances in Neural Information Processing\\nSystems 27 (2014)\\n44. D. Bahdanau, K. Cho, Y. Bengio, International Conference on Learning Representa-\\ntions (2015)\\n45. I. Sutskever, O. Vinyals, Q. Le, Advances in Neural Information Processing Systems\\n27 pp. 3104–3112 (2014)\\n46. K. Cho, et al., Proceedings of the 2014 Conference on Empirical Methods in Natural\\nLanguage Processing (EMNLP) pp. 1724–1734 (2014)',\n",
       " '27 pp. 3104–3112 (2014)\\n46. K. Cho, et al., Proceedings of the 2014 Conference on Empirical Methods in Natural\\nLanguage Processing (EMNLP) pp. 1724–1734 (2014)\\n47. M. Schuster, K. Paliwal, IEEE Transactions on Signal Processing 45(11), 2673 (1997)\\n48. K. Xu, et al., International Conference on Machine Learning pp. 2048–2057 (2015)\\n49. O. Vinyals, et al., In Proceedings of the IEEE Conference on Computer Vision and\\nPattern Recognition pp. 3156–3164 (2015)\\n50. R. Williams, Machine Learning 8(3-4), 229 (1992)\\n51. M.T. Luong, H.P..C. Manning, Proceedings of the 2015 Conference on Empirical Meth-\\nods in Natural Language Processing, Lisbon, Portugal pp. 1412–1421 (2015)\\n52. J. Lu, et al., Advances in Neural Information Processing Systems 29 (2016)\\n53. J. Weston, S. Chopra, A. Bordes, International Conference on Learning Representa-\\ntions (2014)\\n54. A. Graves, G. Wayne, I. Danihelka, arXiv preprint arXiv:1410.5401 (2014)',\n",
       " '53. J. Weston, S. Chopra, A. Bordes, International Conference on Learning Representa-\\ntions (2014)\\n54. A. Graves, G. Wayne, I. Danihelka, arXiv preprint arXiv:1410.5401 (2014)\\n55. S. Sukhbaatar, et al., Advances in Neural Information Processing Systems 28 pp. 2440–\\n2448 (2015)\\n56. J. Cheng, L. Dong, M. Lapata, Proceedings of the 2016 Conference on Empirical\\nMethods in Natural Language Processing pp. 551–561 (2016)\\n57. A. Parikh, et al., Proceedings of the 2016 Conference on Empirical Methods in Natural\\nLanguage Processing, Austin, Texas pp. 2249–2255 (2016)\\n58. Q. You, et al., In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition (CVPR), Las Vegas, NV pp. 4651–4659 (2016)\\n59. A. Rush, S. Chopra, J. Weston, Proceedings of the 2015 Conference on Empirical\\nMethods in Natural Language Processing, Lisbon, Portugal pp. 379–389 (2015)\\n60. D. Yu, et al., Interspeech pp. 17–21 (2016)',\n",
       " 'Methods in Natural Language Processing, Lisbon, Portugal pp. 379–389 (2015)\\n60. D. Yu, et al., Interspeech pp. 17–21 (2016)\\n61. J. Chorowski, et al., Advances in Neural Information Processing Systems 28 pp. 577–\\n585 (2015)\\n62. M. Zanﬁr, E. Marinoiu, C. Sminchisescu, In Asian Conference on Computer Vision,\\nSpringer, Cham pp. 104—-119 (2016)\\n63. Y. Cheng, et al., Proceedings of the 25th International Joint Conference on Artiﬁcial\\nIntelligence (2016)\\n64. T. Rockt International Conference on Learning Representations (2016)\\n65. Y. Zhu, et al., Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition pp. 4995–5004 (2016)\\n66. K. Chen, et al., arXiv preprint arXiv:1511.05960 (2015)\\n67. H. Xu, K. Saenko, In European Conference on Computer Vision pp. 451–466 (2016)\\n68. W. Yin, et al., Transactions of the Association for Computational Linguistics 4, 259\\n(2016)\\n69. S. Sharma, R. Kiros, R. Salakhutdinov, International Conference on Learning Repre-\\nsentations (2016)',\n",
       " '(2016)\\n69. S. Sharma, R. Kiros, R. Salakhutdinov, International Conference on Learning Repre-\\nsentations (2016)\\n70. Z. Yang, et al., In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition pp. 21–29 (2016)\\n71. I. Sorokin, et al., arXiv preprint arXiv:1512.01693 (2015)\\n72. J. Ba, et al., Advances in Neural Information Processing Systems 28 pp. 2593–2601\\n(2015)',\n",
       " '20 Derya Soydaner\\n73. K. Gregor, et al., International Conference on Machine Learning pp. 1462–1471 (2015)\\n74. E. Mansimov, et al., International Conference on Learning Representations (2016)\\n75. S. Reed, et al., Advances in Neural Information Processing Systems 29 pp. 217–225\\n(2016)\\n76. E. Voita, et al., In Proceedings of the 57th Annual Meeting of the Association for\\nComputational Linguistics, Florence, Italy pp. 5797–5808 (2019)\\n77. G. Kerg, et al., Advances in Neural Information Processing Systems 33 (2020)\\n78. J.B. Cordonnier, A. Loukas, M. Jaggi, International Conference on Learning Repre-\\nsentations (2020)\\n79. Z. Lin, et al., International Conference on Learning Representations (2017)\\n80. R. Paulus, C. Xiong, R. Socher, International Conference on Learning Representations\\n(2018)\\n81. N. Kitaev, D. Klein, In Proceedings of the 56th Annual Meeting of the Association for\\nComputational Linguistics (Long papers) pp. 2676–2686 (2018)',\n",
       " '(2018)\\n81. N. Kitaev, D. Klein, In Proceedings of the 56th Annual Meeting of the Association for\\nComputational Linguistics (Long papers) pp. 2676–2686 (2018)\\n82. D. Povey, et al., IEEE International Conference on Acoustics, Speech and Signal Pro-\\ncessing (ICASSP), IEEE pp. 5874–5878 (2018)\\n83. A. Vyas, et al., Advances in Neural Information Processing Systems 33 (2020)\\n84. W. Chan, et al., IEEE International Conference on Acoustics, Speech and Signal Pro-\\ncessing (ICASSP), Shanghai pp. 4960—-4964 (2016)\\n85. M. Sperber, et al., In proceedings of Annual Conference of the International Speech\\nCommunication Association (InterSpeech) pp. 3723–3727 (2018)\\n86. L. Kaiser, et al., arXiv preprint arXiv:1706.05137 (2017)\\n87. C. Xu, et al., Proceedings of the 56th Annual Meeting of the Association for Compu-\\ntational Linguistics (Short papers), Melbourne, Australia pp. 778–783 (2018)\\n88. S. Maruf, A. Martins, G. Haﬀari, Proceedings of NAACL-HLT, Minneapolis, Minnesota\\npp. 3092–3102 (2019)',\n",
       " 'tational Linguistics (Short papers), Melbourne, Australia pp. 778–783 (2018)\\n88. S. Maruf, A. Martins, G. Haﬀari, Proceedings of NAACL-HLT, Minneapolis, Minnesota\\npp. 3092–3102 (2019)\\n89. P. Ramachandran, et al., Advances in Neural Information Processing Systems 32 pp.\\n68–80 (2019)\\n90. Y. Li, et al., International Conference on Machine Learning (2019)\\n91. I. Goodfellow, et al., Advances in Neural Information Processing Systems 27 pp. 2672–\\n2680 (2014)\\n92. H. Zhang, et al., International Conference on Machine Learning pp. 7354–7363 (2019)\\n93. T. Xu, et al., In Proceedings of the IEEE Conference on Computer Vision and Pattern\\nRecognition (CVPR) pp. 1316–1324 (2018)\\n94. A. Yu, et al., International Conference on Learning Representations (2018)\\n95. J. Zhang, et al., Conference on Uncertainty in Artiﬁcial Intelligence (2018)\\n96. D. Romero, et al., International Conference on Machine Learning (2020)\\n97. R. Al-Rfou, et al., AAAI Conference on Artiﬁcial Intelligence 33, 3159 (2019)',\n",
       " '96. D. Romero, et al., International Conference on Machine Learning (2020)\\n97. R. Al-Rfou, et al., AAAI Conference on Artiﬁcial Intelligence 33, 3159 (2019)\\n98. J. Du, et al., Proceedings of the 2018 Conference on Empirical Methods in Natural\\nLanguage Processing pp. 2216–2225 (2018)\\n99. X. Li, et al., Advances in Neural Information Processing Systems 33 (2020)\\n100. B. Yang, et al., AAAI Conference on Artiﬁcial Intelligence 33, 387 (2019)\\n101. B. Yang, et al., Proceedings of the 2018 Conference on Empirical Methods in Natural\\nLanguage Processing, Brussels, Belgium pp. 4449–4458 (2018)\\n102. Proceedings of the IEEE International Conference on Computer Vision pp. 3286–3295\\n103. P. Shaw, J. Uszkoreit, A. Vaswani, Proceedings of NAACL-HLT, New Orleans,\\nLouisiana pp. 464–468 (2018)\\n104. T. Shen, et al., AAAI Conference on Artiﬁcial Intelligence pp. 5446–5455 (2018)\\n105. T. Shen, et al., In Proceedings of the 27th International Joint Conference on Artiﬁcial',\n",
       " '104. T. Shen, et al., AAAI Conference on Artiﬁcial Intelligence pp. 5446–5455 (2018)\\n105. T. Shen, et al., In Proceedings of the 27th International Joint Conference on Artiﬁcial\\nIntelligence, (IJCAI-18) pp. 4345–4352 (2018)\\n106. H. Le, T. Tran, S. Venkatesh, International Conference on Machine Learning (2020)\\n107. T. Shen, et al., International Conference on Learning Representations (2018)\\n108. S. Bhojanapalli, et al., International Conference on Machine Learning (2020)\\n109. Y. Tay, et al., International Conference on Machine Learning (2020)\\n110. S. Sukhbaatar, et al., Proceedings of the 57th Annual Meeting of the Association for\\nComputational Linguistics, Florence, Italy pp. 331–335 (2019)\\n111. Y. Jernite, et al., International Conference on Learning Representations (2017)\\n112. R. Shu, H. Nakayama, In Proceedings of the First Workshop on Neural Machine Trans-\\nlation, Vancouver, Canada pp. 1–10 (2017)',\n",
       " 'Attention Mechanism in Neural Networks: 21\\n113. J. Hao, et al., Proceedings of NAACL-HLT, Minneapolis, Minnesota pp. 1198–1207\\n(2019)\\n114. X. Huang, et al., International Conference on Machine Learning (2020)\\n115. V. Shiv, C. Quirk, Advances in Neural Information Processing Systems 32 pp. 12,081–\\n12,091 (2019)\\n116. Z. Li, et al., International Conference on Machine Learning (2020)\\n117. Y. Hoshen, Advances in Neural Information Processing Systems 30, Long Beach, CA,\\nUSA (2017)\\n118. S. Hu, et al., International Conference on Learning Representations (2021)\\n119. E. Parisotto, R. Salakhutdinov, International Conference on Learning Representations\\n(2021)\\n120. S. Wu, et al., Advances in Neural Information Processing Systems 33 (2020)\\n121. A. Bosselut, et al., Proceedings of the 57th Annual Meeting of the Association for\\nComputational Linguistics (2019)\\n122. D. So, C. Liang, Q. Le, International Conference on Machine Learning (2019)',\n",
       " 'Computational Linguistics (2019)\\n122. D. So, C. Liang, Q. Le, International Conference on Machine Learning (2019)\\n123. K. Choi, et al., International Conference on Machine Learning (2020)\\n124. C. Doersch, A. Gupta, A. Zisserman, Advances in Neural Information Processing Sys-\\ntems 33 pp. 21,981–21,993 (2020)\\n125. N. Carion, et al., European Conference on Computer Vision pp. 213—-229 (2020)\\n126. X. Zhu, et al., International Conference on Learning Representations (2021)\\n127. X. Liu, et al., International Conference on Machine Learning pp. 6327–6335 (2020)\\n128. J. Kasai, et al., International Conference on Machine Learning (2020)\\n129. D. Hudson, L. Zitnick, International Conference on Machine Learning pp. 4487–4499\\n(2021)\\n130. A. Radford, et al., Technical Report, OpenAI (2018)\\n131. A. Radford, et al., OpenAI blog p. 9 (2019)\\n132. M. Dehghani, et al., International Conference on Learning Representations (2019)\\n133. N. Parmar, International Conference on Machine Learning (2018)',\n",
       " '132. M. Dehghani, et al., International Conference on Learning Representations (2019)\\n133. N. Parmar, International Conference on Machine Learning (2018)\\n134. Z. Dai, et al., Proceedings of the 57th Annual Meeting of the Association for Compu-\\ntational Linguistics pp. 2978–2988 (2019)\\n135. E. Parisotto, International Conference on Machine Learning (2020)\\n136. X. Ma, et al., Advances in Neural Information Processing Systems 32 pp. 2232–2242\\n(2019)\\n137. L. Lathauwer, SIAM Journal on Matrix Analysis and Applications 30(3), 1033 (2008)\\n138. L. Tucker, Psychometrika 31(3), 279 (1966)\\n139. J. Devlin, et al., Proceedings of NAACL-HLT 2019 pp. 4171–4186 (2019)\\n140. W. Taylor, Journalism Bulletin 30(4), 415 (1953)\\n141. K. Clark, et al., arXiv preprint arXiv:1906.04341 (2019)\\n142. S. Sun, et al., Proceedings of the 2019 Conference on Empirical Methods in Natural\\nLanguage Processing and the 9th International Joint Conference on Natural Language\\nProcessing, Hong Kong, China pp. 4323–4332 (2019)',\n",
       " 'Language Processing and the 9th International Joint Conference on Natural Language\\nProcessing, Hong Kong, China pp. 4323–4332 (2019)\\n143. W. Wang, et al., Advances in Neural Information Processing Systems 33 (2020)\\n144. J. McCarley, R. Chakravarti, A. Sil, arXiv preprint arXiv:1910.06360 (2020)\\n145. O. Zafrir, et al., The 5th Workshop on Energy Eﬃcient Machine Learning and Cogni-\\ntive Computing - NeurIPS (2019)\\n146. M. Joshi, et al., In Proceedings of the 2019 Conference on Empirical Methods in\\nNatural Language Processing and the 9th International Joint Conference on Natural\\nLanguage Processing pp. 5803–5808 (2019)\\n147. L. Gong, et al., International Conference on Machine Learning pp. 2337–2346 (2019)\\n148. Z. Lan, et al., International Conference on Learning Representations (2020)\\n149. S. Goyal, et al., International Conference on Machine Learning (2020)\\n150. X. Jiao, et al., arXiv preprint arXiv:1909.10351 (2019)',\n",
       " '149. S. Goyal, et al., International Conference on Machine Learning (2020)\\n150. X. Jiao, et al., arXiv preprint arXiv:1909.10351 (2019)\\n151. M. Joshi, et al., Transactions of the Association for Computational Linguistics 8, 64\\n(2020)\\n152. Y. Liu, et al., arXiv preprint arXiv:1907.11692 (2019)\\n153. P. He, et al., International Conference on Learning Representations (2021)\\n154. V. Sanh, et al., the 5th Workshop on Energy Eﬃcient Machine Learning and Cognitive\\nComputing - NeurIPS (2019)',\n",
       " '22 Derya Soydaner\\n155. W. Wang, et al., International Conference on Learning Representations (2020)\\n156. S. Shen, et al., AAAI Conference on Artiﬁcial Intelligence 34, 8815 (2020)\\n157. J. Lee, et al., Bioinformatics 36(4), 1234 (2020)\\n158. P. Prakash, et al., AAAI Conference on Artiﬁcial Intelligence 35, 453 (2021)\\n159. Z. Wu, et al., International Conference on Learning Representations (2020)\\n160. S. Mehta, et al., International Conference on Learning Representations (2021)\\n161. Y. Tay, et al., International Conference on Learning Representations (2021)\\n162. S. Yun, et al., International Conference on Learning Representations (2018)\\n163. Y. Rong, et al., Advances in Neural Information Processing Systems 33 (2020)\\n164. J. Yang, et al., Advances in Neural Information Processing Systems 34 (2021)\\n165. C. Zhao, et al., International Conference on Learning Representations (2020)\\n166. R. You, et al., Advances in Neural Information Processing Systems 32 (2019)',\n",
       " '165. C. Zhao, et al., International Conference on Learning Representations (2020)\\n166. R. You, et al., Advances in Neural Information Processing Systems 32 (2019)\\n167. X. Fan, et al., Advances in Neural Information Processing Systems 33 (2020)\\n168. G. Brunner, et al., International Conference on Learning Representations (2020)\\n169. A. Dosovitskiy, et al., International Conference on Learning Representations (2021)\\n170. A. Katharopoulos, et al., International Conference on Machine Learning (2020)\\n171. L. Greengard, V. Rokhlin, Journal of Computational Physics 73(2), 325– (1987)\\n172. T. Nguyen, et al., Advances in Neural Information Processing Systems 34 (2021)\\n173. N. Kitaev, L. Kaiser, A. Levskaya, International Conference on Learning Representa-\\ntions (2020)\\n174. J. Lee, et al., International Conference on Machine Learning pp. 3744–3753 (2019)\\n175. A. Roy, et al., Transactions of the Association for Computational Linguistics pp. 53–68\\n(2020)',\n",
       " 'tions (2020)\\n174. J. Lee, et al., International Conference on Machine Learning pp. 3744–3753 (2019)\\n175. A. Roy, et al., Transactions of the Association for Computational Linguistics pp. 53–68\\n(2020)\\n176. R. Child, et al., arXiv preprint arXiv:1904.10509 (2019)\\n177. G. Correia, V. Niculae, A. Martins, Proceedings of the 2019 Conference on Empirical\\nMethods in Natural Language Processing and the 9th International Joint Conference\\non Natural Language Processing pp. 2174–2184 (2019)\\n178. H. Peng, et al., International Conference on Learning Representations (2021)\\n179. Y. Chen, et al., Advances in Neural Information Processing Systems 34 (2021)\\n180. M. Zaheer, et al., Advances in Neural Information Processing Systems 33 (2020)\\n181. C.Z. Huang, et al., International Conference on Learning Representations (2019)\\n182. J. Lu, et al., Advances in Neural Information Processing Systems 34 (2021)\\n183. Z. Pan, et al., Proceedings of the IEEE/CVF International Conference on Computer',\n",
       " '182. J. Lu, et al., Advances in Neural Information Processing Systems 34 (2021)\\n183. Z. Pan, et al., Proceedings of the IEEE/CVF International Conference on Computer\\nVision pp. 377–386 (2021)\\n184. C. Zhu, et al., Advances in Neural Information Processing Systems 34 (2021)\\n185. A. Jaegle, et al., International Conference on Machine Learning pp. 4651–4664 (2021)\\n186. K. Choromanski, et al., International Conference on Learning Representations (2021)\\n187. A. El-Nouby, et al., Advances in Neural Information Processing Systems 34 (2021)\\n188. Q. Yu, et al., Advances in Neural Information Processing Systems 34 (2021)\\n189. Z. Zeng, et al., International Conference on Machine Learning pp. 12,321–12,332 (2021)\\n190. Z. Shen, et al., Proceedings of the IEEE/CVF Winter Conference on Applications of\\nComputer Vision pp. 3531–3539 (2021)\\n191. S. Luo, et al., Advances in Neural Information Processing Systems 34 (2021)\\n192. X. Ma, et al., Advances in Neural Information Processing Systems 34 (2021)',\n",
       " 'QZhou-Embedding Technical Report\\n Kingsoft AI\\nQZhou-Embedding Technical Report\\nPeng Yu, En Xu, Bin Chen, Haibiao Chen, Yinfei Xu\\nKingsoft AI ∗\\nAugust 2025\\nAbstract\\nWe present QZhou-Embedding, a general-purpose contextual text embed-\\nding model with exceptional text representation capabilit ies. Built upon the\\nQwen2.5-7B-Instruct foundation model, we designed a uniﬁe d multi-task frame-\\nwork comprising specialized data transformation and train ing strategies. The\\ndata transformation scheme enables the incorporation of mo re diverse textual\\ntraining datasets, while the task-speciﬁc training strate gies enhance model learn-\\ning eﬃciency. We developed a data synthesis pipeline levera ging LLM API, in-\\ncorporating techniques such as Paraphrasing, Augmentatio n, and Hard negative\\nexample generation to improve the semantic richness and sam ple diﬃculty of\\nthe training set. Additionally, we employ a two-stage train ing strategy, compris-',\n",
       " 'example generation to improve the semantic richness and sam ple diﬃculty of\\nthe training set. Additionally, we employ a two-stage train ing strategy, compris-\\ning initial retrieval-focused pretraining followed by ful l-task ﬁne-tuning, enabling\\nthe embedding model to extend its capabilities based on robu st retrieval perfor-\\nmance. Our model achieves state-of-the-art results on the M TEB and CMTEB\\nbenchmarks, ranking ﬁrst on both leaderboards(August 27, 2 025), simultaneously\\nachieves state-of-the-art performance on tasks including Reranking, Clustering,\\netc. Our ﬁndings demonstrate that higher-quality, more div erse data is crucial for\\nadvancing retrieval model performance, and that leveragin g LLMs’ generative ca-\\npabilities can further optimize data quality for embedding model breakthroughs.\\nOur model weights are released on HuggingFace 1 under Apache 2.0 license. For\\nreproducibility, we provide evaluation code and instructi ons on GitHub 2.\\n1 Introduction',\n",
       " 'Our model weights are released on HuggingFace 1 under Apache 2.0 license. For\\nreproducibility, we provide evaluation code and instructi ons on GitHub 2.\\n1 Introduction\\nText embedding models, which transform natural language text int o mathematical vec-\\ntor representations, play an indispensable role in text mining, quest ion-answering sys-\\ntems, recommendation systems, and retrieval-augmented gener ation. Recently, LLM-\\nbased agent technology has experienced rapid development and wid espread adoption,\\nembedding models, which transform textual or multimodal data into vector represen-\\ntations for knowledge base construction, have signiﬁcantly enhan ced agent systems\\n∗ https://kingsoft.com/\\n1https://huggingface.co/Kingsoft-LLM/QZhou-Embedding\\n2https://github.com/Kingsoft-LLM/QZhou-Embedding\\narXiv:2508.21632v1  [cs.CL]  29 Aug 2025',\n",
       " 'QZhou-Embedding Technical Report\\n Kingsoft AI\\nin terms of real-time performance, long-term memory, data privac y preservation, and\\nknowledge integration capabilities. With the continuous advancemen t of neural net-\\nworks and deep learning, text embeddings have evolved from early s parse representa-\\ntions (e.g., BM25[ 1]) to dense representations based on ﬁne-tuned deep networks s uch\\nas BERT[2] and T5[ 3], leading to signiﬁcant performance improvements[ 4][5][6][7][8]. In\\n2022, the rise of large language models (LLMs), exempliﬁed by ChatG PT[9], ushered in\\na new era of text embeddings based on LLM representations, includ ing models like text-\\nembedding-3-large and RepLLaMA[ 10]. Recent research on optimizing text embedding\\nmodels has explored diverse perspectives and focal points. For ins tance, to address\\nthe limitation of decoder-only architectures—where causal atten tion mechanisms re-\\nstrict token embeddings to unidirectional semantic capture—seve ral approaches have',\n",
       " 'the limitation of decoder-only architectures—where causal atten tion mechanisms re-\\nstrict token embeddings to unidirectional semantic capture—seve ral approaches have\\nbeen proposed: Echo Embedding[ 11] employs input repetition and instruction design\\nto enable preceding tokens to capture subsequent token semant ics. LLM2Vec[ 12] modi-\\nﬁes attention to bi-directional mechanism to remove backward dep endency constraints.\\nConan-Embedding-v2[13] proposes a novel soft masking mechanism combined with dy-\\nnamic rank reduction. Another widely adopted approach is knowledg e distillation,\\nwhere text embeddings are treated as the ”signal states” repre senting textual seman-\\ntics. By distilling knowledge from high-performing teacher models to s tudent models,\\nthe objective is to optimize the embedding performance. For instan ce, Jasper[ 14] em-\\nploys a multi-stage knowledge distillation framework, combining with mu ltiple carefully',\n",
       " 'the objective is to optimize the embedding performance. For instan ce, Jasper[ 14] em-\\nploys a multi-stage knowledge distillation framework, combining with mu ltiple carefully\\ndesigned loss functions and ﬁnally achieving superior results. Debat er[16] proposes a\\nstep-by-step thinking mechanism for embedding generation, itera tively optimizing doc-\\nument representations through continuous COT. Distillation is applie d to constrain\\nthe ﬁnal token representation to learn the optimal semantic stat es from these thinking\\nsteps. Additionally, hard negative sampling has emerged as a crucial research direc-\\ntion in text embedding models, serving as a pivotal technique for mod el optimization.\\nANCE[18] identiﬁed that conventional dense retrieval training leads to dimin ishing gra-\\ndient norms during optimization. Thus they developed an asynchron ous Approximate\\nNearest Neighbor (ANN) indexing mechanism that periodically refres hes the negative',\n",
       " 'dient norms during optimization. Thus they developed an asynchron ous Approximate\\nNearest Neighbor (ANN) indexing mechanism that periodically refres hes the negative\\nsample pool using the current model parameters, thereby ensur ing the maintenance\\nof up-to-date and optimally challenging negative samples. Both Cona n-Embedding[24]\\nand its v2 version incorporated similar dynamic hard negative sampling techniques to\\nenhance model performance. NV-Embed[ 19] implemented an alternative approach by\\nleveraging their previously developed NV-Retriever’s[ 20] positive-aware negative min-\\ning strategy, including TopK-MarginPos and TopKPercPos ﬁltering m echanisms.\\nIn this work, we present QZhou-Embedding, built upon the powerfu l Qwen2.5-7B-\\nInstruct[21] model, which pushes the boundaries of text embedding capabilities. To\\nenhance the model’s semantic understanding, we designed a uniﬁed m ulti-task learn-\\ning framework that not only accommodates more diverse training da ta but also bring',\n",
       " 'enhance the model’s semantic understanding, we designed a uniﬁed m ulti-task learn-\\ning framework that not only accommodates more diverse training da ta but also bring\\neﬃcient learning across three key tasks: retrieval, natural langu age inference (NLI),\\nand classiﬁcation. Our framework comprises two core components : 1. Data Trans-\\nformation: We carefully adapt data formats to the speciﬁc require ments of retrieval,\\nNLI, and classiﬁcation tasks, enabling eﬀective feature extractio n from heterogeneous\\ndata sources, signiﬁcantly beneﬁting retrieval model training. 2. Training Strategy:\\nWe designed specialized loss functions based on each task’s charact eristics, optimizing\\nmodel training eﬃciency. To further improve the robustness and g eneralization of vec-\\n2',\n",
       " 'QZhou-Embedding Technical Report\\n Kingsoft AI\\ntor representation, we propose a data synthesis method by emplo ying three techniques\\nto address data scarcity: Paraphrasing & Data augmentation for limited datasets and\\nHard negative generation for negative sample enrichment. Building u pon prior work, we\\ndesigned a strategy named ”Data Grouping Strategy”, enabling ba tch sampling within\\nsingle datasets, inadvertently increasing training diﬃculty through in-batch negative\\nsampling from the same distribution. For model training, we used a tw o-phase train-\\ning approach, through the ﬁrst-stage retrieval training and sec ond-stage full-capability\\ntraining, our model acquires a solid foundation of retrieval capabilit ies, while eﬀectively\\nextending to multiple capability dimensions. Our model achieved state -of-the-art av-\\nerage scores on CMTEB[ 22] and MTEB[ 23] benchmarks, ranking ﬁrst overall on both\\nCMTEB and MTEB leaderboards, demonstrating the eﬀectiveness o f our approach.',\n",
       " 'erage scores on CMTEB[ 22] and MTEB[ 23] benchmarks, ranking ﬁrst overall on both\\nCMTEB and MTEB leaderboards, demonstrating the eﬀectiveness o f our approach.\\nThe contributions of our work are summarized as follows:\\n• We propose a uniﬁed multi-task learning framework that systematic ally coordi-\\nnates both data processing and training pipelines, enhancing divers ity in datasets\\nand eﬃciency in model training ;\\n• We develop advanced data synthesis techniques powered by LLM, in cluding Para-\\nphrasing, Data augmentation, and Hard negative generation. The se methods\\nsigniﬁcantly enhance the quality of training corpora, thereby impro ving model’s\\nrobustness and generalization capabilities;\\n• We emply a two-stage training paradigm: Stage 1 focuses exclusively on retrieval\\ncapability building, establishing strong foundational retrieval perf ormance; and\\nstage 2 implements balanced training with controled retrieval/non-r etrieval task',\n",
       " 'capability building, establishing strong foundational retrieval perf ormance; and\\nstage 2 implements balanced training with controled retrieval/non-r etrieval task\\nratios, achieving superior performance on classiﬁcation (CLS), pa ir classiﬁcation\\n(PairCLS), and semantic textual similarity (STS) tasks while maintain ing re-\\ntrieval eﬀectiveness;\\n• Our model achieves state-of-the-art performance on both MTE B and CMTEB\\nbenchmarks, which validates the eﬀectiveness of our proposed me thods.\\n2 Related Works\\n2.1 Text Embedding Models\\nText vector representation is a fundamental research area in na tural language processing\\n(NLP) and serves as the cornerstone for language understandin g. Early approaches re-\\nlied on sparse vector representations, such as TF-IDF[\\n25], BM25[26], and LSA[ 27]. With\\nthe advent of pretrained language models, dense contextualized r epresentations based\\non architectures like BERT[ 2] and T5[ 3] became widely studied and applied[ 4][5][6]. In',\n",
       " 'the advent of pretrained language models, dense contextualized r epresentations based\\non architectures like BERT[ 2] and T5[ 3] became widely studied and applied[ 4][5][6]. In\\nthe era of large language models (LLMs), major advancements hav e led to the devel-\\nopment of LLM-based embedding models, such as text-embedding- 3-small/large (Ope-\\nnAI), E5-Mistral-7B[28], SFR-Embedding-Mistral[29], SFR-Embedding-2R[ 30], GRITLM[31],\\nLLM2Vec[12], RepLLaMA[10], BGE-en-icl[32], NV-Embed[19], gte-Qwen2-7B-Instruct[33],\\nQwen3-Embedding[34], etc. These models beneﬁt from optimized LLM architectures—suc h\\nas RoPE positional encoding[ 35], RMSNorm[ 36], and GeGLU activation[ 37]—combined\\nwith their strong semantic contextualization capabilities acquired th rough large-scale\\n3',\n",
       " 'QZhou-Embedding Technical Report\\n Kingsoft AI\\npretraining. As a result, LLM-based embeddings achieve superior p erformance in re-\\ntrieval and related tasks.\\n2.2 Embedding Model Training\\nThe mainstream approaches currently involve contrastive learning pretraining on un-\\nsupervised/weakly supervised corpora and supervised contrast ive learning training on\\nhigh-quality labeled positive and negative samples. In unsupervised le arning, early\\nwork like SimCSE[\\n7] proposed feeding continuous inputs of both original and noise-\\naugmented texts while employing contrastive learning to enhance th e model’s dis-\\ncriminative representation capability. For weakly supervised learnin g, gte[ 33] utilized\\nlarge-scale structured data (web search data, title-article pairs , etc.) for pretraining,\\nfollowed by ﬁne-tuning on high-quality open-source retrieval train ing data, achieving\\nperformance comparable to OpenAI embeddings with signiﬁcantly fe wer parameters.',\n",
       " 'followed by ﬁne-tuning on high-quality open-source retrieval train ing data, achieving\\nperformance comparable to OpenAI embeddings with signiﬁcantly fe wer parameters.\\nConan-Embedding[24] and v2 similarly adopted the weakly supervised pretraining &\\nsupervised ﬁne-tuning approach but incorporated techniques like cross-GPU batch loss\\nbalancing, dynamic hard negative mining, and soft masking (v2) to op timize the model.\\nSeed1.6-Embedding[38] employed a phased training strategy combining text and multi-\\nmodal pretraining followed by business-scenario-speciﬁc ﬁne-tun ing, achieving superior\\nrepresentation quality.\\nSubstantial research has also been conducted on modeling diﬀeren t tasks. Piccolo2[\\n39]\\nintroduced multi-task hybrid loss functions for diverse downstrea m tasks, an approach\\nwe also incorporate. SFR-Embedding[ 30] utilized multi-task learning techniques to\\nregularize embeddings, signiﬁcantly enhancing domain data discrimina tion. Xiaobu-',\n",
       " 'we also incorporate. SFR-Embedding[ 30] utilized multi-task learning techniques to\\nregularize embeddings, signiﬁcantly enhancing domain data discrimina tion. Xiaobu-\\nembedding uniﬁed the treatment of major CMTEB problem categorie s from the per-\\nspective of circle loss[ 40], fully leveraging multiple positive examples in original datasets\\nwhile carefully balancing diﬀerent loss weights.\\n2.3 Data Synthesis\\nData quantity and quality are the most critical factors in model opt imization, data\\nsynthesis methods have become a critical research direction due t o the high cost of\\nmanual annotation. Doc2Query[\\n41] and Query2Doc[ 42] employ question-answering\\nmodels to generate pseudo-queries and pseudo-documents resp ectively, enhancing data\\nfor improved RAG performance. Promptagator[ 43] addresses few-shot retrieval sce-\\nnarios by generating queries of diverse intents using few-shot dem onstrations and an-',\n",
       " 'for improved RAG performance. Promptagator[ 43] addresses few-shot retrieval sce-\\nnarios by generating queries of diverse intents using few-shot dem onstrations and an-\\nnotations, eﬀectively improving retrieval capabilities across varyin g intents or distri-\\nbutions. GPL[ 44] utilizes existing T5 encoder-decoder models to generate queries,\\nretrieves similar passages as hard negatives using existing retrieva l models, and em-\\nploys cross-encoders to score each (query, passage) pair. Unn atural Instructions[ 45]\\nleverages prompt and in-context learning (ICL) techniques to gen erate synthetic ex-\\namples through controlled instructions, inputs, and constraints, producing 64k diverse\\ndata entries from several seed examples with promising experiment al results. Qwen3-\\nEmbedding[34] designs a diversiﬁed prompting strategy by assigning document-s peciﬁc\\nroles to simulate potential users querying that document, enabling LLMs to generate',\n",
       " 'Embedding[34] designs a diversiﬁed prompting strategy by assigning document-s peciﬁc\\nroles to simulate potential users querying that document, enabling LLMs to generate\\nstylistically authentic queries that enhance diversity and realism.\\n4',\n",
       " 'QZhou-Embedding Technical Report\\n Kingsoft AI\\n2.4 Hard Negative Mining Techniques\\nHard negatives serve as essential components in contrastive lear ning for retrieval model\\ntraining. Early work like ANCE[\\n46] proposed an asynchronous ANN indexing mech-\\nanism that periodically updates hard negatives using checkpoint sta tes to maintain\\noptimally challenging samples. Conan-Embedding[ 24] and its v2 version implemented\\na dynamic hard negative sampling strategy by excluding and refresh ing samples when\\ntheir scores fall below a threshold. NV-Retriever[ 47] proposed positive-aware negative\\nmining, introducing TopK-MarginPos and TopKPercPos ﬁltering crite ria to minimize\\nfalse negatives. LGAI-Embedding[ 17] built upon NV-Retriever’s strategy with adap-\\ntive margin-based mining strategies, employing ANNA IR as a teacher retrieval model\\nto identify high-quality hard negatives while using TopKPercPos ﬁlter ing to eliminate\\nfalse negatives.\\n3 Uniﬁed Multi-task Learning Framework',\n",
       " 'to identify high-quality hard negatives while using TopKPercPos ﬁlter ing to eliminate\\nfalse negatives.\\n3 Uniﬁed Multi-task Learning Framework\\nEmbedding models support numerous downstream tasks including re trieval, reranking,\\nSTS, and classiﬁcation. Given the diversity of these tasks and their associated data\\ncomplexity, we explore a uniﬁed strategy to eﬀectively handle them c ollectively while\\npromoting optimization of the embedding model. Existing research on uniﬁed task pro-\\ncessing includes circle loss[\\n40], which approaches sentence pair similarity from a global\\nperspective by categorizing tasks into class-level labels and pair-w ise labels, Xiaobu-\\nembedding demonstrated signiﬁcant improvements by adopting this approach. Other\\nmodels like Piccolo2[ 39], SFR-Embedding[ 30], NV-Embed[ 47], Conan-Embedding[ 24] ,\\nand Conan-Embedding-v2 have incorporated multi-task learning us ing diverse train-\\ning data with varying label processing methods, some employing task -speciﬁc losses',\n",
       " 'and Conan-Embedding-v2 have incorporated multi-task learning us ing diverse train-\\ning data with varying label processing methods, some employing task -speciﬁc losses\\n(InfoNCE[48], Cosent[ 49], etc.).\\nOur design principle aims to accommodate more tasks and data types , enabling cross-\\ndomain and cross-task data to eﬀectively enhance embedding capa bilities. We propose\\na uniﬁed multi-task learning framework that categorizes training da ta into three task\\ntypes: retrieval, NLI, and classiﬁcation, with customized data and training solutions\\nfor each, allowing most natural text data to be converted into emb edding training data\\nthrough this framework. The following sections detail the framewo rk’s components and\\nimplementation methods.\\n3.1 Model Architecture\\nEmbedding models based on BERT or T5 [\\n39][15][50][24] exhibit powerful contextual\\nrepresentation capabilities, primarily attributed to their bidirection al attention mech-',\n",
       " '3.1 Model Architecture\\nEmbedding models based on BERT or T5 [\\n39][15][50][24] exhibit powerful contextual\\nrepresentation capabilities, primarily attributed to their bidirection al attention mech-\\nanisms. However, recent large language models predominantly adop t decoder-only ar-\\nchitectures with unidirectional attention, signiﬁcantly constrainin g tokens’ ability to\\ncapture contextual information. Several studies have address ed this limitation through\\narchitectural modiﬁcations or attention mechanism optimizations[ 12][31][47]. Our work\\nbuilds upon the Qwen2.5-7B-Instruct architecture and checkpoin t due to its exceptional\\nChinese language contextual capabilities. Consequently, we impleme nted the following\\nmodiﬁcations: (1) modifying the original causal attention to bi-dire ctional attention\\n5',\n",
       " 'QZhou-Embedding Technical Report\\n Kingsoft AI\\nFigure 1: QZhou-Embedding Architecture\\nto enable comprehensive context capture, and (2) employing mean pooling with sub-\\nsequent normalization to produce ﬁnal embedding vectors. The mo del architecture is\\nshown in Figure 1\\n3.2 Data Transformation\\n3.2.1 Retrieval-oriented Process\\nWhile open-source datasets such as MS MARCO[\\n64] are readily accessible, they alone\\nare insuﬃcient for further advancing embedding model capabilities, thus we supplement\\nwith data from additional sources, such as news, academic paper a nd QA datasets.\\nGiven the heterogeneous nature of these datasets across doma ins and purposes, we\\ndesign a retrieval-oriented data transformation methodology to c onvert diverse sources\\nand formats into training data suitable for retrieval task. Below we outline selected\\ncategories of training data used for transformation and their pro cessing procedures:\\n• Title-Body/Abstract ”Title-Body/Abstract” type data primarily consists of',\n",
       " 'categories of training data used for transformation and their pro cessing procedures:\\n• Title-Body/Abstract ”Title-Body/Abstract” type data primarily consists of\\ntitle-body/article pairs typically sourced from online news, articles, documents,\\narXiv publications and Wikipedia. For these data types, the transfo rmation pro-\\ncess involves using the title as the query and the body/abstract as the positive\\nsample. However, since the latter are documents, truncation is ap plied when they\\nexceed the maximum training length.\\n• Claim-Evidence This data type typically presents a claim or statement followed\\nby extracted evidence that either supports or refutes it, commo nly used for multi-\\nhop fact extraction and claim veriﬁcation tasks. Datasets genera lly contain claims\\nand corresponding evidence, with each evidence instance labeled as ”Supports”\\nor ”Refutes”. The transformation process involves: converting the claim portion\\n6',\n",
       " 'QZhou-Embedding Technical Report\\n Kingsoft AI\\ninto a query sample, for evidence labeled as ”Supports”, the text is treated as a\\npositive sample; for evidence labeled as ”Refutes”, it is converted in to a negative\\nsample.\\n• Question-Answer Question-answering data and conversational Q-A pairs pri-\\nmarily originate from chat platforms and forums. Within the current wave of\\nLLM and reinforcement learning research, such data exhibits rema rkable volume\\nand diversity. Virtually single-turn Q-A datasets(one question pair ed with one\\nanswer) represents the most suitable format for retrieval train ing. For transfor-\\nmation, the ”Question/Query/User” portion is converted into que ries, while the\\n”Answer/Response/Assistant” portion is processed as documen ts.\\n3.2.2 NLI-oriented Process\\nNatural Language Inference (NLI) represents a fundamental capability of NLP models,\\nencompassing tasks such as semantic similarity, textual entailment , and sentiment anal-',\n",
       " '3.2.2 NLI-oriented Process\\nNatural Language Inference (NLI) represents a fundamental capability of NLP models,\\nencompassing tasks such as semantic similarity, textual entailment , and sentiment anal-\\nysis. This section describes the methodology for transforming and constructing training\\nsets from NLI-style data, using textual semantic similarity (STS) a nd textual entailment\\ntasks as illustrative examples. Our approach distinctively reformula tes NLI tasks into\\ntext\\npair-score formats compatible with Cosent loss[ 49] training strategy, where sample\\npairs are quantitatively scored based on their semantic relationship s. The processing\\nprocedures for each are detailed below:\\n• STS Semantic Textual Similarity (STS) is characterized by its symmetric s e-\\nmantic matching to determine whether two sentences share equiva lent meaning.\\nSTS datasets typically consist of sentence pairs with associated lab els, which may',\n",
       " 'mantic matching to determine whether two sentences share equiva lent meaning.\\nSTS datasets typically consist of sentence pairs with associated lab els, which may\\nbe binary classiﬁcations (yes/no, true/false) or numerical score s (e.g., 1.2, 3.1,\\n4.8). For binary labels, ”yes”/”true” are mapped to a numerical va lue of 1, while\\n”no”/”false” are converted to 0. The data is then structured int o (query, docu-\\nment, score) triplets. Due to the symmetric nature of STS, each s ingle original\\ndata sample can generate two training triplets by interchanging the query and\\npositive document roles.\\n• Textual Entailment Textual entailment further examines a model’s capabilities\\nin reasoning, typically featuring three-class labels: entailment, neu tral, contradic-\\ntion. Our processing method employs a three-tier scoring system: labels are\\nassigned values of 2, 1, and 0 for entailment, neutral, and contrad iction respec-',\n",
       " 'tion. Our processing method employs a three-tier scoring system: labels are\\nassigned values of 2, 1, and 0 for entailment, neutral, and contrad iction respec-\\ntively. We construct (query, document, score) triplets accordin gly, and similarly\\nleverage symmetry to double the dataset size.\\n3.2.3 CLS-oriented Process\\nClassiﬁcation tasks encompass text categorization and sentiment classiﬁcation scenar-\\nios, it typically follows a (text, label) format, where texts within the s ame category\\nexhibit semantic proximity while distinct boundaries separate diﬀeren t classes. NV-\\nEmbed[\\n47] compared label-based and example-based data construction met hods, with\\nexperimental results demonstrating the superiority of the latter . Adopting the example-\\nbased approach, we process classiﬁcation data (text, label) by us ing the text as query,\\n7',\n",
       " 'QZhou-Embedding Technical Report\\n Kingsoft AI\\nFigure 2: CLS-oriented data transformation\\nsampling other texts sharing the same label as positive examples, an d selecting texts\\nfrom diﬀerent labels as negative examples. Figure 2 provides a detailed schematic\\nillustration of this process.\\n3.3 Training Strategy\\nEach task category—retrieval, NLI, and classiﬁcation—operates within a data construc-\\ntion process respectively, for which we have designed specialized tr aining objectives to\\nto enhance model training eﬃciency. This section elaborates on the design of loss\\nfunctions for retrieval, NLI, and classiﬁcation tasks.\\n3.3.1 Retrieval\\nFor the retrieval task, we adopt the widely used InfoNCE loss[\\n48], but incorporate an\\nimprovement inspired by gte[ 33] by augmenting the original query-negative loss with an\\nadditional query-query loss term. Speciﬁcally, each query within a b atch is treated as a\\nnegative sample for all other queries. The ﬁnal loss formulation is ex plicitly described',\n",
       " 'additional query-query loss term. Speciﬁcally, each query within a b atch is treated as a\\nnegative sample for all other queries. The ﬁnal loss formulation is ex plicitly described\\nin Equation ( 1).\\nLRetrieval = − 1\\nn\\n∑\\ni\\nlog esim(qi,d +\\ni )/τ\\nesim(qi,d +\\ni )/τ + ∑\\nj esim(qi,d −\\nj )/τ + ∑\\nj̸=i esim(qi,q j )/τ\\n(1)\\n3.3.2 NLI\\nFor NLI tasks, the transformed labels are numerically comparable a nd exhibit ordinal\\nrelationships. We employ Cosent loss[\\n49] to optimize such data, which is designed\\nbased on the principles of Circle loss[ 40]. As a ranking-sensitive loss function, Cosent\\nloss requires only ordinal label information for optimization while demo nstrating faster\\nconvergence. Its mathematical formulation is presented in Equat ion ( 2).\\n8',\n",
       " 'QZhou-Embedding Technical Report\\n Kingsoft AI\\nLNLI = log(1 +\\n∑\\nsim(i,j )>sim(k,l )\\nexp(sim(xk, x l) − sim(xi, x j)\\nτ )) (2)\\n3.3.3 CLS\\nThe classiﬁcation loss also adopts the InfoNCE objective. However , since CLS data is\\nprocessed in an example-based manner, directly applying in-batch n egative sampling\\non classiﬁcation datasets with limited categories may lead to false neg atives from items\\nof diﬀerent classes. Numerous studies have proposed diverse app roaches to address\\nthis issue[\\n51][52][47]. We propose a masking mechanism that appends class labels to\\neach positive and negative sample during preprocessing (recorded as separate variables\\nrather than modifying raw text). During in-batch negative sampling , for each negative\\nsample from other data instances, we check whether its label matc hes the current query’s\\nclass. If matched, the negative loss contribution is masked to zero to prevent erroneous',\n",
       " 'sample from other data instances, we check whether its label matc hes the current query’s\\nclass. If matched, the negative loss contribution is masked to zero to prevent erroneous\\npenalization; otherwise, it is normally computed. The core loss remain s InfoNCE, with\\nthe CLS loss formulation shown in Equation ( 3). Where Cti denotes the class label of\\nsample ti, and nrepresents the number of negative samples per data instance.\\nLCLS = − 1\\nn\\n∑\\ni\\nlog esim(ti,t +\\ni )/τ\\nZi\\n(3)\\nwhere Zi = esim(ti,t +\\ni )/τ +\\n∑\\nn\\nMASK(ti, t −\\ni,n ) ·esim(ti,t −\\ni,n )/τ +\\n∑\\nj̸=i\\nMASK(ti, t j ) ·esim(ti,t j )/τ +\\n∑\\nj̸=i\\n∑\\nn\\nMASK(ti, t −\\nj,n ) ·esim(ti,t −\\nj,n )/τ\\nand Cti = Ct+\\ni\\nand MASK( ti, t j ) =\\n{\\n0 if Cti = Ctj ,\\n1 otherwise\\n4 Data Synthesis\\nThe production of higher-quality data through data production ha s gained critical im-\\nportance in embedding training. Manual annotation incurs higher co sts and lower\\nproduction eﬃciency, thus developing eﬀective automated data sy nthesis methods has',\n",
       " 'portance in embedding training. Manual annotation incurs higher co sts and lower\\nproduction eﬃciency, thus developing eﬀective automated data sy nthesis methods has\\nemerged as a key research focus. Recent advancements in large la nguage models (LLMs)\\nhave signiﬁcantly improved their linguistic capabilities, enabling accura te interpretation\\nof human instructions and generation of high-quality outputs. Mult iple existing meth-\\nods have eﬀectively leveraged LLMs to generate high-quality data[\\n28][34], we similarly\\n9',\n",
       " 'QZhou-Embedding Technical Report\\n Kingsoft AI\\nleverages LLM capabilities for data production across three dimens ions: structural di-\\nversity, semantic diversity, and diﬃculty, with dedicated synthesis strategies for each.\\nFor structural diversity, we propose Paraphrasing techniques; for semantic diversity,\\nwe introduce Augmentation methods; and to increase training diﬃcu lty and improve\\nsemantic discriminability, we employ LLMs to generate more challenging hard negative\\nexamples. The following sections detail these methodologies. The co nstraint compo-\\nnents for all data synthesis techniques are speciﬁed in Table 5 of Appendix A.1.\\n4.1 Structural Diversity Enhancement\\nLinguistic structures of text encompass lexical, syntactic, and gr ammatical features,\\nwhich represent relatively surface-level characteristics reﬂect ing word arrangements,\\ncombinations, tenses, voices, and other formal attributes. Emb edding models must',\n",
       " 'which represent relatively surface-level characteristics reﬂect ing word arrangements,\\ncombinations, tenses, voices, and other formal attributes. Emb edding models must\\naccurately capture underlying semantics despite variations in surf ace form, ensuring\\nrobustness to external structural changes. For example, the following two sentences,\\ndespite structural diﬀerences, should be recognized as semantic ally equivalent:\\n• The cat chased the mouse.\\n• The mouse was chased by the cat.\\nTo eﬀectively train an embedding model that remains invariant to str uctural variations\\nwhile accurately capturing semantic information, we propose a Para phrasing strategy.\\nFor each training sample containing a query and a positive document, we apply LLM-\\nbased paraphrasing to both contents, generating augmented ins tances that preserve\\nsemantic equivalence while introducing structural divergence. The prompt constraints\\nand workﬂow are illustrated in Figure\\n3.\\nFigure 3: LLM-based Paraphrasing Workﬂow',\n",
       " 'semantic equivalence while introducing structural divergence. The prompt constraints\\nand workﬂow are illustrated in Figure\\n3.\\nFigure 3: LLM-based Paraphrasing Workﬂow\\n4.2 Semantic Diversity Enhancement\\nMerely augmenting data through superﬁcial structural modiﬁcat ions yields negligible\\nimprovements in model capabilities, as generalization relies not only on structural dis-\\nentanglement but also on diverse topics and content to ensure unif orm vector rep-\\nresentations in the spatial domain. Therefore, beyond paraphra sing, we propose an\\naugmentation method using LLM to diversify semantics. The core co ncept is: given a\\n10',\n",
       " 'QZhou-Embedding Technical Report\\n Kingsoft AI\\ncomplete (query, positive) pair, the model must comprehend the d omain and perspec-\\ntive discussed and learn to expand into diﬀerent topics, aspects, a nd viewpoints while\\nremaining contextually anchored. This process is governed via prom pt constraints. The\\nAugmentation framework is illustrated in Figure 4.\\nFigure 4: Semantic Augmentation Workﬂow\\nFigure 5: Hard Negative Synthesis Workﬂow\\n4.3 More challenging embeddings\\nHard negative examples are crucial for enhancing the performanc e of text embedding\\nmodels, often requiring substantial eﬀort to acquire. Leveraging the linguistic capabili-\\nties of large language models, we design an automated hard negative synthesis method\\ntailored for retrieval datasets. Our domain-speciﬁc experiments demonstrate that large\\nlanguage models can generate examples that are indistinguishable, t he framework is\\nillustrated in Figure\\n5.',\n",
       " 'tailored for retrieval datasets. Our domain-speciﬁc experiments demonstrate that large\\nlanguage models can generate examples that are indistinguishable, t he framework is\\nillustrated in Figure\\n5.\\nDuring Data paraphrasing and Augmentation, we implement task-sp eciﬁc strategies:\\nfor retrieval tasks, we rewrite/expand (query, positive) pairs a nd add them to the orig-\\ninal dataset; for NLI tasks, we rewrite individual sentences by ra ndomly duplicating\\nexisting entries containing the original sentences and replacing the m with rewritten\\nversions to achieve data expansion—without applying augmentation to prevent ambi-\\nguity; for classiﬁcation tasks, we rewrite sentences while retaining their original labels,\\nexample-based processing was applied using the rewritten results, again without em-\\nploying augmentation. We provide several data synthesis examples in Appendix A.3\\nfor reference.\\n11',\n",
       " 'QZhou-Embedding Technical Report\\n Kingsoft AI\\nFigure 6: Training pipeline\\n5 Training Optimization\\n5.1 Data Grouping Strategy\\nPrior works like Linq-Embedding[\\n52] and SFR-Embedding-Mistral[ 30] adopted task-\\nhomogeneous batching, partitioning data by task rather than mixin g them, and sam-\\npling tasks based on weighted randomness during training. Building on this, we propose\\na reﬁned Data Grouping Strategy, extending the granularity from task-level to dataset-\\nlevel partitioning. We posit that dataset-level grouping captures more domain-speciﬁc\\nclustering patterns—samples within the same dataset often exhibit inherent domain\\nsimilarities, while such consistency may not hold across datasets.\\nOur approach partitions training data into subsets by name. During training, only\\nsamples from a single dataset are sampled per batch, with ﬁle pointer s recorded to\\nenable sequential reading in subsequent iterations. For sampling we ights, we adopt\\nthe data sampling strategy from gte[',\n",
       " 'enable sequential reading in subsequent iterations. For sampling we ights, we adopt\\nthe data sampling strategy from gte[\\n33] and mgte[ 50], scaling weights by dataset size\\nfollowed by normalization. For dataset i with size li, its sampling weight is computed\\nas Equation ( 4)\\npi = lα\\ni∑ m\\nj=1 lα\\nj\\n(4)\\n5.2 Two-Stage Training\\nInspired by NV-Embed’s[\\n47] two-stage contrastive learning instruction tuning tech-\\nnique, we adopt a similar training approach: the ﬁrst stage exclusive ly uses retrieval-\\noriented training data, while the second stage integrates both ret rieval and non-retrieval\\ntasks, the overall training framework is illustrated in the ﬁgure 6. Two key distinctions\\nare incorporated: ﬁrst, we integrate the previously described Da ta Grouping Strat-\\negy; second, we implement global control over the sampling ratio of retrieval training\\ndatasets, since our ﬁndings indicate that naively incorporating add itional data signiﬁ-\\ncantly degrades retrieval performance.',\n",
       " 'datasets, since our ﬁndings indicate that naively incorporating add itional data signiﬁ-\\ncantly degrades retrieval performance.\\nFor global control of sampling ratio, a hyperparameter η is introduced into the sampling\\n12',\n",
       " 'QZhou-Embedding Technical Report\\n Kingsoft AI\\nfunction to control the proportion of retrieval training, ensurin g that throughout the\\nsecond training stage, the computational contribution of retriev al data accounts for η,\\nwhile non-retrieval data constitutes 1 − η. The following set of equations formalizes the\\ncomputational process from partitioned datasets to sampling rat io determination. Let\\nthe training data D = [ d1, d 2, ..., d N ] , where each di represents a distinct dataset (e.g.,\\nMSMARCO passage, SQUAD), with corresponding sizes L = [ l1, l 2, ..., l N ]. Following\\nthe aforementioned strategy, we ﬁrst apply an exponential scalin g factor α , a mask fac-\\ntor M is then applied to ﬁlter retrieval and non-retrieval training sets fo r summation.\\nThe equations are as follows:\\nSret =\\n∑\\ni\\nMi ·lα\\ni\\nSnon ret =\\n∑\\ni\\n(1 − Mi) ·lα\\ni\\nwhere M i =\\n{\\n0 if di ∈ RET,\\n1 else\\nwhere RET denotes the set of retrieval training datasets. The re trieval ratio is then',\n",
       " 'Sret =\\n∑\\ni\\nMi ·lα\\ni\\nSnon ret =\\n∑\\ni\\n(1 − Mi) ·lα\\ni\\nwhere M i =\\n{\\n0 if di ∈ RET,\\n1 else\\nwhere RET denotes the set of retrieval training datasets. The re trieval ratio is then\\nscaled using η to derive the ﬁnal normalized sampling ratios for the training sets:\\nLsamp = [ lsamp\\n1 , l samp\\n2 , ...l samp\\nN ]\\nwhere l samp\\ni =\\n{ ηRET ·lα\\ni\\nSret\\nif di ∈ RET,\\n(1−ηRET )·lα\\ni\\nSnon ret\\nelse\\n6 Experiments\\n6.1 Training Dataset\\nPrimary data sources include bge-en-icl, bge-m3-data, and bge-m ultilingual-gemma2-\\ndata\\n3 . The E5 dataset (approximately 1.5M samples) 4, utilized in E5-Mistral-7B[ 28],\\nEcho Embedding[ 11], and LLM2Vec[ 12], is also incorporated. The aforementioned\\ndatasets include commonly used retrieval training corpora such as MS MARCO (both\\npassage and document versions)[ 64], Natural Questions (NQ)[ 65], ELI5[66], HotpotQA[ 67],\\nMIRACL[68], SQuAD[ 69], FEVER[70], Quora Question Pairs(QQP), and DuReader[ 71],',\n",
       " 'passage and document versions)[ 64], Natural Questions (NQ)[ 65], ELI5[66], HotpotQA[ 67],\\nMIRACL[68], SQuAD[ 69], FEVER[70], Quora Question Pairs(QQP), and DuReader[ 71],\\netc. Previous researchers have already systematically collected a nd organized these\\ndatasets, making them readily usable, we solely utilized the proposed method to update\\nharder negative samples. Stella’s[ 53] retrieval data llm 5 provides high-quality (query,\\npositive, negative) triplets, while zpoint leverages datasets such a s Huatuo medical QA 6,\\nall above data has been incorporated. Additional data from huggin gface’s sentence-\\ntransformers7 repository includes reddit, hover[ 72], mr-tydi[ 73], law-gpt, and s2orc[ 74].\\n3https://github.com/FlagOpen/FlagEmbedding/tree/master/dataset\\n4https://drive.google.com/ﬁle/d/1YqgaJIzmBIH37XBxpRPCVzV CLh6aOI4/view\\n5https://huggingface.co/datasets/infgrad/retrieval data llm\\n6https://huggingface.co/iampanda/zpoint large embedding zh',\n",
       " '4https://drive.google.com/ﬁle/d/1YqgaJIzmBIH37XBxpRPCVzV CLh6aOI4/view\\n5https://huggingface.co/datasets/infgrad/retrieval data llm\\n6https://huggingface.co/iampanda/zpoint large embedding zh\\n7https://huggingface.co/sentence-transformers\\n13',\n",
       " 'QZhou-Embedding Technical Report\\n Kingsoft AI\\nOther sources encompass web questions, BioASQ[ 54], cmrc[ 55], CSL 8, nli for simcse\\n(used in SimCSE[ 7] and GTE[ 33]), MLDR 9, GLUE Benchmark[ 56], Yelp Reviews[ 57]\\nand Weibo Sentiment 10 training sets.\\nWe further integrate MTEB evaluation-related datasets like Imdb- Classiﬁcation[58],\\nMassiveIntent-Classiﬁcation[59], MassiveScenario-Classiﬁcation[59], STS12[60], LCQMC[61],\\nPAWSX[62], and STSB[ 63], we utilized the training split from these datasets with con-\\ntamination exclusion applied to remove samples highly similar to test set s.\\nFor data requiring format conversion, we apply the methodologies d escribed in Sen-\\ntion 3.2. Datasets with limited samples (e.g., subsets of bge and e5 series, Im db-\\nClassiﬁcation, STS12, LCQMC) are augmented via Paraphrasing and Augmentation\\n(typically applied to datasets with fewer than 60k samples), we ultima tely obtained ap-',\n",
       " 'Classiﬁcation, STS12, LCQMC) are augmented via Paraphrasing and Augmentation\\n(typically applied to datasets with fewer than 60k samples), we ultima tely obtained ap-\\nproximately 5M high-quality training samples through API interfaces . We deduplicate\\nall training sets and ﬁlter out samples with low query-pos scores usin g GTE-Qwen2-7B-\\nInstruct 11. For retrieval data lacking hard negatives, we employ synthetic ha rd negative\\ngeneration. Due to API cost constraints, only 30% of hard negativ es are synthetically\\ngenerated; the remainder are produced using stella-large-zh-v3 -1792d[53], with top-10\\nto top-30 ranked results selected as hard negatives. The ﬁnal tr aining dataset contains\\n11M quadruples (query, pos, neg, instruction) in total.\\n6.2 Trainset Instructions\\nFor most training data containing instruction formats, we retain th eir original con-\\ntents. For the MTEB training set, we adopt instructions correspo nding to its evalu-',\n",
       " '6.2 Trainset Instructions\\nFor most training data containing instruction formats, we retain th eir original con-\\ntents. For the MTEB training set, we adopt instructions correspo nding to its evalu-\\nation(consistent with Qwen3-Embedding runtime). For external d ata lacking instruc-\\ntions (e.g., Huatuo, Reddit, Law-GPT, GLUE), we design task-spec iﬁc and domain-\\nadaptive instructions. Partial instruction templates are provided in Appendix\\nA.2.\\n6.3 Training Details\\nAs previously mentioned, we adopt a two-stage training approach. For the ﬁrst-stage\\nretrieval training, we train on all retrieval datasets, with a warm- up step of 300 and\\na learning rate of 3e-5, the total step of training is 32k. In the sec ond stage, we use\\nall training data, set the learning rate to 2e-5, and train for 8k ste ps, keeping all other\\nconﬁgurations the same as in the ﬁrst stage. We employ a batch size of 256 for all data\\nusing the InfoNCE loss (i.e., retrieval and classiﬁcation), considerin g data using the',\n",
       " 'conﬁgurations the same as in the ﬁrst stage. We employ a batch size of 256 for all data\\nusing the InfoNCE loss (i.e., retrieval and classiﬁcation), considerin g data using the\\ncosent loss (i.e., NLI), due to lower memory consumption from the ab sence of forward\\ncomputation for negative samples, the batch size is set to 768. Acr oss all stages, we\\nemploy bﬂoat16 precision, with 4 hard negative samples and a cosine t emperature of\\n0.02, using Adam optimizer with a weight decay of 0.01. The Data Group ing Strategy\\nremains unchanged between the two stages, except that the sec ond stage incorporates\\nall data with a global retrieval ratio ηRET of 0.72. Unlike existing works that commonly\\n8https://github.com/ydli-ai/CSL?tab=readme-ov-ﬁle\\n9https://huggingface.co/datasets/Shitao/MLDR\\n10https://github.com/SophonPlus/ChineseNlpCorpus?tab=readme-ov-ﬁle\\n11https://huggingface.co/Alibaba-NLP/gte-Qwen2-7B-instruct\\n14',\n",
       " 'QZhou-Embedding Technical Report\\n Kingsoft AI\\nuse LoRA ﬁne-tuning, we employ full-parameter ﬁne-tuning at all st ages to ensure\\nmaximum performance improvement. The query and passage length s are set to 256\\nand 1536 respectively. However, in practice, the model can handle sequences up to 8k\\nin length due to the strong length extrapolation capability of the RoP E[35] positional\\nencoding used in most LLMs. The hyperparameter conﬁgurations f or all training stages\\nare provided in the table 1.\\nTable 1: Training Hyperparameter Speciﬁcations\\nItem Stage1 Stage2\\nWarm-up 300\\nSteps 3e-5 2e-5\\nLR 32k 8k\\nBatch Size InfoNCE 256\\nBatch Size Cosent - 768\\nPrecision bﬂoat16\\nTemperature 0.02\\nOptimizer Adam\\nQuery Length 256\\nPassage Length 1536\\n6.4 Compared Methods\\nWe selected the top-10 ranked models(August 27, 2025) on the MT EB/CMTEB leader-\\nboards prior to the release of QZhou-Embedding as baselines. For M TEB, the compar-\\native models include LGAI-Embedding-Preview[',\n",
       " 'boards prior to the release of QZhou-Embedding as baselines. For M TEB, the compar-\\native models include LGAI-Embedding-Preview[\\n17], the Seed series (v1.5[ 75] , v1.6[ 38]),\\nQwen series (8B, 4B)[ 34], ritrieve zh v1, xiaobu-embedding-v2, gemini-embedding-001[ 76],\\njasper en vision language v1[14], Linq-Embed-Mistral[52], SFR-Embedding-Mistral[ 30],\\nand NV-Embed-v2[ 47]. For CMTEB, the baseline models comprise the Seed series (as\\nabove), Qwen series (as above), Conan series (v1[ 24], v2[13]), zpoint large embedding zh,\\nand piccolo-large-zh-v2[ 39].\\n6.5 Main Results\\nThis section presents the evaluation results of Qzhou-embedding o n MTEB/CMTEB\\nbenchmarks, alongside comparative scores from the top 10 ranke d models. As detailed\\nin Table\\n2, Table 3, Qzhou-embedding achieves state-of-the-art performance ac ross\\nboth task-level and task-type average metrics, demonstrating the eﬀectiveness of our\\napproach. Furthermore, under MTEB’s oﬃcial ranking protocol, Q zhou-embedding',\n",
       " 'both task-level and task-type average metrics, demonstrating the eﬀectiveness of our\\napproach. Furthermore, under MTEB’s oﬃcial ranking protocol, Q zhou-embedding\\nsecured the top position on both leaderboards. ( Note: Highlighted maximum values\\nin certain columns may reﬂect the best performance among the liste d models rather\\nthan the overall leaderboard maximum, as exempliﬁed by the MTEB/c lassiﬁcation\\nbenchmark where the top score does not appear in the top 10 mode ls.)\\n15',\n",
       " 'QZhou-Embedding Technical Report\\n Kingsoft AI\\nTable 2: Performance on MTEB(eng, v2)\\nModel Class. Clust. Pair Class. Rerank. STS Retr. Summ. Mean(Task) Mean(TaskType)\\nLGAI-Embedding-Preview 89.97 59.25 88.67 49.13 66.18 86.69 38.93 74.12 68.4\\nSeed1.5-Embedding 89.88 60.83 87.39 50.67 67.45 87.23 36.44 74.76 68.56\\nQwen3-Embedding-8B 90.43 58.57 87.52 51.56 69.44 88.58 34.83 75.22 68.71\\nQwen3-Embedding-4B 89.84 57.51 87.01 50.76 68.46 88.72 34.39 74.6 68.1\\nSeed1.6-embedding 92.42 59.22 85.07 50.28 64.9 86.87 37.1 74.07 67.98\\ngemini-embedding-001 90.05 59.39 87.7 48.59 64.35 85.29 38.28 73.3 67.67\\njasper en vision language v1 90.27 60.52 88.14 50 56.05 84.37 37.19 71.41 66.65\\nLinq-Embed-Mistral 83 54.07 88.44 49.44 60.14 84.69 37.26 69.8 65.29\\nSFR-Embedding-Mistral 80.47 54.93 88.59 50.15 59.33 84.77 36.32 69.31 64.94\\nNV-Embed-v2 87.19 47.66 88.69 49.61 62.84 83.82 35.21 69.81 65\\nQZhou-Embedding(Ours) 88.97 61.65 92.43 51.77 67.12 91.65 33.05 75.97 69.52',\n",
       " 'NV-Embed-v2 87.19 47.66 88.69 49.61 62.84 83.82 35.21 69.81 65\\nQZhou-Embedding(Ours) 88.97 61.65 92.43 51.77 67.12 91.65 33.05 75.97 69.52\\nTable 3: Performance on CMTEB(cmn, v1)\\nModel Class. Clust. Pair Class. Rerank. STS Retr. Mean(Task) Mean(TaskType)\\nSeed1.6-embedding 77.98 73.11 88.71 71.65 79.69 68.94 75.63 76.68\\nSeed1.5-Embedding 79.37 71.11 89.57 70.14 79.33 66.56 74.87 76.01\\nritrieve zh v1 76.88 66.5 85.98 72.86 76.97 63.92 72.71 73.85\\nConan-embedding-v2 76.47 68.84 92.44 74.41 78.31 65.48 74.24 75.99\\nxiaobu-embedding-v2 76.53 65.17 85.94 72.58 76.49 64.18 72.36 73.48\\nQwen3-Embedding-8B 76.97 80.08 84.23 66.99 78.21 63.53 73.84 75\\nConan-embedding-v1 76.77 66.33 85.68 72.76 76.67 63.67 72.5 73.65\\nzpoint large embedding zh 76.4 62.23 85.75 72.33 76.36 63.86 71.81 72.82\\npiccolo-large-zh-v2 76.42 62.16 85.22 70 74.36 63.46 70.86 71.94\\nQwen3-Embedding-4B 75.46 77.89 83.34 66.05 77.03 61.26 72.27 73.51\\nQZhou-Embedding(Ours) 79.99 70.91 95.07 74.85 78.80 71.89 76.99 78.58',\n",
       " 'Qwen3-Embedding-4B 75.46 77.89 83.34 66.05 77.03 61.26 72.27 73.51\\nQZhou-Embedding(Ours) 79.99 70.91 95.07 74.85 78.80 71.89 76.99 78.58\\n7 Conclusion\\nIn this technical report, we present QZhou-Embedding, a genera l-purpose contextual\\ntext embedding model with exceptional text representation capa bilities. We designed a\\nuniﬁed multi-task framework comprising specialized data transform ation and training\\nstrategies, eﬀectively enhanced the diversity of training data. To further improve the\\nquality of training data and the model’s generalization capabilities, we d eveloped a data\\nsynthesis pipeline leveraging LLM API, incorporating techniques suc h as Paraphrasing,\\nAugmentation, and Hard negative example generation. We employ a t wo-stage training\\nstrategy comprising initial retrieval-focused training followed by fu ll-task ﬁne-tuning,\\nenabling the embedding model to extend its capabilities based on robu st retrieval per-',\n",
       " 'strategy comprising initial retrieval-focused training followed by fu ll-task ﬁne-tuning,\\nenabling the embedding model to extend its capabilities based on robu st retrieval per-\\nformance. The model achieves state-of-the-art results on the MTEB and CMTEB\\nbenchmarks, ranking ﬁrst on both leaderboards. Our ﬁndings est ablish that data qual-\\nity and diversity are pivotal for improving embedding model capabilitie s. In the future,\\nwe will focus on developing multimodal and multilingual embedding models , as well\\nas exploring eﬀective applications of embedding models in agent syste ms, aiming to\\nintegrate cutting-edge technologies to optimize this classical modu le.\\nReferences\\n[1] Robertson, Stephen E., and Steve Walker. ”Some simple eﬀective approximations to\\nthe 2-poisson model for probabilistic weighted retrieval.” In SIGIR’9 4: Proceedings\\n16',\n",
       " 'QZhou-Embedding Technical Report\\n Kingsoft AI\\nof the Seventeenth Annual International ACM-SIGIR Conferen ce on Research and\\nDevelopment in Information Retrieval, organised by Dublin City Univer sity, pp.\\n232-241. London: Springer London, 1994.\\n[2] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutano va. Bert: Pre-\\ntraining of deep bidirectional transformers for language underst anding. arXiv\\npreprint arXiv:1810.04805, 2018.\\n[3] Colin Raﬀel, Noam Shazeer, Adam Roberts, Katherine Lee, Shara n Narang, Michael\\nMatena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of tr ansfer learn-\\ning with a uniﬁed text-to-text transformer. Journal of machine le arning research,\\n21(140):1–67, 2020.\\n[4] Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, D axin Jiang,\\nRangan Majumder, and Furu Wei. Text embeddings by weakly-super vised con-\\ntrastive pre-training. arXiv preprint arXiv:2212.03533, 2022.',\n",
       " 'Rangan Majumder, and Furu Wei. Text embeddings by weakly-super vised con-\\ntrastive pre-training. arXiv preprint arXiv:2212.03533, 2022.\\n[5] Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Ried el, Piotr Bo-\\njanowski, Armand Joulin, and Edouard Grave. Unsupervised dense information\\nretrieval with contrastive learning. arXiv preprint arXiv:2112.0911 8, 2021.\\n[6] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence em beddings using\\nsiamese bert-networks. arXiv preprint arXiv:1908.10084, 2019.\\n[7] Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple contrastive\\nlearning of sentence embeddings. In Proceedings of the 2021 Conf erence on Empir-\\nical Methods in Natural Language Processing, pages 6894–6910, Online and Punta\\nCana, Dominican Republic. Association for Computational Linguistics .\\n[8] Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hern´ andez ´Abrego, Ji Ma,\\nVincent Y Zhao, Yi Luan, Keith B Hall, Ming-Wei Chang, et al. Large du al encoders',\n",
       " '[8] Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hern´ andez ´Abrego, Ji Ma,\\nVincent Y Zhao, Yi Luan, Keith B Hall, Ming-Wei Chang, et al. Large du al encoders\\nare generalizable retrievers. arXiv preprint arXiv:2112.07899, 202 1.\\n[9] Brown, Tom, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D . Kaplan, Pra-\\nfulla Dhariwal, Arvind Neelakantan et al. ”Language models are few-s hot learners.”\\nAdvances in neural information processing systems 33 (2020): 18 77-1901.\\n[10] Ma, Xueguang, Liang Wang, Nan Yang, Furu Wei, and Jimmy Lin. ”F ine-tuning\\nllama for multi-stage text retrieval.” In Proceedings of the 47th Int ernational ACM\\nSIGIR Conference on Research and Development in Information Re trieval, pp. 2421-\\n2425. 2024.\\n[11] Springer, Jacob Mitchell, Suhas Kotha, Daniel Fried, Graham Ne ubig, and Aditi\\nRaghunathan. ”Repetition improves language model embeddings.” a rXiv preprint\\narXiv:2402.15449 (2024).\\n[12] BehnamGhader, Parishad, Vaibhav Adlakha, Marius Mosbach, D zmitry Bah-',\n",
       " 'Raghunathan. ”Repetition improves language model embeddings.” a rXiv preprint\\narXiv:2402.15449 (2024).\\n[12] BehnamGhader, Parishad, Vaibhav Adlakha, Marius Mosbach, D zmitry Bah-\\ndanau, Nicolas Chapados, and Siva Reddy. ”Llm2vec: Large languag e models are\\nsecretly powerful text encoders.” arXiv preprint arXiv:2404.0596 1 (2024).\\n[13] https://cloud.tencent.com/developer/news/2461911\\n17',\n",
       " 'QZhou-Embedding Technical Report\\n Kingsoft AI\\n[14] Zhang, Dun, Jiacheng Li, Ziyang Zeng, and Fulong Wang. ”Jaspe r and stella:\\ndistillation of sota embedding models.” arXiv preprint arXiv:2412.19048 (2024).\\n[15] Chen, Jianlv, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, and Zheng\\nLiu. ”Bge m3-embedding: Multi-lingual, multi-functionality, multi-gran ularity text\\nembeddings through self-knowledge distillation.” arXiv preprint arXiv :2402.03216\\n(2024).\\n[16] Ji, Yifan, Zhipeng Xu, Zhenghao Liu, Yukun Yan, Shi Yu, Yishan L i, Zhiyuan\\nLiu, Yu Gu, Ge Yu, and Maosong Sun. ”Learning more eﬀective repre senta-\\ntions for dense retrieval through deliberate thinking before sear ch.” arXiv preprint\\narXiv:2502.12974 (2025).\\n[17] Choi J, Kim H, Jang H, et al. LG-ANNA-Embedding technical repo rt[J]. arXiv\\npreprint arXiv:2506.07438, 2025.\\n[18] Xiong, Lee, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Pau l Bennett,\\nJunaid Ahmed, and Arnold Overwijk. ”Approximate nearest neighbo r negative con-',\n",
       " 'preprint arXiv:2506.07438, 2025.\\n[18] Xiong, Lee, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Pau l Bennett,\\nJunaid Ahmed, and Arnold Overwijk. ”Approximate nearest neighbo r negative con-\\ntrastive learning for dense text retrieval.” arXiv preprint arXiv:200 7.00808 (2020).\\n[19] Lee, Chankyu, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad\\nShoeybi, Bryan Catanzaro, and Wei Ping. ”Nv-embed: Improved t echniques for\\ntraining llms as generalist embedding models.” arXiv preprint arXiv:2405 .17428\\n(2024).\\n[20] Moreira, Gabriel de Souza P., Radek Osmulski, Mengyao Xu, Rona y Ak, Benedikt\\nSchiﬀerer, and Even Oldridge. ”NV-Retriever: Improving text emb edding models\\nwith eﬀective hard-negative mining.” arXiv preprint arXiv:2407.15831 (2024).\\n[21] Team, Qwen. ”Qwen2 technical report.” arXiv preprint arXiv:24 07.10671 (2024).\\n[22] Xiao, Shitao, Zheng Liu, Peitian Zhang, Niklas Muennighoﬀ, Defu L ian, and Jian-',\n",
       " '[21] Team, Qwen. ”Qwen2 technical report.” arXiv preprint arXiv:24 07.10671 (2024).\\n[22] Xiao, Shitao, Zheng Liu, Peitian Zhang, Niklas Muennighoﬀ, Defu L ian, and Jian-\\nYun Nie. ”C-pack: Packed resources for general chinese embedd ings.” In Proceedings\\nof the 47th international ACM SIGIR conference on research and development in\\ninformation retrieval, pp. 641-649. 2024. Team, Qwen.\\n[23] Muennighoﬀ, Niklas, Nouamane Tazi, Lo¨ ıc Magne, and Nils Reimers . ”Mteb: Mas-\\nsive text embedding benchmark.” arXiv preprint arXiv:2210.07316 (2 022).\\n[24] Li, Shiyu, Yang Tang, Shizhe Chen, and Xi Chen. ”Conan-embed ding: Gen-\\neral text embedding with more and better negative samples.” arXiv p reprint\\narXiv:2408.15710 (2024).\\n[25] Aizawa, Akiko. ”An information-theoretic perspective of tf–id f measures.” Infor-\\nmation Processing & Management 39, no. 1 (2003): 45-65.\\n[26] Robertson, Stephen E., and Steve Walker. ”Some simple eﬀectiv e approximations',\n",
       " 'mation Processing & Management 39, no. 1 (2003): 45-65.\\n[26] Robertson, Stephen E., and Steve Walker. ”Some simple eﬀectiv e approximations\\nto the 2-poisson model for probabilistic weighted retrieval.” In SIGI R’94: Proceed-\\nings of the Seventeenth Annual International ACM-SIGIR Confe rence on Research\\nand Development in Information Retrieval, organised by Dublin City Un iversity,\\npp. 232-241. London: Springer London, 1994.\\n18',\n",
       " 'QZhou-Embedding Technical Report\\n Kingsoft AI\\n[27] Deerwester, Scott, Susan T. Dumais, George W. Furnas, Tho mas K. Landauer, and\\nRichard Harshman. ”Indexing by latent semantic analysis.” Journal of the American\\nsociety for information science 41, no. 6 (1990): 391-407.\\n[28] Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Maj umder, and\\nFuru Wei. Improving text embeddings with large language models. arX iv preprint\\narXiv:2401.00368, 2023b.\\n[29] Meng, Rui, Ye Liu, Shaﬁq Rayhan Joty, Caiming Xiong, Yingbo Zhou , and Semih\\nYavuz. ”Sfrembedding-mistral: enhance text retrieval with tran sfer learning.” Sales-\\nforce AI Research Blog 3 (2024): 6.\\n[30] Meng R, Liu Y, Joty S R, et al. Sfr-embedding-2: Advanced text embedding with\\nmulti-stage training, 2024[J].\\n[31] Muennighoﬀ, Niklas, S. U. Hongjin, Liang Wang, Nan Yang, Furu W ei, Tao Yu,\\nAmanpreet Singh, and Douwe Kiela. ”Generative representational instruction tun-',\n",
       " 'multi-stage training, 2024[J].\\n[31] Muennighoﬀ, Niklas, S. U. Hongjin, Liang Wang, Nan Yang, Furu W ei, Tao Yu,\\nAmanpreet Singh, and Douwe Kiela. ”Generative representational instruction tun-\\ning.” In The Thirteenth International Conference on Learning Rep resentations.\\n2024.\\n[32] Chaofan Li, MingHao Qin, Shitao Xiao, Jianlyu Chen, Kun Luo, Yingx ia Shao,\\nDefu Lian, and Zheng Liu. Making text embedders few-shot learner s. arXiv preprint\\narXiv:2409.15700, 2024.\\n[33] Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie , and Meis-\\nhan Zhang. Towards general text embeddings with multi-stage con trastive learning,\\n2023. URL https://arxiv.org/abs/2308.03281.\\n[34] Zhang, Yanzhao, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, B aosong Yang,\\nPengjun Xie et al. ”Qwen3 Embedding: Advancing Text Embedding and Reranking\\nThrough Foundation Models.” arXiv preprint arXiv:2506.05176 (2025 ).\\n[35] Su, Jianlin, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, an d Yunfeng Liu.',\n",
       " 'Through Foundation Models.” arXiv preprint arXiv:2506.05176 (2025 ).\\n[35] Su, Jianlin, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, an d Yunfeng Liu.\\n”Roformer: Enhanced transformer with rotary position embeddin g.” Neurocomput-\\ning 568 (2024): 127063.\\n[36] Zhang, Biao, and Rico Sennrich. ”Root mean square layer norma lization.” Ad-\\nvances in neural information processing systems 32 (2019).\\n[37] Shazeer, Noam. ”Glu variants improve transformer.” arXiv pre print\\narXiv:2002.05202 (2020).\\n[38] https://seed1-6-embedding.github.io/\\n[39] Huang, Junqin, Zhongjie Hu, Zihao Jing, Mengya Gao, and Yichao Wu. ”Pic-\\ncolo2: General text embedding with multi-task hybrid loss training.” a rXiv preprint\\narXiv:2405.06932 (2024).\\n[40] Sun, Yifan, Changmao Cheng, Yuhan Zhang, Chi Zhang, Liang Z heng, Zhongdao\\nWang, and Yichen Wei. ”Circle loss: A uniﬁed perspective of pair similarit y op-\\ntimization.” In Proceedings of the IEEE/CVF conference on comput er vision and',\n",
       " 'Wang, and Yichen Wei. ”Circle loss: A uniﬁed perspective of pair similarit y op-\\ntimization.” In Proceedings of the IEEE/CVF conference on comput er vision and\\npattern recognition, pp. 6398-6407. 2020.\\n19',\n",
       " 'QZhou-Embedding Technical Report\\n Kingsoft AI\\n[41] Rodrigo Nogueira, Wei Yang, Jimmy Lin, and Kyunghyun Cho. 201 9. Document\\nexpansion by query prediction. ArXiv preprint, abs/1904.08375.\\n[42] Liang Wang, Nan Yang, and Furu Wei. 2023. Query2doc: Query e xpansion with\\nlarge language models. In Proceedings of the 2023 Conference on E mpirical Meth-\\nods in Natural Language Processing, pages 9414–9423, Singapor e. Association for\\nComputational Linguistics.\\n[43] Zhuyun Dai, Vincent Y Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, An ton Bakalov,\\nKelvin Guu, Keith Hall, and Ming-Wei Chang. 2022. Promptagator: Fe wshot dense\\nretrieval from 8 examples. In The Eleventh International Confer ence on Learning\\nRepresentations.\\n[44] Kexin Wang, Nandan Thakur, Nils Reimers, and Iryna Gurevych. 2022a. GPL:\\nGenerative pseudo labeling for unsupervised domain adaptation of d ense retrieval.\\nIn Proceedings of the 2022 Conference of the North American Cha pter of the',\n",
       " 'Generative pseudo labeling for unsupervised domain adaptation of d ense retrieval.\\nIn Proceedings of the 2022 Conference of the North American Cha pter of the\\nAssociation for Computational Linguistics: Human Language Techn ologies, pages\\n2345–2360, Seattle, United States. Association for Computation al Linguistics.\\n[45] Honovich, Or, Thomas Scialom, Omer Levy, and Timo Schick. ”Unn atural in-\\nstructions: Tuning language models with (almost) no human labor.” ar Xiv preprint\\narXiv:2212.09689 (2022).\\n[46] Xiong, Lee, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Pau l Bennett,\\nJunaid Ahmed, and Arnold Overwijk. ”Approximate nearest neighbo r negative con-\\ntrastive learning for dense text retrieval.” arXiv preprint arXiv:200 7.00808 (2020).\\n[47] Moreira, Gabriel de Souza P., Radek Osmulski, Mengyao Xu, Rona y Ak, Benedikt\\nSchiﬀerer, and Even Oldridge. ”NV-Retriever: Improving text emb edding models\\nwith eﬀective hard-negative mining.” arXiv preprint arXiv:2407.15831 (2024).',\n",
       " 'Schiﬀerer, and Even Oldridge. ”NV-Retriever: Improving text emb edding models\\nwith eﬀective hard-negative mining.” arXiv preprint arXiv:2407.15831 (2024).\\n[48] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representatio n learning with\\ncontrastive predictive coding. arXiv preprint arXiv:1807.03748, 20 18.\\n[49] https://www.kexue.fm/archives/8847\\n[50] Xin Zhang, Yanzhao Zhang, Dingkun Long, Wen Xie, Ziqi Dai, Jialon g Tang, Huan\\nLin, Baosong Yang, Pengjun Xie, Fei Huang, Meishan Zhang, Wenjie Li, and Min\\nZhang. mgte: Generalized long-context text representation and reranking models\\nfor multilingual text retrieval, 2024.\\n[51] Lee, Jinhyuk, Zhuyun Dai, Xiaoqi Ren, Blair Chen, Daniel Cer, Je remy R. Cole,\\nKai Hui et al. ”Gecko: Versatile text embeddings distilled from large la nguage\\nmodels, 2024.” URL https://arxiv. org/abs/2403.20327.\\n[52] Junseong Kim, Seolhwa Lee, Jihoon Kwon, Sangmo Gu, Yejin Kim, M inkyung',\n",
       " 'models, 2024.” URL https://arxiv. org/abs/2403.20327.\\n[52] Junseong Kim, Seolhwa Lee, Jihoon Kwon, Sangmo Gu, Yejin Kim, M inkyung\\nCho, Jy yong Sohn, and Chanyeol Choi. Linq-embed-mistral: Elevat ing text re-\\ntrieval with improved gpt data through task-speciﬁc control and quality reﬁnement.\\nlinq ai research blog, 2024.\\n[53] https://huggingface.co/dunzhang/stella-large-zh-v3-1792d\\n20',\n",
       " 'QZhou-Embedding Technical Report\\n Kingsoft AI\\n[54] Tsatsaronis G, Balikas G, Malakasiotis P, et al. An overview of the BIOASQ large-\\nscale biomedical semantic indexing and question answering competitio n[J]. BMC\\nbioinformatics, 2015, 16(1): 138.\\n[55] Cui Y, Liu T, Che W, et al. A span-extraction dataset for Chines e machine reading\\ncomprehension[J]. arXiv preprint arXiv:1810.07366, 2018.\\n[56] Wang A, Singh A, Michael J, et al. GLUE: A multi-task benchmark a nd analysis\\nplatform for natural language understanding[J]. arXiv preprint ar Xiv:1804.07461,\\n2018.\\n[57] Yelp Dataset. Yelp Inc., [Year]. Available: https://www.yelp.com/dataset\\n[58] Maas A, Daly R E, Pham P T, et al. Learning word vectors for sent iment analy-\\nsis[C]//Proceedings of the 49th annual meeting of the association f or computational\\nlinguistics: Human language technologies. 2011: 142-150.\\n[59] Jack FitzGerald, Christopher Hench, Charith Peris, Scott Mac kie, Kay Rottmann,',\n",
       " 'linguistics: Human language technologies. 2011: 142-150.\\n[59] Jack FitzGerald, Christopher Hench, Charith Peris, Scott Mac kie, Kay Rottmann,\\nAna Sanchez, Aaron Nash, Liam Urbach, Vishesh Kakarala, Richa Sin gh, Swetha\\nRanganath, Laurie Crist, Misha Britan, Wouter Leeuwis, Gokhan Tu r, and Prem\\nNatarajan. 2022. Massive: A 1m-example multilingual natural langu age understand-\\ning dataset with 51 typologically-diverse languages.\\n[60] Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre . 2012. Semeval-\\n2012 task 6: A pilot on semantic textual similarity. In * SEM 2012: The First\\nJoint Conference on Lexical and Computational Semantics–Volume 1: Proceedings\\nof the main conference and the shared task, and Volume 2: Procee dings of the Sixth\\nInternational Workshop on Semantic Evaluation (SemEval 2012), pages 385–393.\\n[61] Liu, Xin, Qingcai Chen, Chong Deng, Huajun Zeng, Jing Chen, Do ngfang Li,\\nand Buzhou Tang. ”Lcqmc: A large-scale chinese question matching corpus.” In',\n",
       " '[61] Liu, Xin, Qingcai Chen, Chong Deng, Huajun Zeng, Jing Chen, Do ngfang Li,\\nand Buzhou Tang. ”Lcqmc: A large-scale chinese question matching corpus.” In\\nProceedings of the 27th international conference on computatio nal linguistics, pp.\\n1952-1962. 2018.\\n[62] Yang, Yinfei, Yuan Zhang, Chris Tar, and Jason Baldridge. ”PAW S-X: A\\ncross-lingual adversarial dataset for paraphrase identiﬁcation .” arXiv preprint\\narXiv:1908.11828 (2019).\\n[63] Cer, Daniel, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and L ucia Specia.\\n”Semeval-2017 task 1: Semantic textual similarity-multilingual and c ross-lingual\\nfocused evaluation.” arXiv preprint arXiv:1708.00055 (2017).\\n[64] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh T iwary, Rangan\\nMajumder, and Li Deng. 2016. MS MARCO: A human generated mach ine read-\\ning comprehension dataset. In Proceedings of the Workshop on Co gnitive Com-\\nputation: Integrating neural and symbolic approaches 2016 co-lo cated with the',\n",
       " 'ing comprehension dataset. In Proceedings of the Workshop on Co gnitive Com-\\nputation: Integrating neural and symbolic approaches 2016 co-lo cated with the\\n30th Annual Conference on Neural Information Processing Syst ems (NIPS 2016),\\nBarcelona, Spain, December 9, 2016, volume 1773 of CEUR Worksho p Proceedings.\\nCEUR-WS.org.\\n21',\n",
       " 'QZhou-Embedding Technical Report\\n Kingsoft AI\\n[65] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redﬁeld, Michael Collins , Ankur\\nParikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Ke nton Lee,\\net al. Natural questions: a benchmark for question answering res earch. Transactions\\nof the Association for Computational Linguistics, 7:453–466, 2019 .\\n[66] Angela Fan, Yacine Jernite, Ethan Perez, David Grangier, Jaso n Weston, and\\nMichael Auli. 2019. ELI5: Long Form Question Answering. In Procee dings of\\nthe 57th Annual Meeting of the Association for Computational Ling uistics, pages\\n3558–3567, Florence, Italy. Association for Computational Lingu istics.\\n[67] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan\\nSalakhutdinov, and Christopher D. Manning. HotpotQA: A dataset for diverse,\\nexplainable multi-hop question answering. In Proceedings of the 201 8 Conference\\non Empirical Methods in Natural Language Processing, pp. 2369–2 380, Brussels,',\n",
       " 'explainable multi-hop question answering. In Proceedings of the 201 8 Conference\\non Empirical Methods in Natural Language Processing, pp. 2369–2 380, Brussels,\\nBelgium, October-November 2018. Association for Computational Linguistics. doi:\\n10.18653/v1/D18-1259. URL https://aclanthology.org/D18-125 9.\\n[68] Xinyu Zhang, Nandan Thakur, Odunayo Ogundepo, Ehsan Kama lloo, David\\nAlfonso-Hermelo, Xiaoguang Li, Qun Liu, Mehdi Rezagholizadeh, and Jimmy Lin.\\nMiracl: A multilingual retrieval dataset covering 18 diverse language s. Transactions\\nof the Association for Computational Linguistics, 11:1114–1131, 2 023.\\n[69] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Per cy Liang.\\nSquad: 100,000+ questions for machine comprehension of text. ar Xiv preprint\\narXiv:1606.05250, 2016.\\n[70] James Thorne, Andreas Vlachos, Christos Christodoulopoulos , and Arpit Mit-\\ntal. Fever: a large-scale dataset for fact extraction and veriﬁca tion. arXiv preprint\\narXiv:1803.05355, 2018.',\n",
       " '[70] James Thorne, Andreas Vlachos, Christos Christodoulopoulos , and Arpit Mit-\\ntal. Fever: a large-scale dataset for fact extraction and veriﬁca tion. arXiv preprint\\narXiv:1803.05355, 2018.\\n[71] Wei He, Kai Liu, Jing Liu, Yajuan Lyu, Shiqi Zhao, Xinyan Xiao, Yu an Liu,\\nYizhong Wang, Hua Wu, Qiaoqiao She, Xuan Liu, Tian Wu, and Haifeng Wa ng.\\n2018. DuReader: a Chinese Machine Reading Comprehension Datase t from Real-\\nworld Applications. In Proceedings of the Workshop on Machine Read ing for Ques-\\ntion Answering, pages 37–46, Melbourne, Australia. Association fo r Computational\\nLinguistics.\\n[72] Yichen Jiang, Shikha Bordia, Zheng Zhong, Charles Dognin, Mane esh Singh, and\\nMohit Bansal. 2020. HoVer: A Dataset for Many-Hop Fact Extract ion And Claim\\nVeriﬁcation. In Findings of the Association for Computational Lingu istics: EMNLP\\n2020, pages 3441–3460, Online. Association for Computational Lin guistics.\\n[73] Zhang X, Ma X, Shi P, et al. Mr. TyDi: A multi-lingual benchmark fo r dense',\n",
       " '2020, pages 3441–3460, Online. Association for Computational Lin guistics.\\n[73] Zhang X, Ma X, Shi P, et al. Mr. TyDi: A multi-lingual benchmark fo r dense\\nretrieval[J]. arXiv preprint arXiv:2108.08787, 2021.\\n[74] Kyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Danie l Weld. 2020.\\nS2ORC: The Semantic Scholar Open Research Corpus. In Proceedin gs of the 58th\\nAnnual Meeting of the Association for Computational Linguistics, p ages 4969–4983,\\nOnline. Association for Computational Linguistics.\\n22',\n",
       " 'QZhou-Embedding Technical Report\\n Kingsoft AI\\n[75] https://huggingface.co/spaces/mteb/leaderboard\\n[76] Jinhyuk Lee, Feiyang Chen, Sahil Dua, Daniel Cer, Madhuri Sha nbhogue, Iftekhar\\nNaim, Gustavo Hernandez /acute.ts1Abrego, Zhe Li, Kaifeng Chen, Henrique Schechter\\nVera, et al. Gemini embedding: Generalizable embeddings from gemini. arXiv\\npreprint arXiv:2503.07891, 2025b.\\nA Appendix\\nA.1 Framework Constraints\\nTable 4: Speciﬁcations of framework constraints\\nItem Explanation\\nKeep core semantics Preserving the core semantic content, which is the\\nmost critical requirement.\\nDiversity in morphology,\\nsyntax, grammar, tense,\\nrhetoric, etc\\nVariations in lexical composition, syntactic struc-\\nture, grammatical rules, and tense usage are per-\\nmitted.\\nLength within ±15% The length deviation from the original sentence\\nshould not exceed 15%.\\nKeep language The language used must be consistent with the\\noriginal sentence.\\nClose in ﬁeld The content must remain strictly aligned with the',\n",
       " 'should not exceed 15%.\\nKeep language The language used must be consistent with the\\noriginal sentence.\\nClose in ﬁeld The content must remain strictly aligned with the\\ndomain of the given sentence.\\nTopic transfer, expansion,\\nextension, prohibiting pure\\nrewriting\\nTopic shifting, extension, or elaboration is permit-\\nted, but purely paraphrased content (identical to\\nthe original topic) is prohibited.\\nPOS is the perfect\\nanswer(necessary &\\nsuﬃcient)\\nPositive examples must be unambiguous and pre-\\ncisely address the query (necessity condition) while\\ncontaining exclusively relevant content without ex-\\ntraneous information (suﬃciency condition).\\nHard NEG: Worse than\\nPOS:\\n- Semantic deviation\\n(inadequate)\\n- Including irrelevant\\ninformation(unnecessary)\\n- Diﬀerent aspects of the\\nsame topic\\nHard negative examples must exhibit inferior qual-\\nity compared to positive instances, with noise in-\\ntroduced through three strategies: 1) semantic de-\\nviation (failing to accurately address the query),',\n",
       " 'ity compared to positive instances, with noise in-\\ntroduced through three strategies: 1) semantic de-\\nviation (failing to accurately address the query),\\n2) incorporation of irrelevant information, or 3)\\nmaintaining the same topic but diverging in as-\\npects.\\nImitation: syntax, sentence\\nstructure, structural\\nGenerating hard negative examples by emulating\\nthe structural and syntactic patterns of the given\\npositive instance is a critical step to maximize dis-\\ncriminative challenge for the model.\\n23',\n",
       " 'QZhou-Embedding Technical Report\\n Kingsoft AI\\nA.2 Instruction Examples\\nTable 5: Instruction for partial training data\\nDataset Instruction\\nHuatuo Given a medical question, retrieve user replies that\\nbest answer the question\\nReddit Retrieve the paragraph most semantically similar\\nto the given statement\\nLaw-GPT Retrieve relevant legal provisions or interpreta-\\ntions for the given case\\nMNLI/SNLI Retrieve semantically similar text\\nYelp Classify the customer review of businesses\\nWeibo Classify the sentiment of Weibo comments\\nA.3 Data Synthesis Examples\\nNote: The text highlighted in yellow represents the original sentence, fo llowed by the\\nsynthetically generated sentence.\\nTable 6: Paraphrasing Example (1)\\nquery pos\\nWhat is the best credit\\ncard for someone with no\\ncredit history?\\nIf you’ve never had a credit card before a likely\\nreason can be due to lack of credit history. You\\ncan apply for a department store card.\\nWhat’s the ideal credit\\ncard for a person without\\nany credit history?',\n",
       " 'reason can be due to lack of credit history. You\\ncan apply for a department store card.\\nWhat’s the ideal credit\\ncard for a person without\\nany credit history?\\nIf you’ve never had a credit card, it’s probably\\nbecause you don’t have a credit history. A depart-\\nment store card could be a good option to apply\\nfor.\\nWhat’s the top credit card\\nchoice for someone who has\\nno credit history?\\nIf you’ve never owned a credit card, it’s probably\\nbecause you don’t have a credit history. A depart-\\nment store card might be a good option to con-\\nsider.\\n24',\n",
       " 'QZhou-Embedding Technical Report\\n Kingsoft AI\\nTable 7: Paraphrasing Example (2)\\nquery pos\\nWhich English Poet\\nLaureate wrote ’The Faerie\\nQueene’?\\nEnglish Renaissance to begin, shakily, in the 1520s,\\nand it continued until perhaps 1620. England had\\na strong tradition of literature in the English ver-\\nnacular, which gradually increased as English use\\nof the printing press became common during the\\nmid 16th century. By the time of Elizabethan liter-\\nature a vigorous literary culture in both drama and\\npoetry included poets such as Edmund Spenser,\\nwhose verse epic ’The Faerie Queene’ had a strong\\ninﬂuence on English literature but was eventu-\\nally overshadowed by the lyrics of William Shake-\\nspeare, Thomas Wyatt and others. Typically, the\\nworks of these playwrights and poets circulated in\\nmanuscript form.\\nWho was the English Poet\\nLaureate that penned ’The\\nFaerie Queene’?\\nThe English Renaissance began, albeit slowly, in\\nthe 1520s and likely lasted until around 1620. Eng-',\n",
       " 'manuscript form.\\nWho was the English Poet\\nLaureate that penned ’The\\nFaerie Queene’?\\nThe English Renaissance began, albeit slowly, in\\nthe 1520s and likely lasted until around 1620. Eng-\\nland boasted a robust literary tradition in the En-\\nglish language, which expanded as the printing\\npress became more widely adopted during the mid-\\n16th century. By the time of Elizabethan litera-\\nture, a dynamic literary scene in both drama and\\npoetry emerged, featuring writers like Edmund\\nSpenser, whose epic poem The Faerie Queene sig-\\nniﬁcantly inﬂuenced English literature, although\\nit was eventually overshadowed by the works of\\nWilliam Shakespeare, Thomas Wyatt, and others.\\nGenerally, the writings of these poets and play-\\nwrights circulated in manuscript form.\\nWhich English Poet\\nLaureate authored ’The\\nFaerie Queene’?\\nThe English Renaissance began somewhat uncer-\\ntainly in the 1520s, continuing through to around\\n1620. England had a rich literary heritage in its',\n",
       " 'Laureate authored ’The\\nFaerie Queene’?\\nThe English Renaissance began somewhat uncer-\\ntainly in the 1520s, continuing through to around\\n1620. England had a rich literary heritage in its\\nnative tongue, which expanded as the printing\\npress became more widely used in the mid-16th\\ncentury. By the Elizabethan era, a thriving lit-\\nerary culture in both drama and poetry emerged,\\nwith poets like Edmund Spenser, whose epic The\\nFaerie Queene profoundly impacted English liter-\\nature, though it was eventually eclipsed by the\\nworks of William Shakespeare, Thomas Wyatt,\\nand others. Generally, the writings of these play-\\nwrights and poets were circulated in manuscript\\nform.\\n25',\n",
       " 'QZhou-Embedding Technical Report\\n Kingsoft AI\\nTable 8: Augmentation Example\\nquery pos neg\\nWhat is the best\\ncredit card for\\nsomeone with no\\ncredit history?\\nIf you’ve never had a credit\\ncard before a likely reason\\ncan be due to lack of credit\\nhistory. You can apply for a\\ndepartment store card.\\n-\\nWhich credit card is\\neasiest to get\\napproved for with bad\\ncredit?\\nFor those with poor credit\\nscores, secured credit cards\\nfrom Capital One or Dis-\\ncover are often the most ac-\\ncessible options since they\\nrequire a refundable de-\\nposit.\\nSome premium travel cards\\nlike Chase Sapphire have\\ngreat rewards but usually\\nneed excellent credit. Store\\ncards may accept lower\\nscores but have high inter-\\nest rates.\\nWhat credit cards\\ncan I get as a college\\nstudent with no\\ncredit?\\nStudents without credit his-\\ntory often qualify for starter\\ncards like Discover it Stu-\\ndent or Capital One Jour-\\nney, which are designed to\\nhelp build credit.\\nPremium cards like Amer-\\nican Express Platinum re-',\n",
       " 'tory often qualify for starter\\ncards like Discover it Stu-\\ndent or Capital One Jour-\\nney, which are designed to\\nhelp build credit.\\nPremium cards like Amer-\\nican Express Platinum re-\\nquire good credit history,\\nthough some banks oﬀer\\nstudent accounts with debit\\ncards.\\nWhich English Poet\\nLaureate wrote ’The\\nFaerie Queene’?\\n...By the time of Eliz-\\nabethan literature a vig-\\norous literary culture in\\nboth drama and poetry in-\\ncluded poets such as Ed-\\nmund Spenser, whose verse\\nepic ’The Faerie Queene’\\nhad a strong inﬂuence on\\nEnglish literature but was\\neventually overshadowed by\\nthe lyrics of William ...\\n-\\nWhat major epic\\npoem did Edmund\\nSpenser write during\\nQueen Elizabeth’s\\nreign?\\nEdmund Spenser composed\\n’The Faerie Queene’, an\\nallegorical epic poem that\\nbecame one of the most\\nsigniﬁcant works of Eliz-\\nabethan literature though\\nlater eclipsed by Shake-\\nspeare’s popularity.\\nChristopher Marlowe’s\\n’Hero and Leander’ was an-\\nother notable Elizabethan\\npoem, but unlike Spenser’s',\n",
       " 'signiﬁcant works of Eliz-\\nabethan literature though\\nlater eclipsed by Shake-\\nspeare’s popularity.\\nChristopher Marlowe’s\\n’Hero and Leander’ was an-\\nother notable Elizabethan\\npoem, but unlike Spenser’s\\nwork it wasn’t an epic\\nallegory.\\nWhich poet created\\n’Paradise Lost’ during\\nthe English\\nRenaissance?\\nJohn Milton authored the\\nepic poem ’Paradise Lost’\\nin the 17th century, a mon-\\numental work that explored\\nbiblical themes through\\nblank verse and became\\na cornerstone of English\\nliterature.\\nWilliam Blake’s ’The Mar-\\nriage of Heaven and Hell’\\nalso dealt with religious\\nthemes, though it was more\\nprophetic than epic in style\\ncompared to Milton’s mas-\\nterpiece.\\n26',\n",
       " 'QZhou-Embedding Technical Report\\n Kingsoft AI\\nTable 9: Hard-Negative Generation Example\\nquery pos neg\\nWhat territory was\\nKing Hussein afraid\\nIsrael would obtain?\\n...Hussein was nonetheless\\nwary that an Egyptian-\\nIsraeli war would risk the\\nWest Bank’s occupation by\\nIsrael...\\n-\\nWhat territory was\\nKing Hussein afraid\\nIsrael would obtain?\\n...Hussein was nonetheless\\nwary that an Egyptian-\\nIsraeli war would risk the\\nWest Bank’s occupation by\\nIsrael...\\nKing Hussein expressed\\nconcerns about potential\\nIsraeli expansion during\\nthe Arab-Israeli conﬂicts,\\nthough his warnings to\\nNasser were delayed and\\ninitially dismissed, while\\nother Arab leaders focused\\nmore on direct military\\npreparations against Israel.\\nWhat territory was\\nKing Hussein afraid\\nIsrael would obtain?\\n...Hussein was nonetheless\\nwary that an Egyptian-\\nIsraeli war would risk the\\nWest Bank’s occupation by\\nIsrael...\\nKing Hussein expressed\\nconcerns about potential\\nIsraeli territorial expansion\\nduring the 1967 tensions,',\n",
       " 'wary that an Egyptian-\\nIsraeli war would risk the\\nWest Bank’s occupation by\\nIsrael...\\nKing Hussein expressed\\nconcerns about potential\\nIsraeli territorial expansion\\nduring the 1967 tensions,\\nthough his warnings were\\ndelayed in reaching Nasser\\nand mixed with broader\\nregional tensions, while\\nEgyptian military move-\\nments in Sinai were already\\nunderway under Amer’s\\norders.\\n27',\n",
       " 'Title: A Survey of Object Detection: From Region Proposals to End-to-End \\nTransformers \\nAuthor: [Your Name/Institution] Date: October 2025 \\n \\nAbstract \\nObject detection, a fundamental task in computer vision, involves identifying and localizing \\ninstances of objects within an image or video. It goes beyond simple image classification by not only \\ndetermining the class of an object but also providing a bounding box that precisely outlines its \\nlocation. This paper provides a comprehensive survey of the evolution of object detection \\nmethodologies, primarily focusing on the deep learning era. We begin by contextualizing the \\nproblem with a brief overview of traditional computer vision techniques. The core of the review is \\ndedicated to the two dominant paradigms in deep learning-based detectors: two-stage detectors, \\nexemplified by the R-CNN family (R-CNN, Fast R-CNN, Faster R-CNN), which prioritize accuracy',\n",
       " 'dedicated to the two dominant paradigms in deep learning-based detectors: two-stage detectors, \\nexemplified by the R-CNN family (R-CNN, Fast R-CNN, Faster R-CNN), which prioritize accuracy \\nthrough a region proposal mechanism; and single-stage detectors, such as YOLO and SSD, which \\noptimize for speed by performing detection in a single pass. We then explore key architectural \\ncomponents like backbone networks, anchor boxes, and non-maximum suppression. The survey \\nculminates with a discussion of modern architectures, including the paradigm-shifting DETR \\n(DEtection TRansformer), which reframes object detection as an end-to-end set prediction problem. \\nFinally, we cover standard evaluation metrics, common datasets, real-world applications, and the \\nongoing challenges and future directions that are shaping the field. \\nKeywords: Object Detection, Computer Vision, Deep Learning, R-CNN, YOLO, SSD, Transformer, \\nDETR, Bounding Box, mAP.',\n",
       " 'Table of Contents \\n1. Introduction 1.1. Defining Object Detection: Classification and Localization 1.2. Distinction \\nfrom Other Vision Tasks 1.3. The Importance of Object Detection 1.4. Paper Structure \\n2. Background and Foundational Concepts 2.1. Traditional Computer Vision Approaches \\n(Viola-Jones, HOG) 2.2. The Sliding Window Method 2.3. The Deep Learning Revolution \\n3. Two-Stage Object Detectors: A Focus on Accuracy 3.1. The \"Propose, then Classify\" \\nParadigm 3.2. R-CNN: Regions with CNN Features 3.3. Fast R-CNN: Sharing Computation \\n3.4. Faster R-CNN: The Region Proposal Network (RPN) \\n4. Single-Stage Object Detectors: A Focus on Speed 4.1. The \"Single Pass\" Paradigm 4.2. \\nYOLO: You Only Look Once 4.3. SSD: Single Shot MultiBox Detector \\n5. Key Architectural Components and Innovations 5.1. Backbone Networks: The Feature \\nExtractors 5.2. Anchor Boxes: The Priors for Prediction 5.3. Non-Maximum Suppression \\n(NMS): Pruning Redundant Detections',\n",
       " 'Extractors 5.2. Anchor Boxes: The Priors for Prediction 5.3. Non-Maximum Suppression \\n(NMS): Pruning Redundant Detections \\n6. Modern Architectures and the Rise of Transformers 6.1. Balancing Speed and Accuracy: \\nEfficientDet 6.2. DETR: End-to-End Object Detection with Transformers \\n7. Evaluation Metrics and Datasets 7.1. Intersection over Union (IoU) 7.2. Average Precision \\n(AP) and mean Average Precision (mAP) 7.3. Landmark Datasets (PASCAL VOC, COCO) \\n8. Applications and Real-World Use Cases 8.1. Autonomous Vehicles 8.2. Medical Imaging \\n8.3. Retail and Inventory Management 8.4. Security and Surveillance \\n9. Challenges and Future Directions 9.1. Detecting Small and Occluded Objects 9.2. The \\nSpeed vs. Accuracy Trade-off 9.3. Domain Adaptation and Generalization 9.4. Few-Shot and \\nZero-Shot Detection \\n10. Conclusion \\n11. References',\n",
       " '1. Introduction \\n1.1. Defining Object Detection: Classification and Localization \\nObject detection is a core computer vision task concerned with answering two fundamental questions \\nabout an image: \"What objects are in this image?\" and \"Where are they located?\". The first \\nquestion is a classification task, assigning a class label (e.g., \"cat,\" \"car,\" \"person\") to an object. The \\nsecond is a localization task, providing a tight-fitting bounding box (typically defined by x/y \\ncoordinates and width/height) around each identified object. \\n1.2. Distinction from Other Vision Tasks \\nIt is crucial to distinguish object detection from related tasks: \\n• Image Classification: Simply assigns one label to an entire image (e.g., \"this is a picture of a \\ncat\"). \\n• Semantic Segmentation: Assigns a class label to every pixel in the image but does not \\ndistinguish between different instances of the same object (e.g., all pixels belonging to any \\nperson are labeled \"person\").',\n",
       " 'distinguish between different instances of the same object (e.g., all pixels belonging to any \\nperson are labeled \"person\"). \\n• Instance Segmentation: Assigns a class label to every pixel and differentiates between \\nobject instances (e.g., \"person 1,\" \"person 2,\" \"person 3\"). Object detection can be seen as a \\nprecursor to this more complex task. \\n1.3. The Importance of Object Detection \\nThe ability to detect and locate objects is foundational to how machines perceive and interact with \\nthe physical world. It is the technology that enables self-driving cars to see pedestrians and other \\nvehicles, allows doctors to identify tumors in medical scans, and helps robots navigate complex \\nenvironments. Its broad applicability has made it one of the most actively researched areas in \\nartificial intelligence. \\n1.4. Paper Structure \\nThis paper will trace the evolution of object detection methods, beginning with a brief look at pre-',\n",
       " 'artificial intelligence. \\n1.4. Paper Structure \\nThis paper will trace the evolution of object detection methods, beginning with a brief look at pre-\\ndeep learning techniques. We will then delve into the two primary families of deep learning',\n",
       " 'detectors: two-stage and single-stage. We will discuss their core components, modern architectures \\nincluding Transformers, and conclude with evaluation metrics, applications, and future challenges. \\n \\n2. Background and Foundational Concepts \\n2.1. Traditional Computer Vision Approaches \\nBefore deep learning, object detection relied on hand-crafted features. Methods like the Viola-Jones \\nframework (famous for real-time face detection) used simple Haar-like features and a cascade of \\nclassifiers. Other approaches used more complex feature descriptors like HOG (Histogram of \\nOriented Gradients), often paired with a classifier like a Support Vector Machine (SVM), to \\nidentify objects. These methods were effective for specific tasks but were brittle and did not \\ngeneralize well. \\n2.2. The Sliding Window Method \\nA common technique was the sliding window approach. A window of a fixed size would be slid',\n",
       " 'generalize well. \\n2.2. The Sliding Window Method \\nA common technique was the sliding window approach. A window of a fixed size would be slid \\nacross all possible locations and scales of an image. For each window, a feature descriptor would be \\ncomputed and fed to a classifier. This method was computationally exhaustive and prone to errors. \\n2.3. The Deep Learning Revolution \\nThe success of AlexNet in the 2012 ImageNet classification challenge marked a turning point. \\nResearchers quickly realized that the rich, hierarchical features learned automatically by \\nConvolutional Neural Networks (CNNs) were far more powerful than any hand-crafted features. \\nThis discovery paved the way for the modern era of object detection. \\n \\n3. Two-Stage Object Detectors: A Focus on Accuracy \\nTwo-stage detectors break the object detection problem into two distinct steps, a paradigm that \\ngenerally leads to higher accuracy at the cost of speed. \\n3.1. The \"Propose, then Classify\" Paradigm',\n",
       " \"The core idea is to first generate a sparse set of region proposals—areas of the image that are likely \\nto contain an object. In the second stage, a classifier is run only on these proposed regions to \\ndetermine the object's class and refine the bounding box. \\n3.2. R-CNN: Regions with CNN Features \\nR-CNN was the first major breakthrough in applying deep learning to this paradigm. However, its \\nprocess was slow and cumbersome: \\n1. Generate ~2000 region proposals using an external algorithm like Selective Search. \\n2. Warp/resize each proposed region to a fixed size. \\n3. Pass each warped region independently through a pre-trained CNN to extract features. \\n4. Use a set of SVMs to classify the object in each region. \\n3.3. Fast R-CNN: Sharing Computation \\nFast R-CNN made a significant improvement. Instead of running the CNN 2000 times, it passes the \\nentire image through the CNN just once to generate a feature map. The region proposals are then\",\n",
       " 'Fast R-CNN made a significant improvement. Instead of running the CNN 2000 times, it passes the \\nentire image through the CNN just once to generate a feature map. The region proposals are then \\nprojected onto this feature map. A novel RoI (Region of Interest) Pooling layer extracts a fixed-size \\nfeature vector from each proposed region, which is then fed into a classifier. This shared computation \\nmade the process much faster. \\n3.4. Faster R-CNN: The Region Proposal Network (RPN) \\nThe bottleneck in Fast R-CNN was the external Selective Search algorithm for proposing regions. \\nFaster R-CNN introduced the Region Proposal Network (RPN), a small neural network that learns \\nto generate high-quality region proposals directly from the CNN features. By integrating the RPN, \\nFaster R-CNN became the first truly end-to-end, unified deep learning object detector, setting a new \\nstandard for accuracy. \\n \\n4. Single-Stage Object Detectors: A Focus on Speed',\n",
       " 'Faster R-CNN became the first truly end-to-end, unified deep learning object detector, setting a new \\nstandard for accuracy. \\n \\n4. Single-Stage Object Detectors: A Focus on Speed \\nSingle-stage detectors remove the region proposal step and instead perform localization and \\nclassification in a single forward pass of the network, making them extremely fast and suitable for \\nreal-time applications.',\n",
       " '4.1. The \"Single Pass\" Paradigm \\nThese models treat object detection as a regression problem. They look at the image once and \\ndirectly predict a set of bounding boxes and their corresponding class probabilities. \\n4.2. YOLO: You Only Look Once \\nThe YOLO family of models is renowned for its speed. YOLO divides the input image into a grid. \\nFor each grid cell, the model simultaneously predicts: \\n• Several bounding boxes. \\n• A \"confidence\" score for each box, indicating how likely it is to contain an object. \\n• Class probabilities for the object within the box. This unified architecture allows for end-to-\\nend training and blazingly fast inference speeds, making it ideal for video processing. \\n4.3. SSD: Single Shot MultiBox Detector \\nSSD aimed to find a middle ground between the speed of YOLO and the accuracy of Faster R-CNN. \\nIts key innovation is using feature maps from multiple layers of the backbone network to make',\n",
       " 'SSD aimed to find a middle ground between the speed of YOLO and the accuracy of Faster R-CNN. \\nIts key innovation is using feature maps from multiple layers of the backbone network to make \\npredictions. By making predictions at different scales, SSD is much better at detecting objects of \\nvarious sizes, particularly small ones, compared to the original YOLO. \\n \\n5. Key Architectural Components and Innovations \\nModern detectors, whether two-stage or single-stage, share several common components. \\n5.1. Backbone Networks: The Feature Extractors \\nThe backbone is a deep CNN (like ResNet, VGG, or MobileNet) pre-trained on a large image \\nclassification dataset (e.g., ImageNet). Its role is to act as a powerful feature extractor, converting the \\nraw pixel data of an image into rich, hierarchical feature maps that can be used for detection. \\n5.2. Anchor Boxes: The Priors for Prediction \\nInstead of predicting bounding boxes from scratch, most detectors predict offsets relative to a set of',\n",
       " '5.2. Anchor Boxes: The Priors for Prediction \\nInstead of predicting bounding boxes from scratch, most detectors predict offsets relative to a set of \\npre-defined default boxes called anchor boxes. These anchors have various sizes and aspect ratios',\n",
       " 'and are tiled across the image at different locations. Using anchors reframes the problem from \\npredicting absolute coordinates to refining a well-placed prior, which makes learning easier for the \\nnetwork. \\n5.3. Non-Maximum Suppression (NMS): Pruning Redundant Detections \\nA detector will often output multiple, highly overlapping bounding boxes for the same object. NMS \\nis a crucial post-processing step that cleans up these redundant detections. It sorts all boxes by their \\nconfidence scores, keeps the box with the highest score, and suppresses (discards) any other boxes \\nthat have a high overlap with it. \\n \\n6. Modern Architectures and the Rise of Transformers \\n6.1. Balancing Speed and Accuracy: EfficientDet \\nThe EfficientDet family of models introduced a systematic way to scale detectors for different \\nresource constraints. It uses a highly efficient backbone (EfficientNet) and a novel feature fusion',\n",
       " 'The EfficientDet family of models introduced a systematic way to scale detectors for different \\nresource constraints. It uses a highly efficient backbone (EfficientNet) and a novel feature fusion \\nmechanism (BiFPN) to achieve state-of-the-art efficiency, balancing high accuracy with low \\ncomputational cost. \\n6.2. DETR: End-to-End Object Detection with Transformers \\nDETR (DEtection TRansformer) represents a major paradigm shift. It completely eliminates the need \\nfor hand-crafted components like anchor boxes and NMS. DETR uses a standard Transformer \\nencoder-decoder architecture, similar to those used in NLP. It treats object detection as a direct set \\nprediction problem: the model ingests image features and directly outputs the final set of unique \\nobject detections. This simplifies the detection pipeline significantly and has opened up a new and \\nexciting research direction. \\n \\n7. Evaluation Metrics and Datasets \\n7.1. Intersection over Union (IoU)',\n",
       " 'IoU is the fundamental metric used to measure the \"correctness\" of a predicted bounding box. It is \\ncalculated as the area of overlap between the predicted box and the ground-truth box, divided by the \\narea of their union. A detection is typically considered a \"true positive\" if its IoU with a ground-truth \\nbox is above a certain threshold (e.g., 0.5). \\n7.2. Average Precision (AP) and mean Average Precision (mAP) \\nAverage Precision (AP) is the primary metric for evaluating the performance of a detector on a \\nsingle object class. It is calculated from the precision-recall curve and effectively measures the \\ndetector\\'s accuracy across all confidence levels. Mean Average Precision (mAP) is the average of \\nthe AP values across all object classes and is the standard metric for comparing different object \\ndetection models. \\n7.3. Landmark Datasets \\nThe field has been driven by large-scale, publicly available datasets, most notably PASCAL VOC',\n",
       " 'detection models. \\n7.3. Landmark Datasets \\nThe field has been driven by large-scale, publicly available datasets, most notably PASCAL VOC \\nand Microsoft COCO (Common Objects in Context). The COCO dataset, with its large number of \\nobject categories and instances per image, is the current benchmark for modern object detectors. \\n \\n8. Applications and Real-World Use Cases \\nObject detection is a deployed and impactful technology across numerous industries. \\n• Autonomous Vehicles: Detecting cars, pedestrians, cyclists, and traffic signals is essential \\nfor safe navigation. \\n• Medical Imaging: Assisting radiologists by automatically locating tumors, lesions, or other \\nanomalies in X-rays, CT scans, and MRIs. \\n• Retail: Powering cashier-less stores, monitoring shelf inventory, and analyzing customer foot \\ntraffic. \\n• Security and Surveillance: Automatically detecting intruders, unattended baggage, or \\nmonitoring crowd density. \\n \\n9. Challenges and Future Directions',\n",
       " 'Despite immense progress, several challenges remain. \\n• Detecting Small and Occluded Objects: Models still struggle to reliably detect objects that \\nare very small, far away, or partially hidden. \\n• The Speed vs. Accuracy Trade-off: While models are becoming more efficient, the \\nfundamental trade-off between real-time speed and maximum accuracy remains a key design \\nconsideration. \\n• Domain Adaptation and Generalization: A model trained on daytime, sunny weather data \\nmay fail when deployed at night or in the rain. Improving robustness to new environments is \\na major challenge. \\n• Few-Shot and Zero-Shot Detection: Training models to detect new object categories with \\nvery few (or zero) labeled examples is an active and important area of research.',\n",
       " \"10. Conclusion \\nObject detection has undergone a remarkable transformation, moving from slow, brittle systems \\nbased on hand-crafted features to highly accurate and efficient end-to-end deep learning models. The \\nevolution from the methodical two-stage R-CNN family to the rapid single-stage YOLO and SSD \\ndetectors, and now to the elegant, anchor-free Transformer-based models like DETR, showcases the \\nfield's rapid pace of innovation. As a core enabling technology for machine perception, object \\ndetection continues to solve critical real-world problems and will undoubtedly remain a central focus \\nof AI research for years to come. \\n \\n11. References \\n[Viola & Jones, 2001] Viola, P., & Jones, M. (2001). Rapid object detection using a boosted cascade \\nof simple features. Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision \\nand Pattern Recognition. \\n[Girshick et al., 2014] Girshick, R., Donahue, J., Darrell, T., & Malik, J. (2014). Rich feature\",\n",
       " 'and Pattern Recognition. \\n[Girshick et al., 2014] Girshick, R., Donahue, J., Darrell, T., & Malik, J. (2014). Rich feature \\nhierarchies for accurate object detection and semantic segmentation. Proceedings of the IEEE \\nconference on computer vision and pattern recognition. \\n[Ren et al., 2015] Ren, S., He, K., Girshick, R., & Sun, J. (2015). Faster R-CNN: Towards real-time \\nobject detection with region proposal networks. Advances in neural information processing systems. \\n[Redmon et al., 2016] Redmon, J., Divvala, S., Girshick, R., & Farhadi, A. (2016). You only look \\nonce: Unified, real-time object detection. Proceedings of the IEEE conference on computer vision \\nand pattern recognition. \\n[Liu et al., 2016] Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C. Y., & Berg, A. C. \\n(2016). SSD: Single shot multibox detector. European conference on computer vision. \\n[Carion et al., 2020] Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., & Zagoruyko, S.',\n",
       " '(2016). SSD: Single shot multibox detector. European conference on computer vision. \\n[Carion et al., 2020] Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., & Zagoruyko, S. \\n(2020). End-to-end object detection with transformers. European conference on computer vision.',\n",
       " 'Title: Self-Training with Uncertainty-Aware Style Transfer for Cross-Domain \\nObject Detection \\nPrincipal Investigator: [Your Name] Affiliation: [Your Institution/Research Group] Date: October \\n15, 2025 \\n \\nAbstract \\nModern object detection models achieve remarkable performance but suffer a significant drop in \\naccuracy when deployed in environments (target domains) that differ from their training data (source \\ndomain). This problem of domain shift is a major obstacle to the real-world application of \\ntechnologies like autonomous driving, where a vehicle must operate reliably in diverse weather, \\nlighting, and geographic conditions. This proposal outlines a research project to develop a novel \\nframework for unsupervised domain adaptation in object detection. We propose a method that \\ncombines generative style transfer with a robust self-training mechanism. Specifically, we will use a \\nCycle-Consistent Generative Adversarial Network (CycleGAN) to translate images between',\n",
       " 'combines generative style transfer with a robust self-training mechanism. Specifically, we will use a \\nCycle-Consistent Generative Adversarial Network (CycleGAN) to translate images between \\ndomains, artificially augmenting the training data. More importantly, we will enhance a self-training \\npipeline by incorporating uncertainty estimation. By using techniques like Monte Carlo Dropout, \\nour model will only leverage pseudo-labels from the target domain in which it has high confidence, \\npreventing the accumulation of errors from incorrect labels. We hypothesize that this uncertainty-\\naware approach will make the self-training process more stable and effective, leading to a significant \\nimprovement in object detection performance in unseen target domains. The proposed research will \\nbe evaluated on benchmark datasets like Cityscapes and Foggy Cityscapes, with the goal of creating \\nmore robust and reliable perception systems.',\n",
       " 'be evaluated on benchmark datasets like Cityscapes and Foggy Cityscapes, with the goal of creating \\nmore robust and reliable perception systems. \\nKeywords: Research Proposal, Object Detection, Domain Adaptation, Self-Training, Uncertainty \\nEstimation, Style Transfer, Autonomous Vehicles.',\n",
       " 'Table of Contents \\n1. Introduction and Problem Statement 1.1. The Success and Brittleness of Modern Detectors \\n1.2. The Challenge of Domain Shift 1.3. Research Questions and Objectives 1.4. Proposed \\nContribution \\n2. Literature Review 2.1. State-of-the-Art Object Detection Models 2.2. Unsupervised Domain \\nAdaptation (UDA) 2.3. UDA Techniques in Object Detection 2.3.1. Adversarial Training \\nMethods 2.3.2. Style Transfer and Image-to-Image Translation 2.3.3. Self-Training and \\nPseudo-Labeling \\n3. Proposed Methodology 3.1. Overall Framework Architecture 3.2. Module 1: Cross-Domain \\nStyle Transfer 3.3. Module 2: Self-Training with Pseudo-Labeling 3.4. The Core Innovation: \\nUncertainty-Aware Label Filtering \\n4. Experimental Setup and Evaluation 4.1. Datasets and Benchmarks 4.2. Baseline Models \\nfor Comparison 4.3. Evaluation Metrics 4.4. Implementation Details \\n5. Expected Results and Broader Impact 5.1. Hypothesized Performance Gains 5.2. Impact',\n",
       " 'for Comparison 4.3. Evaluation Metrics 4.4. Implementation Details \\n5. Expected Results and Broader Impact 5.1. Hypothesized Performance Gains 5.2. Impact \\non Autonomous Systems and Robotics 5.3. Contribution to the Field of AI \\n6. Plan of Work and Timeline 6.1. Phase 1: Literature Review and Environment Setup \\n(Months 1-2) 6.2. Phase 2: Implementation of Core Modules (Months 3-6) 6.3. Phase 3: \\nExperimentation and Analysis (Months 7-10) 6.4. Phase 4: Dissemination of Results (Months \\n11-12) \\n7. Ethical Considerations \\n8. Conclusion \\n9. References',\n",
       " '1. Introduction and Problem Statement \\n1.1. The Success and Brittleness of Modern Detectors \\nDeep learning-based object detectors like Faster R-CNN and YOLO have become incredibly \\naccurate, forming the perception backbone for many emerging technologies. However, their success \\nis predicated on the assumption that the training and testing data are drawn from the same statistical \\ndistribution. \\n1.2. The Challenge of Domain Shift \\nIn the real world, this assumption is frequently violated. A model trained exclusively on clear, sunny \\nday driving data may fail catastrophically when deployed at night, in the rain, or in a city with \\ndifferent architecture. This phenomenon is known as domain shift. Manually annotating data for \\nevery possible domain is prohibitively expensive and unscalable. Therefore, Unsupervised Domain \\nAdaptation (UDA), which aims to adapt a model trained on a labeled source domain to an unlabeled \\ntarget domain, is a critical area of research.',\n",
       " 'Adaptation (UDA), which aims to adapt a model trained on a labeled source domain to an unlabeled \\ntarget domain, is a critical area of research. \\n1.3. Research Questions and Objectives \\nThis research aims to answer the following question: How can we make an object detector robust to \\ndomain shift without requiring any labeled data from the new domain? Our primary objectives are: \\n1. To design a framework that leverages both image-level style translation and model-level self-\\ntraining. \\n2. To develop a novel mechanism to filter noisy pseudo-labels generated during self-training by \\nestimating model uncertainty. \\n3. To empirically validate the proposed framework and demonstrate its superiority over existing \\nUDA methods. \\n1.4. Proposed Contribution \\nThe main contribution of this work will be an uncertainty-aware self-training framework. While \\nself-training is a common technique, it is often unstable because the model can reinforce its own',\n",
       " 'The main contribution of this work will be an uncertainty-aware self-training framework. While \\nself-training is a common technique, it is often unstable because the model can reinforce its own \\nmistakes by trusting incorrect \"pseudo-labels.\" By introducing a principled mechanism for the model',\n",
       " 'to gauge its own uncertainty, we can select only the most reliable pseudo-labels, leading to more \\nstable and effective adaptation. \\n \\n2. Literature Review \\n2.1. State-of-the-Art Object Detection Models \\nOur work will build upon established object detection architectures. We will consider both a two-\\nstage detector (e.g., Faster R-CNN) and a Transformer-based detector (e.g., DETR) as the base \\nmodels for our adaptation framework. \\n2.2. Unsupervised Domain Adaptation (UDA) \\nUDA is a well-established field in machine learning. The central goal is to leverage a label-rich \\nsource domain to learn a task in a label-scarce target domain. \\n2.3. UDA Techniques in Object Detection \\n• Adversarial Training: These methods use a \"domain discriminator\" network that tries to \\ndistinguish between features from the source and target domains. The main network is then \\ntrained to produce features that can \"fool\" this discriminator, thereby learning domain-\\ninvariant features.',\n",
       " 'trained to produce features that can \"fool\" this discriminator, thereby learning domain-\\ninvariant features. \\n• Style Transfer: Generative models like GANs are used to translate images from the source \\nstyle to the target style (e.g., making a sunny image look foggy). This creates a synthetic \\nlabeled dataset in the target style. \\n• Self-Training: This involves using an initial model to make predictions on the unlabeled \\ntarget data. The most confident predictions are then treated as \"pseudo-labels\" and are used to \\nretrain the model. This approach is powerful but risks error accumulation if the pseudo-labels \\nare noisy. Our proposed work directly addresses this key limitation. \\n \\n3. Proposed Methodology \\n3.1. Overall Framework Architecture',\n",
       " 'The proposed system will consist of three interconnected modules operating on a base object \\ndetector. The model will be trained on labeled source data (e.g., sunny images) and unlabeled target \\ndata (e.g., rainy images). \\n3.2. Module 1: Cross-Domain Style Transfer \\nWe will first train a CycleGAN model to learn the mappings between the source and target domains. \\nThis will allow us to translate a source image into a \"fake\" target image (e.g., sunny -> rainy) and \\nvice-versa. This provides a basic form of data augmentation, allowing the detector to see labeled \\nimages that look like they are from the target domain. \\n3.3. Module 2: Self-Training with Pseudo-Labeling \\nIn parallel, we will use the model trained on the source data to generate predictions (bounding boxes \\nand classes) for the unlabeled target domain images. These predictions will serve as initial pseudo-\\nlabels. \\n3.4. The Core Innovation: Uncertainty-Aware Label Filtering',\n",
       " \"and classes) for the unlabeled target domain images. These predictions will serve as initial pseudo-\\nlabels. \\n3.4. The Core Innovation: Uncertainty-Aware Label Filtering \\nThis is the central component of our proposal. Instead of naively trusting all pseudo-labels above a \\nsimple confidence threshold, we will estimate the model's uncertainty for each prediction. We will \\nuse Monte Carlo Dropout, a technique where dropout is applied at inference time over multiple \\nforward passes. The variance in the resulting predictions serves as a strong indicator of model \\nuncertainty. We will then filter the pseudo-labels using a combined score of confidence and low \\nuncertainty. Only the most certain and confident pseudo-labels will be added to a replay buffer used \\nto fine-tune the detector, making the adaptation process robust to noise. \\n \\n4. Experimental Setup and Evaluation \\n4.1. Datasets and Benchmarks\",\n",
       " 'to fine-tune the detector, making the adaptation process robust to noise. \\n \\n4. Experimental Setup and Evaluation \\n4.1. Datasets and Benchmarks \\nWe will focus on autonomous driving scenarios. The primary experiment will be adapting from the \\nCityscapes dataset (clear weather) to the Foggy Cityscapes dataset. We will also evaluate on other \\ncommon shifts, such as adapting from synthetic data (Sim10k) to real-world data (KITTI).',\n",
       " '4.2. Baseline Models for Comparison \\nWe will compare our method against three baselines: \\n1. A \"Lower Bound\" model trained only on source data. \\n2. A state-of-the-art adversarial training method for UDA. \\n3. A standard self-training method without uncertainty awareness. \\n4.3. Evaluation Metrics \\nPerformance will be measured using the standard object detection metric, mean Average Precision \\n(mAP), calculated on the labeled validation set of the target domain. \\n \\n5. Expected Results and Broader Impact \\n5.1. Hypothesized Performance Gains \\nWe expect our uncertainty-aware framework to significantly outperform the baselines. We \\nhypothesize that by reducing the noise in the pseudo-labeling process, our model will adapt more \\neffectively, resulting in a 5-10% absolute improvement in mAP on the target domain compared to \\nstandard self-training methods. \\n5.2. Impact on Autonomous Systems and Robotics \\nA more robust perception system directly translates to increased safety and reliability for',\n",
       " 'standard self-training methods. \\n5.2. Impact on Autonomous Systems and Robotics \\nA more robust perception system directly translates to increased safety and reliability for \\nautonomous vehicles, drones, and industrial robots. This research could help bridge the gap between \\ndevelopment and real-world deployment of these technologies. \\n \\n6. Plan of Work and Timeline \\nThe project is planned for a 12-month period. \\n• Phase 1 (Months 1-2): In-depth literature review; setting up the computational environment \\nand baseline models.',\n",
       " '• Phase 2 (Months 3-6): Implementation of the style transfer module and the uncertainty-\\naware self-training loop. \\n• Phase 3 (Months 7-10): Conducting extensive experiments on benchmark datasets, \\nanalyzing results, and performing ablation studies. \\n• Phase 4 (Months 11-12): Writing a research paper for submission to a top-tier computer \\nvision conference (e.g., CVPR, ICCV) and finalizing the project report. \\n \\n7. Ethical Considerations \\nThe primary application of this research is to enhance safety in autonomous systems. However, \\nobject detection technology can also be used for surveillance. Our research will be conducted \\ntransparently, and we will focus our evaluation on publicly available datasets related to driving. We \\nwill not use private or personally identifiable data. The code and models will be made publicly \\navailable to ensure reproducibility and encourage positive use. \\n \\n8. Conclusion',\n",
       " 'will not use private or personally identifiable data. The code and models will be made publicly \\navailable to ensure reproducibility and encourage positive use. \\n \\n8. Conclusion \\nThis research proposal addresses the critical problem of domain shift in object detection. By \\nproposing a novel framework that integrates style transfer with a more robust, uncertainty-aware \\nself-training mechanism, we aim to significantly advance the state of the art in unsupervised domain \\nadaptation. The successful completion of this project will produce more reliable perception models, \\nthereby accelerating the safe and responsible deployment of AI in real-world applications. \\n \\n9. References \\n[Hoffman et al., 2018] Hoffman, J., Tzeng, E., Park, T., Zhu, J. Y., Isola, P., Saenko, K., ... & \\nDarrell, T. (2018). CyCADA: Cycle-consistent adversarial domain adaptation. International \\nconference on machine learning.',\n",
       " '[Saito et al., 2017] Saito, K., Watanabe, K., Ushiku, Y., & Harada, T. (2017). Asymmetric tri-\\ntraining for unsupervised domain adaptation. International conference on machine learning. \\n[Gal & Ghahramani, 2016] Gal, Y., & Ghahramani, Z. (2016). Dropout as a Bayesian approximation: \\nRepresenting model uncertainty in deep learning. International conference on machine learning. \\n[Zou et al., 2018] Zou, Y., Yu, Z., Kumar, B., & Wang, J. (2018). Unsupervised domain adaptation \\nfor semantic segmentation via class-balanced self-training. European conference on computer vision.']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### convert text to embeddings\n",
    "texts=[doc.page_content for doc in chunks]\n",
    "texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "966f6813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embedding for a 218 texts....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 7/7 [00:17<00:00,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape :(218, 384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "embeddings=embedding_manager.generate_embeddings(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e377a9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding 218 documents to vector store...\n",
      "Successfully added 218 documents to vector store\n",
      "Total documents in collection: 470\n"
     ]
    }
   ],
   "source": [
    "## Store in the vector dtbase\n",
    "vectorstore.add_documents(chunks,embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317ae5b2",
   "metadata": {},
   "source": [
    "### Retriever Pipeline From VectorStore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "46902d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAGRetriever:\n",
    "    \"\"\"Handles query-based retrieval from the vector store\"\"\"\n",
    "    \n",
    "    def __init__(self, vector_store: VectorStore, embedding_manager: EmbeddingManager):\n",
    "        \"\"\"\n",
    "        Initialize the retriever\n",
    "        \n",
    "        Args:\n",
    "            vector_store: Vector store containing document embeddings\n",
    "            embedding_manager: Manager for generating query embeddings\n",
    "        \"\"\"\n",
    "        self.vector_store = vector_store\n",
    "        self.embedding_manager = embedding_manager\n",
    "\n",
    "    def retrieve(self, query: str, top_k: int = 5, score_threshold: float = 0.0) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Retrieve relevant documents for a query\n",
    "        \n",
    "        Args:\n",
    "            query: The search query\n",
    "            top_k: Number of top results to return\n",
    "            score_threshold: Minimum similarity score threshold\n",
    "            \n",
    "        Returns:\n",
    "            List of dictionaries containing retrieved documents and metadata\n",
    "        \"\"\"\n",
    "        print(f\"Retrieving documents for query: '{query}'\")\n",
    "        print(f\"Top K: {top_k}, Score threshold: {score_threshold}\")\n",
    "        \n",
    "        # Generate query embedding\n",
    "        query_embedding = self.embedding_manager.generate_embeddings([query])[0]\n",
    "        \n",
    "        # Search in vector store\n",
    "        try:\n",
    "            results = self.vector_store.collection.query(\n",
    "                query_embeddings=[query_embedding.tolist()],\n",
    "                n_results=top_k\n",
    "            )\n",
    "            \n",
    "            # Process results\n",
    "            retrieved_docs = []\n",
    "            \n",
    "            if results['documents'] and results['documents'][0]:\n",
    "                documents = results['documents'][0]\n",
    "                metadatas = results['metadatas'][0]\n",
    "                distances = results['distances'][0]\n",
    "                ids = results['ids'][0]\n",
    "                \n",
    "                for i, (doc_id, document, metadata, distance) in enumerate(zip(ids, documents, metadatas, distances)):\n",
    "                    # Convert distance to similarity score (ChromaDB uses cosine distance)\n",
    "                    similarity_score = 1 - distance\n",
    "                    \n",
    "                    if similarity_score >= score_threshold:\n",
    "                        retrieved_docs.append({\n",
    "                            'id': doc_id,\n",
    "                            'content': document,\n",
    "                            'metadata': metadata,\n",
    "                            'similarity_score': similarity_score,\n",
    "                            'distance': distance,\n",
    "                            'rank': i + 1\n",
    "                        })\n",
    "                \n",
    "                print(f\"Retrieved {len(retrieved_docs)} documents (after filtering)\")\n",
    "            else:\n",
    "                print(\"No documents found\")\n",
    "            \n",
    "            return retrieved_docs\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during retrieval: {e}\")\n",
    "            return []\n",
    "\n",
    "rag_retriever=RAGRetriever(vectorstore,embedding_manager)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "42b80c8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.RAGRetriever at 0x1d984987950>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f5d5ac0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'what is embedding'\n",
      "Top K: 5, Score threshold: 0.0\n",
      "Generating embedding for a 1 texts....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 46.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape :(1, 384)\n",
      "Retrieved 5 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 'doc_80e2c4a8_52',\n",
       "  'content': 'While embeddings are incredibly powerful, they are not without their challenges and risks. The field \\nis constantly evolving to address these issues. \\n8.1. Bias in Embeddings \\nSince embeddings are learned from human-generated text, they inevitably capture the biases present \\nin that data. This can have harmful consequences. For example, many early embedding models \\nlearned associations like \"man is to computer programmer as woman is to homemaker.\" This reflects \\nhistorical societal biases in the training text. A significant area of research is dedicated to developing \\ntechniques for identifying and debiasing embeddings to ensure AI systems are fair and equitable. \\n8.2. Interpretability and Explainability \\nEmbeddings are dense vectors of floating-point numbers and are notoriously difficult to interpret. It \\nis hard to look at a 300-dimensional vector and understand why it represents the word \"justice\" or',\n",
       "  'metadata': {'moddate': '2025-10-15T15:37:01+01:00',\n",
       "   'content_length': 916,\n",
       "   'creationdate': '2025-10-15T15:37:01+01:00',\n",
       "   'page': 10,\n",
       "   'doc_index': 52,\n",
       "   'source': '..\\\\data\\\\pdf\\\\embeddings.pdf',\n",
       "   'creator': 'Microsoft® Word\\xa0LTSC',\n",
       "   'total_pages': 13,\n",
       "   'producer': 'Microsoft® Word\\xa0LTSC',\n",
       "   'source_file': 'embeddings.pdf',\n",
       "   'file_type': 'pdf',\n",
       "   'page_label': '11',\n",
       "   'author': 'ISMAIL LAMRANI'},\n",
       "  'similarity_score': 0.29557889699935913,\n",
       "  'distance': 0.7044211030006409,\n",
       "  'rank': 1},\n",
       " {'id': 'doc_a064457c_25',\n",
       "  'content': 'incorrect. This limitation, often referred to as the \"curse of dimensionality,\" necessitates a more \\nsophisticated approach. Dense embeddings provide a solution by mapping discrete objects (like \\nwords or nodes in a graph) to continuous, low-dimensional vectors in a way that preserves \\nunderlying semantic relationships. \\n1.2. The Concept of an Embedding Space \\nAn embedding is a learned representation for a discrete variable as a low-dimensional continuous \\nvector. This mapping is learned from data. The core idea is that objects with similar meanings or \\nfunctions should be positioned close to each other in the learned vector space, known as the \\n\"embedding space.\" Proximity in this space, typically measured by cosine similarity or Euclidean \\ndistance, correlates with semantic similarity. This geometric arrangement allows for powerful \\noperations. For instance, the famous analogy vector(\\'King\\') - vector(\\'Man\\') +',\n",
       "  'metadata': {'file_type': 'pdf',\n",
       "   'moddate': '2025-10-15T15:37:01+01:00',\n",
       "   'producer': 'Microsoft® Word\\xa0LTSC',\n",
       "   'source': '..\\\\data\\\\pdf\\\\embeddings.pdf',\n",
       "   'creator': 'Microsoft® Word\\xa0LTSC',\n",
       "   'author': 'ISMAIL LAMRANI',\n",
       "   'total_pages': 13,\n",
       "   'page_label': '3',\n",
       "   'doc_index': 25,\n",
       "   'content_length': 924,\n",
       "   'creationdate': '2025-10-15T15:37:01+01:00',\n",
       "   'source_file': 'embeddings.pdf',\n",
       "   'page': 2},\n",
       "  'similarity_score': 0.2781546711921692,\n",
       "  'distance': 0.7218453288078308,\n",
       "  'rank': 2},\n",
       " {'id': 'doc_d744550a_26',\n",
       "  'content': \"distance, correlates with semantic similarity. This geometric arrangement allows for powerful \\noperations. For instance, the famous analogy vector('King') - vector('Man') + \\nvector('Woman') results in a vector very close to vector('Queen'), demonstrating that the \\nembedding space has captured complex linguistic and social relationships [Mikolov et al., 2013]. \\nThis ability to encode meaning into geometry is the defining characteristic and primary advantage of \\nembeddings. \\n1.3. Problem Statement and Scope \\nThe field of representation learning has grown exponentially, producing a vast array of embedding \\ntechniques tailored to different data types and applications. For newcomers and even seasoned \\npractitioners, navigating this landscape can be daunting. This paper aims to provide a structured and \\ncomprehensive overview of embedding models. The scope of this review is twofold: first, to trace the\",\n",
       "  'metadata': {'source': '..\\\\data\\\\pdf\\\\embeddings.pdf',\n",
       "   'content_length': 909,\n",
       "   'page': 2,\n",
       "   'file_type': 'pdf',\n",
       "   'producer': 'Microsoft® Word\\xa0LTSC',\n",
       "   'moddate': '2025-10-15T15:37:01+01:00',\n",
       "   'source_file': 'embeddings.pdf',\n",
       "   'page_label': '3',\n",
       "   'creationdate': '2025-10-15T15:37:01+01:00',\n",
       "   'total_pages': 13,\n",
       "   'author': 'ISMAIL LAMRANI',\n",
       "   'doc_index': 26,\n",
       "   'creator': 'Microsoft® Word\\xa0LTSC'},\n",
       "  'similarity_score': 0.2381078004837036,\n",
       "  'distance': 0.7618921995162964,\n",
       "  'rank': 3},\n",
       " {'id': 'doc_e52350f3_49',\n",
       "  'content': 'Amazon. A system can learn an embedding for each user and each item (e.g., a movie). These \\nembeddings are learned such that a user\\'s vector will be close to the vectors of items they have liked \\nin the past. To make a new recommendation, the system simply looks for items in the embedding \\nspace that are near the user\\'s vector but which the user has not yet seen. \\n \\n7. Evaluating Embedding Quality \\nCreating embeddings is one thing, but knowing if they are \"good\" is another. Evaluation methods are \\ntypically split into two categories: intrinsic and extrinsic. \\n7.1. Intrinsic Evaluation \\nIntrinsic evaluations measure the quality of the embeddings directly, often on a specific subtask that \\nis designed to test for certain properties. \\n• Word Analogies: This is the classic test where a model is asked to solve analogies like \"man \\nis to king as woman is to ?\" The model succeeds if the result of vector(\\'king\\') - \\nvector(\\'man\\') + vector(\\'woman\\') is closest to vector(\\'queen\\').',\n",
       "  'metadata': {'file_type': 'pdf',\n",
       "   'moddate': '2025-10-15T15:37:01+01:00',\n",
       "   'producer': 'Microsoft® Word\\xa0LTSC',\n",
       "   'source': '..\\\\data\\\\pdf\\\\embeddings.pdf',\n",
       "   'content_length': 983,\n",
       "   'doc_index': 49,\n",
       "   'creationdate': '2025-10-15T15:37:01+01:00',\n",
       "   'author': 'ISMAIL LAMRANI',\n",
       "   'page_label': '10',\n",
       "   'total_pages': 13,\n",
       "   'creator': 'Microsoft® Word\\xa0LTSC',\n",
       "   'source_file': 'embeddings.pdf',\n",
       "   'page': 9},\n",
       "  'similarity_score': 0.18177473545074463,\n",
       "  'distance': 0.8182252645492554,\n",
       "  'rank': 4},\n",
       " {'id': 'doc_d512dac6_51',\n",
       "  'content': 'related than \"cup\" and \"car\"). The model\\'s quality is judged by how well its vector similarity \\nscores correlate with the human ratings. \\n7.2. Extrinsic Evaluation \\nExtrinsic evaluations are more practical. They measure the performance of the embeddings on a real-\\nworld \"downstream\" task. The embeddings are used as input features for a larger model to perform a \\ntask like sentiment analysis, text classification, or named entity recognition. The quality of the \\nembeddings is then judged by the final performance of that larger model. If using Embedding A \\nresults in 95% accuracy on a task while Embedding B results in 92% accuracy, then Embedding A is \\nconsidered better for that specific task. \\n \\n8. Challenges, Ethical Considerations, and Future Directions \\nWhile embeddings are incredibly powerful, they are not without their challenges and risks. The field \\nis constantly evolving to address these issues. \\n8.1. Bias in Embeddings',\n",
       "  'metadata': {'total_pages': 13,\n",
       "   'producer': 'Microsoft® Word\\xa0LTSC',\n",
       "   'page_label': '11',\n",
       "   'source_file': 'embeddings.pdf',\n",
       "   'creator': 'Microsoft® Word\\xa0LTSC',\n",
       "   'author': 'ISMAIL LAMRANI',\n",
       "   'source': '..\\\\data\\\\pdf\\\\embeddings.pdf',\n",
       "   'page': 10,\n",
       "   'doc_index': 51,\n",
       "   'content_length': 939,\n",
       "   'file_type': 'pdf',\n",
       "   'creationdate': '2025-10-15T15:37:01+01:00',\n",
       "   'moddate': '2025-10-15T15:37:01+01:00'},\n",
       "  'similarity_score': 0.17901283502578735,\n",
       "  'distance': 0.8209871649742126,\n",
       "  'rank': 5}]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_retriever.retrieve(\"what is embedding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdca797",
   "metadata": {},
   "source": [
    "### Integration Vectordb Context pipeline With LLM output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692eea75",
   "metadata": {},
   "outputs": [],
   "source": [
    "### simple RAG pipeline with Groq LLM\n",
    "from langchain_groq import ChatGroq\n",
    "import os \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "### itialize the Groq LLLM (set your Groq_API_Key in environment)\n",
    "# groq_api_key=os.getenv(\"GROK_API_KEY\")\n",
    "groq_api_key =\"GROK_API_KEY\"\n",
    "llm=ChatGroq(groq_api_key=groq_api_key,model_name=\"llama-3.1-8b-instant\",temperature=0.1,max_tokens=1024)\n",
    "#simple RAG function: retrieve context +generate response\n",
    "def rag_simple(query,retriever,llm,top_k=3):\n",
    "    ## retriever the context\n",
    "    results=retriever.retrieve(query,top_k=top_k)\n",
    "    context=\"\\n\\n\".join([doc['content'] for doc in results]) if results else \"\"\n",
    "    if not context:\n",
    "        return \"No relevant context found to answer the question.\"\n",
    "    \n",
    "    ## generate the answwer using GROQ LLM\n",
    "    prompt=f\"\"\"Use the following context to answer the question concisely.\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question: {query}\n",
    "\n",
    "        Answer:\"\"\"\n",
    "    \n",
    "    response=llm.invoke([prompt.format(context=context,query=query)])\n",
    "    return response.content \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "26ab6d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'What is attention mechanism?'\n",
      "Top K: 3, Score threshold: 0.0\n",
      "Generating embedding for a 1 texts....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 44.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape :(1, 384)\n",
      "Retrieved 3 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The attention mechanism is a concept inspired by the human visual system, which allows neural networks to dynamically focus on a subset of the input data, selectively processing and weighing relevant information to improve performance in various tasks.\n"
     ]
    }
   ],
   "source": [
    "answer=rag_simple(\"What is attention mechanism?\",rag_retriever,llm)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689e035e",
   "metadata": {},
   "source": [
    "### Enhanced RAG Pipeline Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7106c378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'What is attention mechanism?'\n",
      "Top K: 3, Score threshold: 0.1\n",
      "Generating embedding for a 1 texts....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 38.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape :(1, 384)\n",
      "Retrieved 3 documents (after filtering)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: The attention mechanism is a concept inspired by the human visual system, which enables neural networks to dynamically focus on specific parts of the input data, selectively processing a subset of the information to answer questions such as \"what\" and \"where\" to look.\n",
      "Sources: [{'source': 'attention.pdf', 'page': 1, 'score': 0.37996917963027954, 'preview': '2 Derya Soydaner\\nuntil the recognition task is complete. This sequential process happens so\\nquickly that we feel as if it happens all at once.\\nBiologically, this is called visual attention system . Visual attention is de-\\nﬁned as the ability to dynamically restrict processing to a subset of the visu...'}, {'source': 'attention.pdf', 'page': 5, 'score': 0.3717472553253174, 'preview': 'On the other side, the local attention is diﬀerentiable. Firstly, an aligned\\nposition pt is generated for each target word at a time t. Then, a window\\ncentered around the source position pt is used to compute the context vector\\nas a weighted average of the source hidden states within the window. The...'}, {'source': 'attention.pdf', 'page': 0, 'score': 0.36422646045684814, 'preview': 'Attention Mechanism in Neural Networks:\\nWhere it Comes and Where it Goes\\nDerya Soydaner\\nReceived: 22 July 2021 / Accepted: 27 April 2022\\nAbstract A long time ago in the machine learning literature, the idea of\\nincorporating a mechanism inspired by the human visual system into neural\\nnetworks was int...'}]\n",
      "Confidence: 0.37996917963027954\n",
      "Context Preview: 2 Derya Soydaner\n",
      "until the recognition task is complete. This sequential process happens so\n",
      "quickly that we feel as if it happens all at once.\n",
      "Biologically, this is called visual attention system . Visual attention is de-\n",
      "ﬁned as the ability to dynamically restrict processing to a subset of the visu\n"
     ]
    }
   ],
   "source": [
    "# --- Enhanced RAG Pipeline Features ---\n",
    "def rag_advanced(query, retriever, llm, top_k=5, min_score=0.2, return_context=False):\n",
    "    \"\"\"\n",
    "    RAG pipeline with extra features:\n",
    "    - Returns answer, sources, confidence score, and optionally full context.\n",
    "    \"\"\"\n",
    "    results = retriever.retrieve(query, top_k=top_k, score_threshold=min_score)\n",
    "    if not results:\n",
    "        return {'answer': 'No relevant context found.', 'sources': [], 'confidence': 0.0, 'context': ''}\n",
    "    \n",
    "    # Prepare context and sources\n",
    "    context = \"\\n\\n\".join([doc['content'] for doc in results])\n",
    "    sources = [{\n",
    "        'source': doc['metadata'].get('source_file', doc['metadata'].get('source', 'unknown')),\n",
    "        'page': doc['metadata'].get('page', 'unknown'),\n",
    "        'score': doc['similarity_score'],\n",
    "        'preview': doc['content'][:300] + '...'\n",
    "    } for doc in results]\n",
    "    confidence = max([doc['similarity_score'] for doc in results])\n",
    "    \n",
    "    # Generate answer\n",
    "    prompt = f\"\"\"Use the following context to answer the question concisely.\\nContext:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer:\"\"\"\n",
    "    response = llm.invoke([prompt.format(context=context, query=query)])\n",
    "    \n",
    "    output = {\n",
    "        'answer': response.content,\n",
    "        'sources': sources,\n",
    "        'confidence': confidence\n",
    "    }\n",
    "    if return_context:\n",
    "        output['context'] = context\n",
    "    return output\n",
    "\n",
    "# Example usage:\n",
    "result = rag_advanced(\"What is attention mechanism?\", rag_retriever, llm, top_k=3, min_score=0.1, return_context=True)\n",
    "print(\"Answer:\", result['answer'])\n",
    "print(\"Sources:\", result['sources'])\n",
    "print(\"Confidence:\", result['confidence'])\n",
    "print(\"Context Preview:\", result['context'][:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4a244690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving documents for query: 'What is attention mechanism?'\n",
      "Top K: 3, Score threshold: 0.1\n",
      "Generating embedding for a 1 texts....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 49.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated embeddings with shape :(1, 384)\n",
      "Retrieved 3 documents (after filtering)\n",
      "Streaming answer:\n",
      "Use the following context to answer the question concisely.\n",
      "Context:\n",
      "2 Derya Soydaner\n",
      "until the recognition task is complete. This sequential process happens so\n",
      "quickly that we feel as if it happens all at once.\n",
      "Biologically, this is called visual attention system . Visual attention is de-\n",
      "ﬁned as the ability to dynami"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cally restrict processing to a subset of the visual\n",
      "ﬁeld [5]. It seeks answers for two main questions: What and where to look?\n",
      "Visual attention has been extensively studied in psychology and neuroscience;\n",
      "for reviews see [6,7,8,9,10]. Besides, there is a large amount of literature on\n",
      "modeling eye movements [11,12,13,14]. These studies have been a source of\n",
      "inspiration for many artiﬁcial intelligence tasks. It has been discovered that\n",
      "the attention idea is useful from image recognition to machine translation.\n",
      "Therefore, diﬀerent types of attention mechanisms inspired from the human\n",
      "visual system have been developed for years. Since the success of deep neural\n",
      "networks has been at the forefront for these artiﬁcial intelligence tasks, these\n",
      "\n",
      "On the other side, the local attention is diﬀerentiable. Firstly, an aligned\n",
      "position pt is generated for each target word at a time t. Then, a window\n",
      "centered around the source position pt is used to compute the context vector\n",
      "as a weighted average of the source hidden states within the window. The\n",
      "local attention selectively focuses on a small window of context, and obtains\n",
      "the alignment vector from the current target state ht and the source states ¯hs\n",
      "in the window [51].\n",
      "The introduction of these novel mechanisms in 2015 triggered the rise of\n",
      "attention for neural networks. Based on the proposed attention mechanisms,\n",
      "signiﬁcant research has been conducted in a variety of tasks. In order to imag-\n",
      "ine the attention idea in neural networks better, two visual examples are shown\n",
      "in Fig. 2. A neural image caption generation task is seen in the top row that\n",
      "implements an attention mechanism [48]. Then, the second example shows\n",
      "\n",
      "Attention Mechanism in Neural Networks:\n",
      "Where it Comes and Where it Goes\n",
      "Derya Soydaner\n",
      "Received: 22 July 2021 / Accepted: 27 April 2022\n",
      "Abstract A long time ago in the machine learning literature, the idea of\n",
      "incorporating a mechanism inspired by the human visual system into neural\n",
      "networks was introduced. This idea is named the attention mechanism, and it\n",
      "has gone through a long development period. Today, many works have been\n",
      "devoted to this idea in a variety of tasks. Remarkable performance has re-\n",
      "cently been demonstrated. The goal of this paper is to provide an overview\n",
      "from the early work on searching for ways to implement attention idea with\n",
      "neural networks until the recent trends. This review emphasizes the impor-\n",
      "tant milestones during this progress regarding diﬀerent tasks. By this way,\n",
      "this study aims to provide a road map for researchers to explore the current\n",
      "development and get inspired for novel approaches beyond the attention.\n",
      "\n",
      "Question: What is attention mechanism?\n",
      "\n",
      "Answer:\n",
      "\n",
      "Final Answer: The attention mechanism is a concept inspired by the human visual system, which enables neural networks to dynamically focus on a subset of the input data, selectively processing and weighing relevant information to improve performance in various tasks.\n",
      "\n",
      "Citations:\n",
      "[1] attention.pdf (page 1)\n",
      "[2] attention.pdf (page 5)\n",
      "[3] attention.pdf (page 0)\n",
      "Summary: The attention mechanism is a concept inspired by the human visual system that allows neural networks to selectively focus on relevant input data. This enables neural networks to dynamically process and weigh important information, improving performance in various tasks.\n",
      "History: {'question': 'What is attention mechanism?', 'answer': 'The attention mechanism is a concept inspired by the human visual system, which enables neural networks to dynamically focus on a subset of the input data, selectively processing and weighing relevant information to improve performance in various tasks.', 'sources': [{'source': 'attention.pdf', 'page': 1, 'score': 0.37996917963027954, 'preview': '2 Derya Soydaner\\nuntil the recognition task is complete. This sequential process happens so\\nquickly that we feel as if i...'}, {'source': 'attention.pdf', 'page': 5, 'score': 0.3717472553253174, 'preview': 'On the other side, the local attention is diﬀerentiable. Firstly, an aligned\\nposition pt is generated for each target wo...'}, {'source': 'attention.pdf', 'page': 0, 'score': 0.36422646045684814, 'preview': 'Attention Mechanism in Neural Networks:\\nWhere it Comes and Where it Goes\\nDerya Soydaner\\nReceived: 22 July 2021 / Accepte...'}], 'summary': 'The attention mechanism is a concept inspired by the human visual system that allows neural networks to selectively focus on relevant input data. This enables neural networks to dynamically process and weigh important information, improving performance in various tasks.'}\n"
     ]
    }
   ],
   "source": [
    "# --- Advanced RAG Pipeline: Streaming, Citations, History, Summarization ---\n",
    "from typing import List, Dict, Any\n",
    "import time\n",
    "\n",
    "class AdvancedRAGPipeline:\n",
    "    def __init__(self, retriever, llm):\n",
    "        self.retriever = retriever\n",
    "        self.llm = llm\n",
    "        self.history = []  # Store query history\n",
    "\n",
    "    def query(self, question: str, top_k: int = 5, min_score: float = 0.2, stream: bool = False, summarize: bool = False) -> Dict[str, Any]:\n",
    "        # Retrieve relevant documents\n",
    "        results = self.retriever.retrieve(question, top_k=top_k, score_threshold=min_score)\n",
    "        if not results:\n",
    "            answer = \"No relevant context found.\"\n",
    "            sources = []\n",
    "            context = \"\"\n",
    "        else:\n",
    "            context = \"\\n\\n\".join([doc['content'] for doc in results])\n",
    "            sources = [{\n",
    "                'source': doc['metadata'].get('source_file', doc['metadata'].get('source', 'unknown')),\n",
    "                'page': doc['metadata'].get('page', 'unknown'),\n",
    "                'score': doc['similarity_score'],\n",
    "                'preview': doc['content'][:120] + '...'\n",
    "            } for doc in results]\n",
    "            # Streaming answer simulation\n",
    "            prompt = f\"\"\"Use the following context to answer the question concisely.\\nContext:\\n{context}\\n\\nQuestion: {question}\\n\\nAnswer:\"\"\"\n",
    "            if stream:\n",
    "                print(\"Streaming answer:\")\n",
    "                for i in range(0, len(prompt), 80):\n",
    "                    print(prompt[i:i+80], end='', flush=True)\n",
    "                    time.sleep(0.05)\n",
    "                print()\n",
    "            response = self.llm.invoke([prompt.format(context=context, question=question)])\n",
    "            answer = response.content\n",
    "\n",
    "        # Add citations to answer\n",
    "        citations = [f\"[{i+1}] {src['source']} (page {src['page']})\" for i, src in enumerate(sources)]\n",
    "        answer_with_citations = answer + \"\\n\\nCitations:\\n\" + \"\\n\".join(citations) if citations else answer\n",
    "\n",
    "        # Optionally summarize answer\n",
    "        summary = None\n",
    "        if summarize and answer:\n",
    "            summary_prompt = f\"Summarize the following answer in 2 sentences:\\n{answer}\"\n",
    "            summary_resp = self.llm.invoke([summary_prompt])\n",
    "            summary = summary_resp.content\n",
    "\n",
    "        # Store query history\n",
    "        self.history.append({\n",
    "            'question': question,\n",
    "            'answer': answer,\n",
    "            'sources': sources,\n",
    "            'summary': summary\n",
    "        })\n",
    "\n",
    "        return {\n",
    "            'question': question,\n",
    "            'answer': answer_with_citations,\n",
    "            'sources': sources,\n",
    "            'summary': summary,\n",
    "            'history': self.history\n",
    "        }\n",
    "\n",
    "# Example usage:\n",
    "adv_rag = AdvancedRAGPipeline(rag_retriever, llm)\n",
    "result = adv_rag.query(\"What is attention mechanism?\", top_k=3, min_score=0.1, stream=True, summarize=True)\n",
    "print(\"\\nFinal Answer:\", result['answer'])\n",
    "print(\"Summary:\", result['summary'])\n",
    "print(\"History:\", result['history'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a92c701",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "YTRAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
