{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d252047",
   "metadata": {},
   "source": [
    "## Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9373d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "### document datastructure\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b74dce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'example.txt', 'pages': 1, 'author': 'Ismail Lamrani', 'date_created': '2025-01-01'}, page_content='this is the main text content I am using to create RAG')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc=Document(\n",
    "page_content=\"this is the main text content I am using to create RAG\", \n",
    "metadata={\n",
    "    \"source\": \"example.txt\",\n",
    "    \"pages\" : 1,\n",
    "    \"author\": \"Ismail Lamrani\",\n",
    "    \"date_created\":\"2025-01-01\"\n",
    "    }\n",
    ")\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9dd7f841",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a simple txt file\n",
    "import os \n",
    "os.makedirs(\"../data/text_files\",exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43b92777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simple text files created\n"
     ]
    }
   ],
   "source": [
    "sample_texts={\n",
    "    \"../data/text_files/python_intro.txt\":\"\"\"Python Programming Introduction\n",
    "\n",
    "Python is a high-level, interpreted programming language known for its simplicity and readability.\n",
    "Created by Guido van Rossum and first released in 1991, Python has become one of the most popular\n",
    "programming languages in the world.\n",
    "\n",
    "Key Features:\n",
    "- Easy to learn and use\n",
    "- Extensive standard library\n",
    "- Cross-platform compatibility\n",
    "- Strong community support\n",
    "\n",
    "Python is widely used in web development, data science, artificial intelligence, and automation.\"\"\",\n",
    "    \n",
    "    \"../data/text_files/machine_learning.txt\": \"\"\"Machine Learning Basics\n",
    "\n",
    "Machine learning is a subset of artificial intelligence that enables systems to learn and improve\n",
    "from experience without being explicitly programmed. It focuses on developing computer programs\n",
    "that can access data and use it to learn for themselves.\n",
    "\n",
    "Types of Machine Learning:\n",
    "1. Supervised Learning: Learning with labeled data\n",
    "2. Unsupervised Learning: Finding patterns in unlabeled data\n",
    "3. Reinforcement Learning: Learning through rewards and penalties\n",
    "\n",
    "Applications include image recognition, speech processing, and recommendation systems\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "}\n",
    "for filepath, content in sample_texts.items():\n",
    "    with open(filepath,'w',encoding=\"utf-8\") as f :\n",
    "        f.write(content)\n",
    "print(\"simple text files created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49bce290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '../data/text_files/python_intro.txt'}, page_content='Python Programming Introduction\\n\\nPython is a high-level, interpreted programming language known for its simplicity and readability.\\nCreated by Guido van Rossum and first released in 1991, Python has become one of the most popular\\nprogramming languages in the world.\\n\\nKey Features:\\n- Easy to learn and use\\n- Extensive standard library\\n- Cross-platform compatibility\\n- Strong community support\\n\\nPython is widely used in web development, data science, artificial intelligence, and automation.')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### TextLoader\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "loader=TextLoader(\"../data/text_files/python_intro.txt\",encoding=\"utf-8\")\n",
    "document=loader.load()\n",
    "document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "296fb75e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '..\\\\data\\\\text_files\\\\machine_learning.txt'}, page_content='Machine Learning Basics\\n\\nMachine learning is a subset of artificial intelligence that enables systems to learn and improve\\nfrom experience without being explicitly programmed. It focuses on developing computer programs\\nthat can access data and use it to learn for themselves.\\n\\nTypes of Machine Learning:\\n1. Supervised Learning: Learning with labeled data\\n2. Unsupervised Learning: Finding patterns in unlabeled data\\n3. Reinforcement Learning: Learning through rewards and penalties\\n\\nApplications include image recognition, speech processing, and recommendation systems\\n\\n\\n    '),\n",
       " Document(metadata={'source': '..\\\\data\\\\text_files\\\\python_intro.txt'}, page_content='Python Programming Introduction\\n\\nPython is a high-level, interpreted programming language known for its simplicity and readability.\\nCreated by Guido van Rossum and first released in 1991, Python has become one of the most popular\\nprogramming languages in the world.\\n\\nKey Features:\\n- Easy to learn and use\\n- Extensive standard library\\n- Cross-platform compatibility\\n- Strong community support\\n\\nPython is widely used in web development, data science, artificial intelligence, and automation.')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Directory loader \n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "## load all text files from the directory\n",
    "dir_loader=DirectoryLoader(\n",
    "    \"../data/text_files\",\n",
    "    glob=\"*.txt\",\n",
    "    loader_cls=TextLoader,\n",
    "    loader_kwargs={\"encoding\":\"utf-8\"},\n",
    "    show_progress=False\n",
    "    \n",
    ")\n",
    "documents = dir_loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d00dce4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\Desktop\\YTRAG\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:47:01+01:00', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'file_path': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': '', 'author': 'ISMAIL LAMRANI', 'subject': '', 'keywords': '', 'moddate': '2025-10-15T15:47:01+01:00', 'trapped': '', 'modDate': \"D:20251015154701+01'00'\", 'creationDate': \"D:20251015154701+01'00'\", 'page': 0}, page_content=\"Title: The Attention Mechanism: From Sequence Alignment to the Foundation of \\nTransformer Architectures \\n \\n \\nAbstract \\nThe attention mechanism has revolutionized the field of artificial intelligence, particularly in natural \\nlanguage processing and computer vision. Initially designed to overcome the limitations of \\ntraditional sequence-to-sequence models in tasks like machine translation, the concept of attention \\nhas evolved to become the cornerstone of today's highest-performing architectures, most notably the \\nTransformer. This paper traces the evolution of the attention mechanism, beginning with its original \\napplication for word alignment in encoder-decoder models. We then explore its most influential \\ndevelopment, self-attention, which allows a model to weigh the relative importance of elements \\nwithin a single sequence. We detail how this concept led to the Transformer architecture, which has \\nsupplanted recurrent and convolutional models in many tasks. The paper also covers the application \\nof attention beyond text, particularly in computer vision, where it enables models to focus on \\nrelevant regions of an image. Finally, we discuss key applications, current challenges such as \\ncomputational cost, and future research directions in this foundational area. \\nKeywords: Attention Mechanism, Self-Attention, Transformer, Sequence-to-Sequence Models, \\nNatural Language Processing (NLP), Computer Vision, BERT, GPT.\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:47:01+01:00', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'file_path': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': '', 'author': 'ISMAIL LAMRANI', 'subject': '', 'keywords': '', 'moddate': '2025-10-15T15:47:01+01:00', 'trapped': '', 'modDate': \"D:20251015154701+01'00'\", 'creationDate': \"D:20251015154701+01'00'\", 'page': 1}, page_content='Table of Contents \\n1. Introduction 1.1. The Memory Limitation: The Bottleneck Problem 1.2. The Human \\nIntuition of Attention 1.3. Objectives and Paper Structure \\n2. Background and Preliminaries: The Encoder-Decoder Architecture 2.1. Recurrent \\nNeural Networks (RNNs) 2.2. The Sequence-to-Sequence (Seq2Seq) Model 2.3. The Context \\nVector: An Information Bottleneck \\n3. The Foundational Attention Mechanism 3.1. Allowing the Decoder to \"Look Back\" 3.2. \\nThe Conceptual Steps of Attention 3.3. The Dynamic Context Vector \\n4. Different Types of Attention Mechanisms 4.1. Additive (Bahdanau) vs. Multiplicative \\n(Luong) Attention 4.2. Global vs. Local Attention \\n5. The Self-Attention Revolution 5.1. From Inter-Sequence Alignment to Intra-Sequence \\nAnalysis 5.2. Understanding the Internal Relationships of a Sentence 5.3. Multi-Head \\nAttention: Multiple Attention Perspectives \\n6. The Transformer Architecture 6.1. Abandoning Recurrence for Pure Attention 6.2. The \\nTransformer\\'s Encoder and Decoder Blocks 6.3. The Impact on Parallelization and \\nComputational Efficiency \\n7. Beyond Text: Attention in Computer Vision 7.1. Visual Attention for Image Captioning \\n7.2. Vision Transformers (ViT): Treating Images as Sequences \\n8. Applications and Use Cases 8.1. Neural Machine Translation (NMT) 8.2. Text \\nSummarization and Content Generation 8.3. Sentiment Analysis and Text Classification 8.4. \\nLarge Language Models (BERT, GPT) \\n9. Challenges, Limitations, and Future Directions 9.1. The Quadratic Computational Cost of \\nSelf-Attention 9.2. Towards More Efficient Attention Mechanisms 9.3. Interpretability of \\nAttention Weights \\n10. Conclusion \\n11. References'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:47:01+01:00', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'file_path': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': '', 'author': 'ISMAIL LAMRANI', 'subject': '', 'keywords': '', 'moddate': '2025-10-15T15:47:01+01:00', 'trapped': '', 'modDate': \"D:20251015154701+01'00'\", 'creationDate': \"D:20251015154701+01'00'\", 'page': 2}, page_content='1. Introduction \\n1.1. The Memory Limitation: The Bottleneck Problem \\nBefore the advent of attention, sequence processing models, like those used for translation, operated \\nin two stages. An \"encoder\" would read the source sentence (e.g., in French) and compress it into a \\nsingle, fixed-size vector, which was supposed to contain all its information. Then, a \"decoder\" would \\nuse this single vector to generate the target sentence (in English). This approach worked reasonably \\nwell for short sentences but failed on longer ones. The single vector became an information \\nbottleneck, unable to store all the nuances of a long passage of text. \\n1.2. The Human Intuition of Attention \\nThe idea for the attention mechanism is inspired by human cognition. When you translate a sentence, \\nyou don\\'t memorize it entirely before writing. You focus on the part of the source sentence that is \\nrelevant to the word you are currently generating. Similarly, when looking at an image, your gaze \\nfocuses on areas of interest to describe it. The attention mechanism seeks to replicate this ability by \\nallowing a model to dynamically \"look at\" and focus on the most relevant parts of the input \\ninformation at each step of its task. \\n1.3. Objectives and Paper Structure \\nThis paper aims to provide a conceptual overview of the attention mechanism and its transformative \\nimpact on AI. We will begin by explaining the context of sequence-to-sequence models that \\nmotivated its invention. We will then describe how the original attention mechanism works, before \\nmoving on to its most powerful form, self-attention, which is the heart of the Transformer. Finally, \\nwe will explore its applications and current challenges. \\n \\n2. Background and Preliminaries: The Encoder-Decoder Architecture \\n2.1. Recurrent Neural Networks (RNNs) \\nRNNs are neural networks designed to process sequential data, such as text. They have an internal \\n\"memory\" that allows them to retain information about previous elements in the sequence while \\nprocessing the current element. This property made them ideal for language tasks.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:47:01+01:00', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'file_path': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': '', 'author': 'ISMAIL LAMRANI', 'subject': '', 'keywords': '', 'moddate': '2025-10-15T15:47:01+01:00', 'trapped': '', 'modDate': \"D:20251015154701+01'00'\", 'creationDate': \"D:20251015154701+01'00'\", 'page': 3}, page_content='2.2. The Sequence-to-Sequence (Seq2Seq) Model \\nA Seq2Seq model is composed of two RNNs: an encoder and a decoder. The encoder reads the input \\nsequence (word by word) and updates its internal memory state at each step. The decoder takes the \\nfinal memory state of the encoder and generates the output sequence word by word. \\n2.3. The Context Vector: An Information Bottleneck \\nThe final memory state of the encoder is called the context vector. It is this single vector that poses \\nthe problem. For a 50-word sentence, all the information about grammar, meaning, and relationships \\nbetween words must be compressed into this single vector. This is a nearly impossible task, and \\ninformation about the first words of the sentence is often lost or diluted by the time the encoder \\nfinishes reading. \\n \\n3. The Foundational Attention Mechanism \\n3.1. Allowing the Decoder to \"Look Back\" \\nThe attention mechanism was introduced to solve this bottleneck problem. Instead of forcing the \\nencoder to compress everything into a single vector, the memory states of the encoder for each word \\nin the input sentence are kept. Thus, the decoder, when generating a new word, is allowed to \"look \\nback\" at the entire set of states from the source sentence. \\n3.2. The Conceptual Steps of Attention \\nAt each step of generation, the decoder proceeds as follows: \\n1. Calculate Alignment Scores: It compares its own current memory state with each of the \\nencoder\\'s memory states. This comparison produces a \"score\" that indicates how relevant \\neach source word is to the word it is about to generate. \\n2. Convert to Weights: These scores are then passed through a function (softmax) that \\ntransforms them into \"attention weights.\" These weights are percentages that sum to 100%. A \\nsource word with a weight of 70% is considered very important for the current step. \\n3. Create a Weighted Context Vector: Finally, the decoder creates a new context vector by \\ntaking a weighted average of the encoder states, using the attention weights it just calculated.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:47:01+01:00', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'file_path': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': '', 'author': 'ISMAIL LAMRANI', 'subject': '', 'keywords': '', 'moddate': '2025-10-15T15:47:01+01:00', 'trapped': '', 'modDate': \"D:20251015154701+01'00'\", 'creationDate': \"D:20251015154701+01'00'\", 'page': 4}, page_content='3.3. The Dynamic Context Vector \\nThe result is a dynamic context vector. Instead of a single fixed vector for the entire translation, the \\ndecoder creates a new, custom-made context vector at each step, focusing on the source words most \\nrelevant to that specific moment. \\n \\n4. Different Types of Attention Mechanisms \\n4.1. Additive (Bahdanau) vs. Multiplicative (Luong) Attention \\nThe earliest forms of attention are distinguished mainly by how the \"alignment score\" is calculated. \\nBahdanau\\'s attention uses a small neural network to compute this score (an additive approach). \\nLuong\\'s attention, which is simpler, calculates the score by multiplying the state vectors (a \\nmultiplicative approach). Both methods are effective, with the multiplicative version often being \\nfaster in practice. \\n4.2. Global vs. Local Attention \\nGlobal attention is the standard approach where the decoder looks at all the words in the source \\nsentence. Local attention, on the other hand, only focuses on a small window of source words, which \\ncan make it more efficient for very long documents. \\n \\n5. The Self-Attention Revolution \\n5.1. From Inter-Sequence Alignment to Intra-Sequence Analysis \\nThe real breakthrough came when researchers asked, \"What if we used attention not to align two \\ndifferent sequences, but to analyze the relationships within a single sequence?\" This is the idea \\nbehind self-attention. \\n5.2. Understanding the Internal Relationships of a Sentence \\nSelf-attention allows each word in a sentence to look at all other words in the same sentence to better \\nunderstand its own context. For example, in the sentence \"The cat didn\\'t cross the road because it'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:47:01+01:00', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'file_path': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': '', 'author': 'ISMAIL LAMRANI', 'subject': '', 'keywords': '', 'moddate': '2025-10-15T15:47:01+01:00', 'trapped': '', 'modDate': \"D:20251015154701+01'00'\", 'creationDate': \"D:20251015154701+01'00'\", 'page': 5}, page_content='was tired,\" self-attention helps the model determine that the pronoun \"it\" refers to the \"cat\" and not \\nthe \"road.\" It does this by creating strong links between \"it\" and \"cat.\" \\n5.3. Multi-Head Attention: Multiple Attention Perspectives \\nMulti-Head Attention is an improvement on self-attention. Instead of calculating attention just \\nonce, the model does it several times in parallel (with multiple \"heads\"). Each attention head can \\nlearn to focus on a different type of relationship. One head might focus on subject-verb relationships, \\nanother on pronominal links, and so on. This allows the model to capture a much richer range of \\nlinguistic relationships. \\n \\n6. The Transformer Architecture \\n6.1. Abandoning Recurrence for Pure Attention \\nThe paper \"Attention Is All You Need\" introduced the Transformer, an architecture that gets rid of \\nrecurrent networks (RNNs) entirely and relies exclusively on self-attention to process sequences. \\n6.2. The Transformer\\'s Encoder and Decoder Blocks \\nThe Transformer maintains an encoder-decoder structure, but each block is built from layers of \\nmulti-head attention and feed-forward neural networks. The encoder uses self-attention to build rich \\nrepresentations of the input sequence. The decoder uses self-attention on the sequence it has already \\ngenerated, as well as a standard attention mechanism to look at the encoder\\'s output. \\n6.3. The Impact on Parallelization and Computational Efficiency \\nA major advantage of the Transformer is that it is much more parallelizable than RNNs. Since there \\nis no step-by-step sequential processing, the calculations for all words in a sentence can be \\nperformed simultaneously, which has made it possible to train much larger models on much larger \\ndatasets. \\n \\n7. Beyond Text: Attention in Computer Vision'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:47:01+01:00', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'file_path': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': '', 'author': 'ISMAIL LAMRANI', 'subject': '', 'keywords': '', 'moddate': '2025-10-15T15:47:01+01:00', 'trapped': '', 'modDate': \"D:20251015154701+01'00'\", 'creationDate': \"D:20251015154701+01'00'\", 'page': 6}, page_content='7.1. Visual Attention for Image Captioning \\nAttention is also very useful in vision. In an image captioning task, a model can learn to focus on \\ndifferent parts of the image as it generates different words. To write \"A dog catches a red frisbee,\" \\nthe model will focus its attention on the dog, then on the frisbee. \\n7.2. Vision Transformers (ViT) \\nMore recently, Vision Transformers (ViT) have applied the Transformer architecture directly to \\nimages. They cut an image into a grid of patches, treat these patches as a sequence of \"words,\" and \\nuse self-attention to analyze the relationships between the different parts of the image. This approach \\nhas achieved state-of-the-art performance on many image classification tasks. \\n \\n8. Applications and Use Cases \\nThe attention mechanism, especially via the Transformer, is at the heart of the greatest successes in \\nmodern AI. \\n• \\nNeural Machine Translation (NMT): Services like Google Translate rely on attention-\\nbased architectures. \\n• \\nText Summarization: Models can identify the most important sentences in a document to \\ncreate a summary. \\n• \\nLarge Language Models: Models like BERT and GPT are Transformer architectures. \\nBERT uses it to understand context, while GPT uses it to generate coherent and relevant text. \\nThese models are the basis for most modern chatbots and generative AI tools. \\n \\n9. Challenges, Limitations, and Future Directions \\n9.1. The Quadratic Computational Cost of Self-Attention \\nThe main drawback of self-attention is that its computational and memory cost increases \\nquadratically with the length of the sequence. Doubling the length of the sentence quadruples the'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:47:01+01:00', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'file_path': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': '', 'author': 'ISMAIL LAMRANI', 'subject': '', 'keywords': '', 'moddate': '2025-10-15T15:47:01+01:00', 'trapped': '', 'modDate': \"D:20251015154701+01'00'\", 'creationDate': \"D:20251015154701+01'00'\", 'page': 7}, page_content='required resources. This makes it very expensive for very long documents or very high-resolution \\nimages. \\n9.2. Towards More Efficient Attention Mechanisms \\nMuch of current research focuses on creating more efficient attention variants (Sparse Attention, \\nLinformer, etc.) that approximate full self-attention with a much lower computational cost, with the \\ngoal of processing ever-longer contexts. \\n9.3. Interpretability of Attention Weights \\nAlthough one can visualize attention weights to see where a model is \"looking,\" it is sometimes \\ndifficult to know if these visualizations truly reflect the model\\'s \"reasoning\" process or if they are \\njust an artifact of its operation. \\n \\n10. Conclusion \\nThe attention mechanism was much more than a simple technical improvement; it represented a \\nparadigm shift. Starting as an elegant solution to the bottleneck problem of Seq2Seq models, it \\nevolved to become the organizing principle of the most powerful AI architectures to date. By \\nallowing models to dynamically weigh and contextualize information, attention unlocked levels of \\nperformance and capabilities that were previously out of reach. Its embodiment in the Transformer \\nhas set a new standard and continues to shape the future of artificial intelligence, from language to \\nvision and beyond. The current challenges, especially in efficiency, are only spurring innovation \\ntowards even more powerful and scalable forms of attention.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:47:01+01:00', 'source': '..\\\\data\\\\pdf\\\\attention.pdf', 'file_path': '..\\\\data\\\\pdf\\\\attention.pdf', 'total_pages': 9, 'format': 'PDF 1.7', 'title': '', 'author': 'ISMAIL LAMRANI', 'subject': '', 'keywords': '', 'moddate': '2025-10-15T15:47:01+01:00', 'trapped': '', 'modDate': \"D:20251015154701+01'00'\", 'creationDate': \"D:20251015154701+01'00'\", 'page': 8}, page_content='11. References \\n[Bahdanau et al., 2014] Bahdanau, D., Cho, K., & Bengio, Y. (2014). Neural machine translation by \\njointly learning to align and translate. arXiv preprint arXiv:1409.0473. \\n[Luong et al., 2015] Luong, M. T., Pham, H., & Manning, C. D. (2015). Effective approaches to \\nattention-based neural machine translation. arXiv preprint arXiv:1508.04025. \\n[Vaswani et al., 2017] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., \\n... & Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing \\nsystems. \\n[Dosovitskiy et al., 2020] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., \\nUnterthiner, T., ... & Houlsby, N. (2020). An image is worth 16x16 words: Transformers for image \\nrecognition at scale. arXiv preprint arXiv:2010.11929.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:37:01+01:00', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'file_path': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': '', 'author': 'ISMAIL LAMRANI', 'subject': '', 'keywords': '', 'moddate': '2025-10-15T15:37:01+01:00', 'trapped': '', 'modDate': \"D:20251015153701+01'00'\", 'creationDate': \"D:20251015153701+01'00'\", 'page': 0}, page_content='Title: Vector Representations of Data: A Comprehensive Review of Embedding \\nModels and Their Applications \\nAuthor: [Your Name/Institution] Date: October 2025 \\n \\nAbstract \\nEmbeddings have become a cornerstone of modern machine learning, enabling the representation of \\ncomplex, high-dimensional data in dense, low-dimensional vector spaces. These vector \\nrepresentations, or embeddings, capture the latent semantic relationships between data points, \\nmaking them amenable to a wide range of computational tasks. This paper provides a comprehensive \\nreview of embedding techniques, starting with foundational models for natural language processing, \\nsuch as Word2Vec, GloVe, and fastText, which are predicated on the distributional hypothesis. We \\nthen explore the evolution towards contextualized embeddings, exemplified by models like ELMo \\nand the Transformer-based architectures of BERT and GPT, which address the limitations of static \\nword vectors by capturing polysemy. Beyond text, this review covers the extension of embedding \\nconcepts to other data modalities, including graph embeddings (e.g., Node2Vec) for network analysis \\nand multimodal embeddings for tasks involving a synthesis of text, image, and audio data. We \\nsurvey key applications, including information retrieval, sentiment analysis, and recommendation \\nsystems, and discuss standard methods for evaluating the quality of embeddings. Finally, we address \\nthe current challenges, ethical considerations, and future research directions in this rapidly advancing \\nfield. \\nKeywords: Embeddings, Vector Space Models, Representation Learning, Natural Language \\nProcessing, Word2Vec, BERT, Graph Embeddings, Multimodal Learning.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:37:01+01:00', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'file_path': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': '', 'author': 'ISMAIL LAMRANI', 'subject': '', 'keywords': '', 'moddate': '2025-10-15T15:37:01+01:00', 'trapped': '', 'modDate': \"D:20251015153701+01'00'\", 'creationDate': \"D:20251015153701+01'00'\", 'page': 1}, page_content='Table of Contents \\n1. Introduction 1.1. Motivation: The Need for Dense Representations 1.2. The Concept of an \\nEmbedding Space 1.3. Problem Statement and Scope 1.4. Contributions and Paper Structure \\n2. Background and Preliminaries 2.1. Vector Space Models (VSMs) 2.2. The Distributional \\nHypothesis: \"A word is characterized by the company it keeps\" 2.3. Traditional Approaches: \\nOne-Hot Encoding and its Limitations 2.4. Dimensionality Reduction: From PCA to t-SNE \\n3. Foundational Static Embedding Models 3.1. Word2Vec (Mikolov et al., 2013) 3.1.1. \\nContinuous Bag-of-Words (CBOW) 3.1.2. Skip-Gram Model 3.1.3. Optimization \\nTechniques: Hierarchical Softmax and Negative Sampling 3.2. GloVe: Global Vectors for \\nWord Representation (Pennington et al., 2014) 3.2.1. Co-occurrence Matrix 3.2.2. The GloVe \\nObjective Function 3.3. fastText (Bojanowski et al., 2017) 3.3.1. Subword Information for \\nOut-of-Vocabulary Words \\n4. Contextualized Embedding Models 4.1. The Limitation of Static Embeddings: Polysemy \\n4.2. ELMo: Embeddings from Language Models (Peters et al., 2018) 4.3. The Transformer \\nArchitecture and Self-Attention 4.4. BERT: Bidirectional Encoder Representations from \\nTransformers (Devlin et al., 2019) 4.5. GPT and its Variants (Radford et al.) \\n5. Embeddings for Other Modalities 5.1. Graph Embeddings 5.1.1. Matrix Factorization \\nMethods 5.1.2. Random Walk-Based Methods (DeepWalk, Node2Vec) 5.2. Image \\nEmbeddings (e.g., from CNNs like ResNet, VGG) 5.3. Multimodal Embeddings (e.g., CLIP \\nfor Text-Image) \\n6. Applications and Case Studies 6.1. Natural Language Processing Tasks 6.1.1. Sentiment \\nAnalysis and Text Classification 6.1.2. Named Entity Recognition 6.1.3. Machine Translation \\n6.2. Information Retrieval and Semantic Search 6.3. Recommendation Systems \\n7. Evaluating Embedding Quality 7.1. Intrinsic Evaluation: Word Analogies, Similarity Tasks \\n7.2. Extrinsic Evaluation: Performance on Downstream Tasks \\n8. Challenges, Ethical Considerations, and Future Directions 8.1. Bias in Embeddings \\n(Gender, Racial) 8.2. Interpretability and Explainability 8.3. Computational Complexity and \\nScalability 8.4. Future Trends: Dynamic and Cross-Lingual Embeddings \\n9. Conclusion \\n10. References'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:37:01+01:00', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'file_path': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': '', 'author': 'ISMAIL LAMRANI', 'subject': '', 'keywords': '', 'moddate': '2025-10-15T15:37:01+01:00', 'trapped': '', 'modDate': \"D:20251015153701+01'00'\", 'creationDate': \"D:20251015153701+01'00'\", 'page': 2}, page_content='1. Introduction \\n1.1. Motivation: The Need for Dense Representations \\nIn the domain of machine learning, algorithms operate on numerical data. However, a significant \\nportion of the world\\'s data is unstructured and symbolic, such as natural language text, social \\nnetworks, or images. A fundamental challenge is to convert this unstructured data into a suitable \\nnumerical format that machine learning models can process effectively. Early attempts, such as one-\\nhot encoding, resulted in extremely high-dimensional and sparse vectors that failed to capture any \\nsemantic relationship between data points. For example, in a one-hot representation, the vectors for \\n\"king\" and \"queen\" are orthogonal, suggesting they are completely unrelated, which is semantically \\nincorrect. This limitation, often referred to as the \"curse of dimensionality,\" necessitates a more \\nsophisticated approach. Dense embeddings provide a solution by mapping discrete objects (like \\nwords or nodes in a graph) to continuous, low-dimensional vectors in a way that preserves \\nunderlying semantic relationships. \\n1.2. The Concept of an Embedding Space \\nAn embedding is a learned representation for a discrete variable as a low-dimensional continuous \\nvector. This mapping is learned from data. The core idea is that objects with similar meanings or \\nfunctions should be positioned close to each other in the learned vector space, known as the \\n\"embedding space.\" Proximity in this space, typically measured by cosine similarity or Euclidean \\ndistance, correlates with semantic similarity. This geometric arrangement allows for powerful \\noperations. For instance, the famous analogy vector(\\'King\\') - vector(\\'Man\\') + \\nvector(\\'Woman\\') results in a vector very close to vector(\\'Queen\\'), demonstrating that the \\nembedding space has captured complex linguistic and social relationships [Mikolov et al., 2013]. \\nThis ability to encode meaning into geometry is the defining characteristic and primary advantage of \\nembeddings. \\n1.3. Problem Statement and Scope \\nThe field of representation learning has grown exponentially, producing a vast array of embedding \\ntechniques tailored to different data types and applications. For newcomers and even seasoned \\npractitioners, navigating this landscape can be daunting. This paper aims to provide a structured and \\ncomprehensive overview of embedding models. The scope of this review is twofold: first, to trace the'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:37:01+01:00', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'file_path': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': '', 'author': 'ISMAIL LAMRANI', 'subject': '', 'keywords': '', 'moddate': '2025-10-15T15:37:01+01:00', 'trapped': '', 'modDate': \"D:20251015153701+01'00'\", 'creationDate': \"D:20251015153701+01'00'\", 'page': 3}, page_content='historical evolution of word embeddings in Natural Language Processing (NLP), from static models \\nlike Word2Vec to contextual models like BERT; and second, to demonstrate the generalization of the \\nembedding concept to non-textual data, specifically graphs and multimodal inputs. We will focus on \\nthe core methodologies, mathematical underpinnings, and key innovations of these models. \\n1.4. Contributions and Paper Structure \\nThis paper makes the following contributions: \\n• \\nA Unified Framework: It presents various embedding models within a unified framework of \\nrepresentation learning, highlighting their conceptual connections and differences. \\n• \\nComprehensive Coverage: It covers static, contextual, graph, and multimodal embeddings, \\nproviding a broad yet detailed survey of the field. \\n• \\nPractical Orientation: It links theoretical models to their practical applications and discusses \\nstandard evaluation protocols for assessing their quality. \\nThe paper is structured as follows: Chapter 2 reviews the theoretical background. Chapter 3 details \\nfoundational static embedding models. Chapter 4 introduces contextual models that address the \\nshortcomings of static ones. Chapter 5 explores embeddings beyond text. Chapters 6 and 7 discuss \\napplications and evaluation methods. Finally, Chapter 8 addresses challenges and future directions \\nbefore concluding in Chapter 9. \\n2. Background and Preliminaries \\n2.1. Vector Space Models (VSMs) \\nThe idea of representing objects as vectors in a multi-dimensional space is not new. Vector Space \\nModels (VSMs) originated in the field of information retrieval in the 1970s [Salton et al., 1975]. In \\nthe classic VSM, documents and queries are represented as vectors of term frequencies (e.g., tf-idf). \\nThe similarity between documents, or between a document and a query, is calculated as the cosine of \\nthe angle between their corresponding vectors. While effective, these early VSMs were sparse and \\nlacked a deep understanding of semantic relationships (e.g., synonymy). Modern embeddings can be \\nseen as a direct and powerful evolution of this paradigm, where the vector representations are dense \\nand learned, rather than sparse and pre-defined. \\n2.2. The Distributional Hypothesis'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:37:01+01:00', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'file_path': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': '', 'author': 'ISMAIL LAMRANI', 'subject': '', 'keywords': '', 'moddate': '2025-10-15T15:37:01+01:00', 'trapped': '', 'modDate': \"D:20251015153701+01'00'\", 'creationDate': \"D:20251015153701+01'00'\", 'page': 4}, page_content='The linguistic foundation for most word embedding models is the distributional hypothesis, \\nfamously summarized by J.R. Firth in 1957 as: \"You shall know a word by the company it keeps.\" \\nThis principle suggests that words that appear in similar contexts are likely to have similar meanings. \\nFor example, words like \"cat\" and \"dog\" will frequently appear in contexts involving \"pet,\" \"food,\" \\n\"house,\" and \"walk.\" Consequently, an effective model should learn vector representations for \"cat\" \\nand \"dog\" that are close to each other in the embedding space. This hypothesis provides the \\ntheoretical justification for training models on large text corpora to learn word meanings purely from \\nstatistical patterns of co-occurrence. \\nExcellent. Let\\'s proceed with Chapter 3. \\n \\n3. Foundational Static Embedding Models \\nThe models discussed in this chapter represent the first major breakthrough in learning dense vector \\nrepresentations for words from large text corpora. They are referred to as \"static\" or \"non-contextual\" \\nbecause they assign a single, fixed vector to each word, regardless of the specific context in which it \\nappears. While this is a significant limitation, these models laid the groundwork for all subsequent \\ndevelopments in the field. \\n3.1. Word2Vec (Mikolov et al., 2013) \\nDeveloped by a team at Google led by Tomas Mikolov, Word2Vec is arguably the model that \\npopularized the use of word embeddings in NLP. It is not a single model but a collection of related \\nmodels that are computationally efficient and effective at learning high-quality word vectors from \\nraw text. Word2Vec is a predictive, window-based model, meaning it learns by predicting a word \\ngiven its local context. It proposes two distinct model architectures: Continuous Bag-of-Words \\n(CBOW) and Skip-Gram. \\n3.1.1. Continuous Bag-of-Words (CBOW) \\nIn the CBOW architecture, the model predicts the current target word based on its surrounding \\ncontext words. For example, given the sentence \"the quick brown ___ jumps over,\" the CBOW \\nmodel is trained to predict the word \"fox\" from the context {\"the\", \"quick\", \"brown\", \"jumps\", \\n\"over\"}. The input vectors of the context words are typically averaged before being fed into a'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:37:01+01:00', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'file_path': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': '', 'author': 'ISMAIL LAMRANI', 'subject': '', 'keywords': '', 'moddate': '2025-10-15T15:37:01+01:00', 'trapped': '', 'modDate': \"D:20251015153701+01'00'\", 'creationDate': \"D:20251015153701+01'00'\", 'page': 5}, page_content='shallow neural network to predict the target word. This architecture is computationally faster and \\ntends to perform well for frequent words. \\n3.1.2. Skip-Gram Model \\nThe Skip-Gram architecture works in the opposite direction of CBOW. Given a single input word, \\nthe model attempts to predict its surrounding context words. For the same example, if the input word \\nis \"fox,\" the model is trained to predict words like \"quick,\" \"brown,\" and \"jumps.\" The Skip-Gram \\nmodel is generally considered to be more effective for learning representations of infrequent words \\nand often produces slightly better results on semantic tasks, albeit at a higher computational cost \\n[Mikolov et al., 2013]. \\n3.1.3. Optimization Techniques: Hierarchical Softmax and Negative Sampling \\nA naive implementation of these models would require a final softmax layer with a neuron for every \\nword in the vocabulary (often numbering in the millions), which is computationally prohibitive. To \\naddress this, two key optimization techniques were introduced: \\n• \\nHierarchical Softmax: This technique replaces the standard softmax layer with a binary tree \\nstructure (specifically, a Huffman tree), where the words in the vocabulary are the leaves. The \\nprobability of a word is calculated as the probability of traversing from the root to its \\ncorresponding leaf. This reduces the complexity from O(V) to O(log2(V)), where V is the \\nvocabulary size. \\n• \\nNegative Sampling: As a more direct and commonly used alternative, negative sampling \\nsimplifies the problem by reframing it as a binary classification task. For a given \\n(target_word, context_word) pair, the model is trained to distinguish it as a true pair \\n(positive sample). Simultaneously, it is fed a small number of randomly selected \"negative \\nsamples\"—pairs of the target word with context words that do not appear in its context \\nwindow. The objective function for Skip-Gram with negative sampling is to maximize the \\nprobability of true pairs and minimize the probability of negative samples.  \\nHere, σ is the sigmoid function, vw and vc′ are the \"input\" and \"output\" vectors for the target word w \\nand context word c, and the second term involves k negative samples drawn from the noise \\ndistribution Pn(w). This method is highly effective and is the more popular choice in practice. \\n3.2. GloVe: Global Vectors for Word Representation (Pennington et al., 2014)'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:37:01+01:00', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'file_path': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': '', 'author': 'ISMAIL LAMRANI', 'subject': '', 'keywords': '', 'moddate': '2025-10-15T15:37:01+01:00', 'trapped': '', 'modDate': \"D:20251015153701+01'00'\", 'creationDate': \"D:20251015153701+01'00'\", 'page': 6}, page_content='Developed at Stanford University, the GloVe model takes a different approach. While Word2Vec is \\na predictive model that learns from local context windows, GloVe is a count-based model that \\nleverages global corpus statistics directly. It is designed to combine the advantages of two major \\nfamilies of methods: global matrix factorization methods (like Latent Semantic Analysis) and local \\ncontext window methods (like Word2Vec). \\n3.2.1. Co-occurrence Matrix \\nThe core of the GloVe model is a word-word co-occurrence matrix, denoted as X. Each entry Xij in \\nthis matrix represents the number of times word j appears in the context of word i. GloVe is built on \\nthe insight that the ratio of co-occurrence probabilities can encode meaning. For example, the ratio \\nP(solid∣ice)/P(solid∣steam) will be large, while the ratio P(gas∣ice)/P(gas∣steam) will be small. GloVe \\nlearns word vectors such that their dot product relates directly to their probability of co-occurrence. \\n3.3. fastText (Bojanowski et al., 2017) \\nDeveloped by Facebook AI Research, fastText extends the ideas of Word2Vec. Its primary \\ninnovation is its handling of word morphology. While Word2Vec and GloVe treat each word as an \\natomic unit, fastText represents a word as a bag of character n-grams. For instance, the word \\n\"embedding\" with 3-grams would be represented by <em, emb, mbe, bed, edd, ddi, din, ing, ng>. \\nThe vector for a word is then computed as the sum of the vectors of its constituent character n-grams. \\nThis subword information provides a major advantage: the ability to generate meaningful vector \\nrepresentations for out-of-vocabulary (OOV) words. If a word was not seen during training, its \\nvector can be constructed from its n-grams, many of which may have been seen in other words. This \\nmakes fastText particularly powerful for morphologically rich languages (e.g., German, Turkish, \\nFinnish) and for tasks involving rare or novel words. \\n4. Contextualized Embedding Models \\nThe primary drawback of static models like Word2Vec and GloVe is their inability to handle \\npolysemy—the fact that a single word can have multiple meanings depending on its context. For \\nexample, the word \"bank\" means something different in \"river bank\" versus \"financial bank.\" Static \\nmodels produce only one vector for \"bank,\" averaging all its different meanings. Contextual models \\nsolve this problem by generating a unique embedding for a word each time it appears, based on the \\nspecific sentence it is in. \\n4.1. The Limitation of Static Embeddings: Polysemy'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:37:01+01:00', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'file_path': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': '', 'author': 'ISMAIL LAMRANI', 'subject': '', 'keywords': '', 'moddate': '2025-10-15T15:37:01+01:00', 'trapped': '', 'modDate': \"D:20251015153701+01'00'\", 'creationDate': \"D:20251015153701+01'00'\", 'page': 7}, page_content='In a static embedding space, the vector for \"bank\" would be located somewhere between the vectors \\nfor words related to finance (like \"money,\" \"loan\") and words related to geography (like \"river,\" \\n\"shore\"). This single representation is a compromise that doesn\\'t accurately capture the meaning in \\neither context. This limitation makes it difficult for models to understand the nuances of language. \\nThe need to resolve this ambiguity was the main driver behind the development of contextualized \\nembeddings. \\n4.2. ELMo: Embeddings from Language Models \\nELMo (Embeddings from Language Models) was one of the first models to effectively address this \\nchallenge. Instead of learning a fixed vector for each word, ELMo uses a deep, bi-directional \\nlanguage model. When given a sentence, ELMo processes it from left-to-right and from right-to-left. \\nThe embedding for a specific word is then generated by combining the internal states of this two-way \\nlanguage model. The result is that the vector for \"bank\" in \"I went to the bank to get cash\" is \\ncomputed based on the entire sentence, making it very different from the vector for \"bank\" in \"We \\nhad a picnic on the river bank.\" ELMo showed that these deep, context-aware representations could \\nlead to massive performance improvements on a variety of difficult NLP tasks. \\n4.3. The Transformer Architecture and Self-Attention \\nThe true revolution in contextual embeddings came with the introduction of the Transformer \\narchitecture. The core innovation of the Transformer is the self-attention mechanism. Conceptually, \\nself-attention allows a model to weigh the importance of all other words in a sentence when it is \\nencoding a specific word. For example, in the sentence, \"The robot picked up the ball because it was \\nheavy,\" self-attention helps the model understand that \"it\" refers to the \"ball,\" not the \"robot.\" It does \\nthis by creating connections between words across the sentence and deciding which connections are \\nmost important for understanding the context. This ability to capture long-range dependencies was a \\nsignificant improvement over previous architectures. \\n4.4. BERT: Bidirectional Encoder Representations from Transformers \\nBERT leverages the power of the Transformer architecture to build a deeply bidirectional model. \\nUnlike ELMo, which processed left-to-right and right-to-left contexts separately, BERT\\'s self-\\nattention mechanism allows it to consider both directions simultaneously for every word. BERT is \\npre-trained on a massive amount of text using two clever tasks: \\n1. Masked Language Model: It takes a sentence, randomly hides (masks) some of the words, \\nand then tries to predict what those hidden words are based on the surrounding un-masked \\nwords. \\n2. Next Sentence Prediction: It takes two sentences and determines if the second sentence is \\nthe one that logically follows the first. \\nBy training on these tasks, BERT develops a profound understanding of language, grammar, and \\ncontext, creating highly effective contextual embeddings that have become a standard in modern \\nNLP. \\n4.5. GPT and its Variants \\nThe GPT (Generative Pre-trained Transformer) family of models also uses the Transformer \\narchitecture but is typically trained for a different purpose. While BERT is designed to understand'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:37:01+01:00', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'file_path': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': '', 'author': 'ISMAIL LAMRANI', 'subject': '', 'keywords': '', 'moddate': '2025-10-15T15:37:01+01:00', 'trapped': '', 'modDate': \"D:20251015153701+01'00'\", 'creationDate': \"D:20251015153701+01'00'\", 'page': 8}, page_content='context from all directions (making it an \"encoder\"), GPT is an \"auto-regressive\" model trained to \\npredict the very next word in a sequence of text (making it a \"decoder\"). This \"generative\" pre-\\ntraining makes GPT models exceptionally good at producing human-like text. Like BERT, their \\ninternal representations serve as powerful contextual embeddings, but their architecture is optimized \\nfor generation tasks like writing summaries, answering questions, and creating content. \\n \\n5. Embeddings for Other Modalities \\nThe power of learning vector representations is not limited to text. The same core principles can be \\napplied to virtually any type of data, allowing us to capture the relationships between nodes in a \\nnetwork, the content of an image, or even combinations of different data types. \\n5.1. Graph Embeddings \\nIn many domains, data is best represented as a graph or network, such as a social network of users or \\na network of protein interactions. Graph embeddings aim to represent each node (e.g., a user) as a \\ndense vector. The goal is for nodes that are close or structurally similar in the graph to have similar \\nvectors in the embedding space. A popular method, Node2Vec, is inspired by Word2Vec. It \\ngenerates \"sentences\" by performing random walks on the graph (like hopping from node to node) \\nand then uses the Skip-Gram model to learn embeddings for each node based on its neighbors in \\nthose walks. \\n5.2. Image Embeddings \\nModern computer vision relies heavily on Convolutional Neural Networks (CNNs). When a CNN \\nlike ResNet or VGG is trained to classify images, it learns to extract increasingly complex features \\nthrough its layers. The output from one of the final layers before the classification head is a dense \\nvector. This vector is effectively an embedding of the image\\'s content. Images with similar content \\n(e.g., two different pictures of a golden retriever) will produce vectors that are close to each other in \\nthe embedding space. These embeddings are used for tasks like image search and clustering. \\n5.3. Multimodal Embeddings \\nOne of the most exciting areas of research is multimodal learning, which involves combining \\ndifferent types of data, such as text and images. Multimodal embeddings map data from two or more \\nmodalities into a single, shared embedding space. For example, a model like CLIP (Contrastive \\nLanguage–Image Pre-training) is trained on a vast dataset of images and their corresponding text \\ncaptions. It learns to produce a text embedding and an image embedding such that the vector for a \\npicture of a dog is close to the vector for the text \"a photo of a dog.\" This shared space enables \\npowerful applications, such as searching for images using natural language descriptions or generating \\ncaptions for a given image. \\n \\n6. Applications and Case Studies'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:37:01+01:00', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'file_path': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': '', 'author': 'ISMAIL LAMRANI', 'subject': '', 'keywords': '', 'moddate': '2025-10-15T15:37:01+01:00', 'trapped': '', 'modDate': \"D:20251015153701+01'00'\", 'creationDate': \"D:20251015153701+01'00'\", 'page': 9}, page_content='Embeddings are not just a theoretical concept; they are the practical foundation for countless modern \\nAI applications. By turning unstructured data into meaningful vectors, they allow us to perform \\nsophisticated calculations and comparisons. \\n6.1. Natural Language Processing Tasks \\n• \\nSentiment Analysis and Text Classification: By converting sentences or documents into \\nembeddings (often by averaging their word embeddings), we can easily feed them into a \\nclassifier to determine sentiment (positive/negative), topic (sports/politics), or spam. \\n• \\nNamed Entity Recognition (NER): Contextual embeddings from models like BERT have \\ndrastically improved the ability of systems to identify entities like people, organizations, and \\nlocations within a text. \\n• \\nMachine Translation: Modern translation systems encode a sentence from a source \\nlanguage into a vector representation and then decode that vector into a sentence in the target \\nlanguage. \\n6.2. Information Retrieval and Semantic Search \\nTraditional search engines rely on keyword matching. Semantic search, powered by embeddings, is \\nfar more powerful. When you search for \"what to wear for a job interview,\" a semantic search engine \\nconverts your query into a vector. It then finds documents whose vectors are closest in meaning, \\neven if they don\\'t contain the exact keywords. It might return articles about \"professional attire\" or \\n\"business casual,\" because the embeddings for these phrases are close to the embedding for your \\nquery. \\n6.3. Recommendation Systems \\nEmbeddings are at the heart of recommendation engines used by companies like Netflix and \\nAmazon. A system can learn an embedding for each user and each item (e.g., a movie). These \\nembeddings are learned such that a user\\'s vector will be close to the vectors of items they have liked \\nin the past. To make a new recommendation, the system simply looks for items in the embedding \\nspace that are near the user\\'s vector but which the user has not yet seen. \\n \\n7. Evaluating Embedding Quality \\nCreating embeddings is one thing, but knowing if they are \"good\" is another. Evaluation methods are \\ntypically split into two categories: intrinsic and extrinsic. \\n7.1. Intrinsic Evaluation \\nIntrinsic evaluations measure the quality of the embeddings directly, often on a specific subtask that \\nis designed to test for certain properties. \\n• \\nWord Analogies: This is the classic test where a model is asked to solve analogies like \"man \\nis to king as woman is to ?\" The model succeeds if the result of vector(\\'king\\') - \\nvector(\\'man\\') + vector(\\'woman\\') is closest to vector(\\'queen\\'). \\n• \\nSimilarity Tasks: In this evaluation, a set of word pairs is given to both humans and a model. \\nThe humans rate the semantic similarity of the words (e.g., \"cup\" and \"coffee\" are more'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:37:01+01:00', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'file_path': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': '', 'author': 'ISMAIL LAMRANI', 'subject': '', 'keywords': '', 'moddate': '2025-10-15T15:37:01+01:00', 'trapped': '', 'modDate': \"D:20251015153701+01'00'\", 'creationDate': \"D:20251015153701+01'00'\", 'page': 10}, page_content='related than \"cup\" and \"car\"). The model\\'s quality is judged by how well its vector similarity \\nscores correlate with the human ratings. \\n7.2. Extrinsic Evaluation \\nExtrinsic evaluations are more practical. They measure the performance of the embeddings on a real-\\nworld \"downstream\" task. The embeddings are used as input features for a larger model to perform a \\ntask like sentiment analysis, text classification, or named entity recognition. The quality of the \\nembeddings is then judged by the final performance of that larger model. If using Embedding A \\nresults in 95% accuracy on a task while Embedding B results in 92% accuracy, then Embedding A is \\nconsidered better for that specific task. \\n \\n8. Challenges, Ethical Considerations, and Future Directions \\nWhile embeddings are incredibly powerful, they are not without their challenges and risks. The field \\nis constantly evolving to address these issues. \\n8.1. Bias in Embeddings \\nSince embeddings are learned from human-generated text, they inevitably capture the biases present \\nin that data. This can have harmful consequences. For example, many early embedding models \\nlearned associations like \"man is to computer programmer as woman is to homemaker.\" This reflects \\nhistorical societal biases in the training text. A significant area of research is dedicated to developing \\ntechniques for identifying and debiasing embeddings to ensure AI systems are fair and equitable. \\n8.2. Interpretability and Explainability \\nEmbeddings are dense vectors of floating-point numbers and are notoriously difficult to interpret. It \\nis hard to look at a 300-dimensional vector and understand why it represents the word \"justice\" or \\nhow it differs from the vector for \"law.\" This lack of interpretability makes them a \"black box,\" \\nwhich can be problematic in sensitive applications where understanding the model\\'s reasoning is \\ncrucial. \\n8.3. Computational Complexity and Scalability \\nTraining large contextual models like BERT and GPT requires immense computational resources, \\nincluding specialized hardware and massive datasets. This can be a barrier for smaller organizations \\nand researchers. Ongoing research focuses on creating smaller, more efficient models (like \\nDistilBERT) and developing more scalable training methods. \\n8.4. Future Trends \\nThe field continues to advance rapidly. Key future directions include: \\n• \\nCross-Lingual Embeddings: Training models that map words from multiple languages into \\na single shared space, enabling better translation and cross-lingual understanding. \\n• \\nDynamic Embeddings: Developing models that can update or adapt their understanding of \\nwords over time to capture how language evolves.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:37:01+01:00', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'file_path': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': '', 'author': 'ISMAIL LAMRANI', 'subject': '', 'keywords': '', 'moddate': '2025-10-15T15:37:01+01:00', 'trapped': '', 'modDate': \"D:20251015153701+01'00'\", 'creationDate': \"D:20251015153701+01'00'\", 'page': 11}, page_content=\"• \\nHyper-personalization: Creating embeddings that are tailored not just to general language, \\nbut to the specific jargon of a company, the writing style of an individual, or the terminology \\nof a scientific domain. \\n \\n9. Conclusion \\nEmbeddings represent a fundamental paradigm shift in how we enable machines to understand and \\nreason about the world. We have traced their evolution from the foundational static models like \\nWord2Vec, which first demonstrated the power of capturing semantic relationships in a vector space, \\nto the sophisticated contextual models like BERT, which can understand linguistic nuance with \\nremarkable accuracy. Furthermore, the extension of these techniques to graphs, images, and \\nmultimodal data has shown that representation learning is a universally applicable concept. \\nEmbeddings now form the backbone of modern AI, from search engines and recommendation \\nsystems to advanced natural language understanding. However, the journey is far from over. Critical \\nchallenges related to bias, interpretability, and computational cost remain at the forefront of the \\nresearch community's attention. As we move forward, the development of fairer, more transparent, \\nand more efficient embedding models will be essential for building the next generation of intelligent \\nsystems. \\n \\n10. References \\n[Salton et al., 1975] Salton, G., Wong, A., & Yang, C. S. (1975). A vector space model for automatic \\nindexing. Communications of the ACM. \\n[Mikolov et al., 2013] Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient estimation of \\nword representations in vector space. arXiv preprint arXiv:1301.3781. \\n[Pennington et al., 2014] Pennington, J., Socher, R., & Manning, C. D. (2014). GloVe: Global \\nvectors for word representation. Proceedings of the 2014 conference on empirical methods in natural \\nlanguage processing (EMNLP). \\n[Bojanowski et al., 2017] Bojanowski, P., Grave, E., Joulin, A., & Mikolov, T. (2017). Enriching \\nword vectors with subword information. Transactions of the Association for Computational \\nLinguistics. \\n[Peters et al., 2018] Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., & \\nZettlemoyer, L. (2018). Deep contextualized word representations. arXiv preprint arXiv:1802.05365. \\n[Devlin et al., 2019] Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training \\nof deep bidirectional transformers for language understanding. Proceedings of the 2019 Conference \\nof the North American Chapter of the Association for Computational Linguistics. \\n[Radford et al., 2018] Radford, A., Narasimhan, K., Salimans, T., & Sutskever, I. (2018). Improving \\nlanguage understanding by generative pre-training. OpenAI.\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:37:01+01:00', 'source': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'file_path': '..\\\\data\\\\pdf\\\\embeddings.pdf', 'total_pages': 13, 'format': 'PDF 1.7', 'title': '', 'author': 'ISMAIL LAMRANI', 'subject': '', 'keywords': '', 'moddate': '2025-10-15T15:37:01+01:00', 'trapped': '', 'modDate': \"D:20251015153701+01'00'\", 'creationDate': \"D:20251015153701+01'00'\", 'page': 12}, page_content=''),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'file_path': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'format': 'PDF 1.7', 'title': '', 'author': 'ISMAIL LAMRANI', 'subject': '', 'keywords': '', 'moddate': '2025-10-15T15:51:03+01:00', 'trapped': '', 'modDate': \"D:20251015155103+01'00'\", 'creationDate': \"D:20251015155103+01'00'\", 'page': 0}, page_content='Title: A Survey of Object Detection: From Region Proposals to End-to-End \\nTransformers \\nAuthor: [Your Name/Institution] Date: October 2025 \\n \\nAbstract \\nObject detection, a fundamental task in computer vision, involves identifying and localizing \\ninstances of objects within an image or video. It goes beyond simple image classification by not only \\ndetermining the class of an object but also providing a bounding box that precisely outlines its \\nlocation. This paper provides a comprehensive survey of the evolution of object detection \\nmethodologies, primarily focusing on the deep learning era. We begin by contextualizing the \\nproblem with a brief overview of traditional computer vision techniques. The core of the review is \\ndedicated to the two dominant paradigms in deep learning-based detectors: two-stage detectors, \\nexemplified by the R-CNN family (R-CNN, Fast R-CNN, Faster R-CNN), which prioritize accuracy \\nthrough a region proposal mechanism; and single-stage detectors, such as YOLO and SSD, which \\noptimize for speed by performing detection in a single pass. We then explore key architectural \\ncomponents like backbone networks, anchor boxes, and non-maximum suppression. The survey \\nculminates with a discussion of modern architectures, including the paradigm-shifting DETR \\n(DEtection TRansformer), which reframes object detection as an end-to-end set prediction problem. \\nFinally, we cover standard evaluation metrics, common datasets, real-world applications, and the \\nongoing challenges and future directions that are shaping the field. \\nKeywords: Object Detection, Computer Vision, Deep Learning, R-CNN, YOLO, SSD, Transformer, \\nDETR, Bounding Box, mAP.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'file_path': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'format': 'PDF 1.7', 'title': '', 'author': 'ISMAIL LAMRANI', 'subject': '', 'keywords': '', 'moddate': '2025-10-15T15:51:03+01:00', 'trapped': '', 'modDate': \"D:20251015155103+01'00'\", 'creationDate': \"D:20251015155103+01'00'\", 'page': 1}, page_content='Table of Contents \\n1. Introduction 1.1. Defining Object Detection: Classification and Localization 1.2. Distinction \\nfrom Other Vision Tasks 1.3. The Importance of Object Detection 1.4. Paper Structure \\n2. Background and Foundational Concepts 2.1. Traditional Computer Vision Approaches \\n(Viola-Jones, HOG) 2.2. The Sliding Window Method 2.3. The Deep Learning Revolution \\n3. Two-Stage Object Detectors: A Focus on Accuracy 3.1. The \"Propose, then Classify\" \\nParadigm 3.2. R-CNN: Regions with CNN Features 3.3. Fast R-CNN: Sharing Computation \\n3.4. Faster R-CNN: The Region Proposal Network (RPN) \\n4. Single-Stage Object Detectors: A Focus on Speed 4.1. The \"Single Pass\" Paradigm 4.2. \\nYOLO: You Only Look Once 4.3. SSD: Single Shot MultiBox Detector \\n5. Key Architectural Components and Innovations 5.1. Backbone Networks: The Feature \\nExtractors 5.2. Anchor Boxes: The Priors for Prediction 5.3. Non-Maximum Suppression \\n(NMS): Pruning Redundant Detections \\n6. Modern Architectures and the Rise of Transformers 6.1. Balancing Speed and Accuracy: \\nEfficientDet 6.2. DETR: End-to-End Object Detection with Transformers \\n7. Evaluation Metrics and Datasets 7.1. Intersection over Union (IoU) 7.2. Average Precision \\n(AP) and mean Average Precision (mAP) 7.3. Landmark Datasets (PASCAL VOC, COCO) \\n8. Applications and Real-World Use Cases 8.1. Autonomous Vehicles 8.2. Medical Imaging \\n8.3. Retail and Inventory Management 8.4. Security and Surveillance \\n9. Challenges and Future Directions 9.1. Detecting Small and Occluded Objects 9.2. The \\nSpeed vs. Accuracy Trade-off 9.3. Domain Adaptation and Generalization 9.4. Few-Shot and \\nZero-Shot Detection \\n10. Conclusion \\n11. References'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'file_path': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'format': 'PDF 1.7', 'title': '', 'author': 'ISMAIL LAMRANI', 'subject': '', 'keywords': '', 'moddate': '2025-10-15T15:51:03+01:00', 'trapped': '', 'modDate': \"D:20251015155103+01'00'\", 'creationDate': \"D:20251015155103+01'00'\", 'page': 2}, page_content='1. Introduction \\n1.1. Defining Object Detection: Classification and Localization \\nObject detection is a core computer vision task concerned with answering two fundamental questions \\nabout an image: \"What objects are in this image?\" and \"Where are they located?\". The first \\nquestion is a classification task, assigning a class label (e.g., \"cat,\" \"car,\" \"person\") to an object. The \\nsecond is a localization task, providing a tight-fitting bounding box (typically defined by x/y \\ncoordinates and width/height) around each identified object. \\n1.2. Distinction from Other Vision Tasks \\nIt is crucial to distinguish object detection from related tasks: \\n• \\nImage Classification: Simply assigns one label to an entire image (e.g., \"this is a picture of a \\ncat\"). \\n• \\nSemantic Segmentation: Assigns a class label to every pixel in the image but does not \\ndistinguish between different instances of the same object (e.g., all pixels belonging to any \\nperson are labeled \"person\"). \\n• \\nInstance Segmentation: Assigns a class label to every pixel and differentiates between \\nobject instances (e.g., \"person 1,\" \"person 2,\" \"person 3\"). Object detection can be seen as a \\nprecursor to this more complex task. \\n1.3. The Importance of Object Detection \\nThe ability to detect and locate objects is foundational to how machines perceive and interact with \\nthe physical world. It is the technology that enables self-driving cars to see pedestrians and other \\nvehicles, allows doctors to identify tumors in medical scans, and helps robots navigate complex \\nenvironments. Its broad applicability has made it one of the most actively researched areas in \\nartificial intelligence. \\n1.4. Paper Structure \\nThis paper will trace the evolution of object detection methods, beginning with a brief look at pre-\\ndeep learning techniques. We will then delve into the two primary families of deep learning'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'file_path': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'format': 'PDF 1.7', 'title': '', 'author': 'ISMAIL LAMRANI', 'subject': '', 'keywords': '', 'moddate': '2025-10-15T15:51:03+01:00', 'trapped': '', 'modDate': \"D:20251015155103+01'00'\", 'creationDate': \"D:20251015155103+01'00'\", 'page': 3}, page_content='detectors: two-stage and single-stage. We will discuss their core components, modern architectures \\nincluding Transformers, and conclude with evaluation metrics, applications, and future challenges. \\n \\n2. Background and Foundational Concepts \\n2.1. Traditional Computer Vision Approaches \\nBefore deep learning, object detection relied on hand-crafted features. Methods like the Viola-Jones \\nframework (famous for real-time face detection) used simple Haar-like features and a cascade of \\nclassifiers. Other approaches used more complex feature descriptors like HOG (Histogram of \\nOriented Gradients), often paired with a classifier like a Support Vector Machine (SVM), to \\nidentify objects. These methods were effective for specific tasks but were brittle and did not \\ngeneralize well. \\n2.2. The Sliding Window Method \\nA common technique was the sliding window approach. A window of a fixed size would be slid \\nacross all possible locations and scales of an image. For each window, a feature descriptor would be \\ncomputed and fed to a classifier. This method was computationally exhaustive and prone to errors. \\n2.3. The Deep Learning Revolution \\nThe success of AlexNet in the 2012 ImageNet classification challenge marked a turning point. \\nResearchers quickly realized that the rich, hierarchical features learned automatically by \\nConvolutional Neural Networks (CNNs) were far more powerful than any hand-crafted features. \\nThis discovery paved the way for the modern era of object detection. \\n \\n3. Two-Stage Object Detectors: A Focus on Accuracy \\nTwo-stage detectors break the object detection problem into two distinct steps, a paradigm that \\ngenerally leads to higher accuracy at the cost of speed. \\n3.1. The \"Propose, then Classify\" Paradigm'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'file_path': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'format': 'PDF 1.7', 'title': '', 'author': 'ISMAIL LAMRANI', 'subject': '', 'keywords': '', 'moddate': '2025-10-15T15:51:03+01:00', 'trapped': '', 'modDate': \"D:20251015155103+01'00'\", 'creationDate': \"D:20251015155103+01'00'\", 'page': 4}, page_content=\"The core idea is to first generate a sparse set of region proposals—areas of the image that are likely \\nto contain an object. In the second stage, a classifier is run only on these proposed regions to \\ndetermine the object's class and refine the bounding box. \\n3.2. R-CNN: Regions with CNN Features \\nR-CNN was the first major breakthrough in applying deep learning to this paradigm. However, its \\nprocess was slow and cumbersome: \\n1. Generate ~2000 region proposals using an external algorithm like Selective Search. \\n2. Warp/resize each proposed region to a fixed size. \\n3. Pass each warped region independently through a pre-trained CNN to extract features. \\n4. Use a set of SVMs to classify the object in each region. \\n3.3. Fast R-CNN: Sharing Computation \\nFast R-CNN made a significant improvement. Instead of running the CNN 2000 times, it passes the \\nentire image through the CNN just once to generate a feature map. The region proposals are then \\nprojected onto this feature map. A novel RoI (Region of Interest) Pooling layer extracts a fixed-size \\nfeature vector from each proposed region, which is then fed into a classifier. This shared computation \\nmade the process much faster. \\n3.4. Faster R-CNN: The Region Proposal Network (RPN) \\nThe bottleneck in Fast R-CNN was the external Selective Search algorithm for proposing regions. \\nFaster R-CNN introduced the Region Proposal Network (RPN), a small neural network that learns \\nto generate high-quality region proposals directly from the CNN features. By integrating the RPN, \\nFaster R-CNN became the first truly end-to-end, unified deep learning object detector, setting a new \\nstandard for accuracy. \\n \\n4. Single-Stage Object Detectors: A Focus on Speed \\nSingle-stage detectors remove the region proposal step and instead perform localization and \\nclassification in a single forward pass of the network, making them extremely fast and suitable for \\nreal-time applications.\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'file_path': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'format': 'PDF 1.7', 'title': '', 'author': 'ISMAIL LAMRANI', 'subject': '', 'keywords': '', 'moddate': '2025-10-15T15:51:03+01:00', 'trapped': '', 'modDate': \"D:20251015155103+01'00'\", 'creationDate': \"D:20251015155103+01'00'\", 'page': 5}, page_content='4.1. The \"Single Pass\" Paradigm \\nThese models treat object detection as a regression problem. They look at the image once and \\ndirectly predict a set of bounding boxes and their corresponding class probabilities. \\n4.2. YOLO: You Only Look Once \\nThe YOLO family of models is renowned for its speed. YOLO divides the input image into a grid. \\nFor each grid cell, the model simultaneously predicts: \\n• \\nSeveral bounding boxes. \\n• \\nA \"confidence\" score for each box, indicating how likely it is to contain an object. \\n• \\nClass probabilities for the object within the box. This unified architecture allows for end-to-\\nend training and blazingly fast inference speeds, making it ideal for video processing. \\n4.3. SSD: Single Shot MultiBox Detector \\nSSD aimed to find a middle ground between the speed of YOLO and the accuracy of Faster R-CNN. \\nIts key innovation is using feature maps from multiple layers of the backbone network to make \\npredictions. By making predictions at different scales, SSD is much better at detecting objects of \\nvarious sizes, particularly small ones, compared to the original YOLO. \\n \\n5. Key Architectural Components and Innovations \\nModern detectors, whether two-stage or single-stage, share several common components. \\n5.1. Backbone Networks: The Feature Extractors \\nThe backbone is a deep CNN (like ResNet, VGG, or MobileNet) pre-trained on a large image \\nclassification dataset (e.g., ImageNet). Its role is to act as a powerful feature extractor, converting the \\nraw pixel data of an image into rich, hierarchical feature maps that can be used for detection. \\n5.2. Anchor Boxes: The Priors for Prediction \\nInstead of predicting bounding boxes from scratch, most detectors predict offsets relative to a set of \\npre-defined default boxes called anchor boxes. These anchors have various sizes and aspect ratios'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'file_path': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'format': 'PDF 1.7', 'title': '', 'author': 'ISMAIL LAMRANI', 'subject': '', 'keywords': '', 'moddate': '2025-10-15T15:51:03+01:00', 'trapped': '', 'modDate': \"D:20251015155103+01'00'\", 'creationDate': \"D:20251015155103+01'00'\", 'page': 6}, page_content='and are tiled across the image at different locations. Using anchors reframes the problem from \\npredicting absolute coordinates to refining a well-placed prior, which makes learning easier for the \\nnetwork. \\n5.3. Non-Maximum Suppression (NMS): Pruning Redundant Detections \\nA detector will often output multiple, highly overlapping bounding boxes for the same object. NMS \\nis a crucial post-processing step that cleans up these redundant detections. It sorts all boxes by their \\nconfidence scores, keeps the box with the highest score, and suppresses (discards) any other boxes \\nthat have a high overlap with it. \\n \\n6. Modern Architectures and the Rise of Transformers \\n6.1. Balancing Speed and Accuracy: EfficientDet \\nThe EfficientDet family of models introduced a systematic way to scale detectors for different \\nresource constraints. It uses a highly efficient backbone (EfficientNet) and a novel feature fusion \\nmechanism (BiFPN) to achieve state-of-the-art efficiency, balancing high accuracy with low \\ncomputational cost. \\n6.2. DETR: End-to-End Object Detection with Transformers \\nDETR (DEtection TRansformer) represents a major paradigm shift. It completely eliminates the need \\nfor hand-crafted components like anchor boxes and NMS. DETR uses a standard Transformer \\nencoder-decoder architecture, similar to those used in NLP. It treats object detection as a direct set \\nprediction problem: the model ingests image features and directly outputs the final set of unique \\nobject detections. This simplifies the detection pipeline significantly and has opened up a new and \\nexciting research direction. \\n \\n7. Evaluation Metrics and Datasets \\n7.1. Intersection over Union (IoU)'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'file_path': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'format': 'PDF 1.7', 'title': '', 'author': 'ISMAIL LAMRANI', 'subject': '', 'keywords': '', 'moddate': '2025-10-15T15:51:03+01:00', 'trapped': '', 'modDate': \"D:20251015155103+01'00'\", 'creationDate': \"D:20251015155103+01'00'\", 'page': 7}, page_content='IoU is the fundamental metric used to measure the \"correctness\" of a predicted bounding box. It is \\ncalculated as the area of overlap between the predicted box and the ground-truth box, divided by the \\narea of their union. A detection is typically considered a \"true positive\" if its IoU with a ground-truth \\nbox is above a certain threshold (e.g., 0.5). \\n7.2. Average Precision (AP) and mean Average Precision (mAP) \\nAverage Precision (AP) is the primary metric for evaluating the performance of a detector on a \\nsingle object class. It is calculated from the precision-recall curve and effectively measures the \\ndetector\\'s accuracy across all confidence levels. Mean Average Precision (mAP) is the average of \\nthe AP values across all object classes and is the standard metric for comparing different object \\ndetection models. \\n7.3. Landmark Datasets \\nThe field has been driven by large-scale, publicly available datasets, most notably PASCAL VOC \\nand Microsoft COCO (Common Objects in Context). The COCO dataset, with its large number of \\nobject categories and instances per image, is the current benchmark for modern object detectors. \\n \\n8. Applications and Real-World Use Cases \\nObject detection is a deployed and impactful technology across numerous industries. \\n• \\nAutonomous Vehicles: Detecting cars, pedestrians, cyclists, and traffic signals is essential \\nfor safe navigation. \\n• \\nMedical Imaging: Assisting radiologists by automatically locating tumors, lesions, or other \\nanomalies in X-rays, CT scans, and MRIs. \\n• \\nRetail: Powering cashier-less stores, monitoring shelf inventory, and analyzing customer foot \\ntraffic. \\n• \\nSecurity and Surveillance: Automatically detecting intruders, unattended baggage, or \\nmonitoring crowd density. \\n \\n9. Challenges and Future Directions'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'file_path': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'format': 'PDF 1.7', 'title': '', 'author': 'ISMAIL LAMRANI', 'subject': '', 'keywords': '', 'moddate': '2025-10-15T15:51:03+01:00', 'trapped': '', 'modDate': \"D:20251015155103+01'00'\", 'creationDate': \"D:20251015155103+01'00'\", 'page': 8}, page_content='Despite immense progress, several challenges remain. \\n• \\nDetecting Small and Occluded Objects: Models still struggle to reliably detect objects that \\nare very small, far away, or partially hidden. \\n• \\nThe Speed vs. Accuracy Trade-off: While models are becoming more efficient, the \\nfundamental trade-off between real-time speed and maximum accuracy remains a key design \\nconsideration. \\n• \\nDomain Adaptation and Generalization: A model trained on daytime, sunny weather data \\nmay fail when deployed at night or in the rain. Improving robustness to new environments is \\na major challenge. \\n• \\nFew-Shot and Zero-Shot Detection: Training models to detect new object categories with \\nvery few (or zero) labeled examples is an active and important area of research.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'file_path': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'format': 'PDF 1.7', 'title': '', 'author': 'ISMAIL LAMRANI', 'subject': '', 'keywords': '', 'moddate': '2025-10-15T15:51:03+01:00', 'trapped': '', 'modDate': \"D:20251015155103+01'00'\", 'creationDate': \"D:20251015155103+01'00'\", 'page': 9}, page_content=\"10. Conclusion \\nObject detection has undergone a remarkable transformation, moving from slow, brittle systems \\nbased on hand-crafted features to highly accurate and efficient end-to-end deep learning models. The \\nevolution from the methodical two-stage R-CNN family to the rapid single-stage YOLO and SSD \\ndetectors, and now to the elegant, anchor-free Transformer-based models like DETR, showcases the \\nfield's rapid pace of innovation. As a core enabling technology for machine perception, object \\ndetection continues to solve critical real-world problems and will undoubtedly remain a central focus \\nof AI research for years to come. \\n \\n11. References \\n[Viola & Jones, 2001] Viola, P., & Jones, M. (2001). Rapid object detection using a boosted cascade \\nof simple features. Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision \\nand Pattern Recognition. \\n[Girshick et al., 2014] Girshick, R., Donahue, J., Darrell, T., & Malik, J. (2014). Rich feature \\nhierarchies for accurate object detection and semantic segmentation. Proceedings of the IEEE \\nconference on computer vision and pattern recognition. \\n[Ren et al., 2015] Ren, S., He, K., Girshick, R., & Sun, J. (2015). Faster R-CNN: Towards real-time \\nobject detection with region proposal networks. Advances in neural information processing systems. \\n[Redmon et al., 2016] Redmon, J., Divvala, S., Girshick, R., & Farhadi, A. (2016). You only look \\nonce: Unified, real-time object detection. Proceedings of the IEEE conference on computer vision \\nand pattern recognition. \\n[Liu et al., 2016] Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C. Y., & Berg, A. C. \\n(2016). SSD: Single shot multibox detector. European conference on computer vision. \\n[Carion et al., 2020] Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., & Zagoruyko, S. \\n(2020). End-to-end object detection with transformers. European conference on computer vision.\"),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:51:03+01:00', 'source': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'file_path': '..\\\\data\\\\pdf\\\\objectdetection.pdf', 'total_pages': 11, 'format': 'PDF 1.7', 'title': '', 'author': 'ISMAIL LAMRANI', 'subject': '', 'keywords': '', 'moddate': '2025-10-15T15:51:03+01:00', 'trapped': '', 'modDate': \"D:20251015155103+01'00'\", 'creationDate': \"D:20251015155103+01'00'\", 'page': 10}, page_content=''),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:54:40+01:00', 'source': '..\\\\data\\\\pdf\\\\proposal.pdf', 'file_path': '..\\\\data\\\\pdf\\\\proposal.pdf', 'total_pages': 8, 'format': 'PDF 1.7', 'title': '', 'author': 'ISMAIL LAMRANI', 'subject': '', 'keywords': '', 'moddate': '2025-10-15T15:54:40+01:00', 'trapped': '', 'modDate': \"D:20251015155440+01'00'\", 'creationDate': \"D:20251015155440+01'00'\", 'page': 0}, page_content='Title: Self-Training with Uncertainty-Aware Style Transfer for Cross-Domain \\nObject Detection \\nPrincipal Investigator: [Your Name] Affiliation: [Your Institution/Research Group] Date: October \\n15, 2025 \\n \\nAbstract \\nModern object detection models achieve remarkable performance but suffer a significant drop in \\naccuracy when deployed in environments (target domains) that differ from their training data (source \\ndomain). This problem of domain shift is a major obstacle to the real-world application of \\ntechnologies like autonomous driving, where a vehicle must operate reliably in diverse weather, \\nlighting, and geographic conditions. This proposal outlines a research project to develop a novel \\nframework for unsupervised domain adaptation in object detection. We propose a method that \\ncombines generative style transfer with a robust self-training mechanism. Specifically, we will use a \\nCycle-Consistent Generative Adversarial Network (CycleGAN) to translate images between \\ndomains, artificially augmenting the training data. More importantly, we will enhance a self-training \\npipeline by incorporating uncertainty estimation. By using techniques like Monte Carlo Dropout, \\nour model will only leverage pseudo-labels from the target domain in which it has high confidence, \\npreventing the accumulation of errors from incorrect labels. We hypothesize that this uncertainty-\\naware approach will make the self-training process more stable and effective, leading to a significant \\nimprovement in object detection performance in unseen target domains. The proposed research will \\nbe evaluated on benchmark datasets like Cityscapes and Foggy Cityscapes, with the goal of creating \\nmore robust and reliable perception systems. \\nKeywords: Research Proposal, Object Detection, Domain Adaptation, Self-Training, Uncertainty \\nEstimation, Style Transfer, Autonomous Vehicles.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:54:40+01:00', 'source': '..\\\\data\\\\pdf\\\\proposal.pdf', 'file_path': '..\\\\data\\\\pdf\\\\proposal.pdf', 'total_pages': 8, 'format': 'PDF 1.7', 'title': '', 'author': 'ISMAIL LAMRANI', 'subject': '', 'keywords': '', 'moddate': '2025-10-15T15:54:40+01:00', 'trapped': '', 'modDate': \"D:20251015155440+01'00'\", 'creationDate': \"D:20251015155440+01'00'\", 'page': 1}, page_content='Table of Contents \\n1. Introduction and Problem Statement 1.1. The Success and Brittleness of Modern Detectors \\n1.2. The Challenge of Domain Shift 1.3. Research Questions and Objectives 1.4. Proposed \\nContribution \\n2. Literature Review 2.1. State-of-the-Art Object Detection Models 2.2. Unsupervised Domain \\nAdaptation (UDA) 2.3. UDA Techniques in Object Detection 2.3.1. Adversarial Training \\nMethods 2.3.2. Style Transfer and Image-to-Image Translation 2.3.3. Self-Training and \\nPseudo-Labeling \\n3. Proposed Methodology 3.1. Overall Framework Architecture 3.2. Module 1: Cross-Domain \\nStyle Transfer 3.3. Module 2: Self-Training with Pseudo-Labeling 3.4. The Core Innovation: \\nUncertainty-Aware Label Filtering \\n4. Experimental Setup and Evaluation 4.1. Datasets and Benchmarks 4.2. Baseline Models \\nfor Comparison 4.3. Evaluation Metrics 4.4. Implementation Details \\n5. Expected Results and Broader Impact 5.1. Hypothesized Performance Gains 5.2. Impact \\non Autonomous Systems and Robotics 5.3. Contribution to the Field of AI \\n6. Plan of Work and Timeline 6.1. Phase 1: Literature Review and Environment Setup \\n(Months 1-2) 6.2. Phase 2: Implementation of Core Modules (Months 3-6) 6.3. Phase 3: \\nExperimentation and Analysis (Months 7-10) 6.4. Phase 4: Dissemination of Results (Months \\n11-12) \\n7. Ethical Considerations \\n8. Conclusion \\n9. References'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:54:40+01:00', 'source': '..\\\\data\\\\pdf\\\\proposal.pdf', 'file_path': '..\\\\data\\\\pdf\\\\proposal.pdf', 'total_pages': 8, 'format': 'PDF 1.7', 'title': '', 'author': 'ISMAIL LAMRANI', 'subject': '', 'keywords': '', 'moddate': '2025-10-15T15:54:40+01:00', 'trapped': '', 'modDate': \"D:20251015155440+01'00'\", 'creationDate': \"D:20251015155440+01'00'\", 'page': 2}, page_content='1. Introduction and Problem Statement \\n1.1. The Success and Brittleness of Modern Detectors \\nDeep learning-based object detectors like Faster R-CNN and YOLO have become incredibly \\naccurate, forming the perception backbone for many emerging technologies. However, their success \\nis predicated on the assumption that the training and testing data are drawn from the same statistical \\ndistribution. \\n1.2. The Challenge of Domain Shift \\nIn the real world, this assumption is frequently violated. A model trained exclusively on clear, sunny \\nday driving data may fail catastrophically when deployed at night, in the rain, or in a city with \\ndifferent architecture. This phenomenon is known as domain shift. Manually annotating data for \\nevery possible domain is prohibitively expensive and unscalable. Therefore, Unsupervised Domain \\nAdaptation (UDA), which aims to adapt a model trained on a labeled source domain to an unlabeled \\ntarget domain, is a critical area of research. \\n1.3. Research Questions and Objectives \\nThis research aims to answer the following question: How can we make an object detector robust to \\ndomain shift without requiring any labeled data from the new domain? Our primary objectives are: \\n1. To design a framework that leverages both image-level style translation and model-level self-\\ntraining. \\n2. To develop a novel mechanism to filter noisy pseudo-labels generated during self-training by \\nestimating model uncertainty. \\n3. To empirically validate the proposed framework and demonstrate its superiority over existing \\nUDA methods. \\n1.4. Proposed Contribution \\nThe main contribution of this work will be an uncertainty-aware self-training framework. While \\nself-training is a common technique, it is often unstable because the model can reinforce its own \\nmistakes by trusting incorrect \"pseudo-labels.\" By introducing a principled mechanism for the model'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:54:40+01:00', 'source': '..\\\\data\\\\pdf\\\\proposal.pdf', 'file_path': '..\\\\data\\\\pdf\\\\proposal.pdf', 'total_pages': 8, 'format': 'PDF 1.7', 'title': '', 'author': 'ISMAIL LAMRANI', 'subject': '', 'keywords': '', 'moddate': '2025-10-15T15:54:40+01:00', 'trapped': '', 'modDate': \"D:20251015155440+01'00'\", 'creationDate': \"D:20251015155440+01'00'\", 'page': 3}, page_content='to gauge its own uncertainty, we can select only the most reliable pseudo-labels, leading to more \\nstable and effective adaptation. \\n \\n2. Literature Review \\n2.1. State-of-the-Art Object Detection Models \\nOur work will build upon established object detection architectures. We will consider both a two-\\nstage detector (e.g., Faster R-CNN) and a Transformer-based detector (e.g., DETR) as the base \\nmodels for our adaptation framework. \\n2.2. Unsupervised Domain Adaptation (UDA) \\nUDA is a well-established field in machine learning. The central goal is to leverage a label-rich \\nsource domain to learn a task in a label-scarce target domain. \\n2.3. UDA Techniques in Object Detection \\n• \\nAdversarial Training: These methods use a \"domain discriminator\" network that tries to \\ndistinguish between features from the source and target domains. The main network is then \\ntrained to produce features that can \"fool\" this discriminator, thereby learning domain-\\ninvariant features. \\n• \\nStyle Transfer: Generative models like GANs are used to translate images from the source \\nstyle to the target style (e.g., making a sunny image look foggy). This creates a synthetic \\nlabeled dataset in the target style. \\n• \\nSelf-Training: This involves using an initial model to make predictions on the unlabeled \\ntarget data. The most confident predictions are then treated as \"pseudo-labels\" and are used to \\nretrain the model. This approach is powerful but risks error accumulation if the pseudo-labels \\nare noisy. Our proposed work directly addresses this key limitation. \\n \\n3. Proposed Methodology \\n3.1. Overall Framework Architecture'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:54:40+01:00', 'source': '..\\\\data\\\\pdf\\\\proposal.pdf', 'file_path': '..\\\\data\\\\pdf\\\\proposal.pdf', 'total_pages': 8, 'format': 'PDF 1.7', 'title': '', 'author': 'ISMAIL LAMRANI', 'subject': '', 'keywords': '', 'moddate': '2025-10-15T15:54:40+01:00', 'trapped': '', 'modDate': \"D:20251015155440+01'00'\", 'creationDate': \"D:20251015155440+01'00'\", 'page': 4}, page_content='The proposed system will consist of three interconnected modules operating on a base object \\ndetector. The model will be trained on labeled source data (e.g., sunny images) and unlabeled target \\ndata (e.g., rainy images). \\n3.2. Module 1: Cross-Domain Style Transfer \\nWe will first train a CycleGAN model to learn the mappings between the source and target domains. \\nThis will allow us to translate a source image into a \"fake\" target image (e.g., sunny -> rainy) and \\nvice-versa. This provides a basic form of data augmentation, allowing the detector to see labeled \\nimages that look like they are from the target domain. \\n3.3. Module 2: Self-Training with Pseudo-Labeling \\nIn parallel, we will use the model trained on the source data to generate predictions (bounding boxes \\nand classes) for the unlabeled target domain images. These predictions will serve as initial pseudo-\\nlabels. \\n3.4. The Core Innovation: Uncertainty-Aware Label Filtering \\nThis is the central component of our proposal. Instead of naively trusting all pseudo-labels above a \\nsimple confidence threshold, we will estimate the model\\'s uncertainty for each prediction. We will \\nuse Monte Carlo Dropout, a technique where dropout is applied at inference time over multiple \\nforward passes. The variance in the resulting predictions serves as a strong indicator of model \\nuncertainty. We will then filter the pseudo-labels using a combined score of confidence and low \\nuncertainty. Only the most certain and confident pseudo-labels will be added to a replay buffer used \\nto fine-tune the detector, making the adaptation process robust to noise. \\n \\n4. Experimental Setup and Evaluation \\n4.1. Datasets and Benchmarks \\nWe will focus on autonomous driving scenarios. The primary experiment will be adapting from the \\nCityscapes dataset (clear weather) to the Foggy Cityscapes dataset. We will also evaluate on other \\ncommon shifts, such as adapting from synthetic data (Sim10k) to real-world data (KITTI).'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:54:40+01:00', 'source': '..\\\\data\\\\pdf\\\\proposal.pdf', 'file_path': '..\\\\data\\\\pdf\\\\proposal.pdf', 'total_pages': 8, 'format': 'PDF 1.7', 'title': '', 'author': 'ISMAIL LAMRANI', 'subject': '', 'keywords': '', 'moddate': '2025-10-15T15:54:40+01:00', 'trapped': '', 'modDate': \"D:20251015155440+01'00'\", 'creationDate': \"D:20251015155440+01'00'\", 'page': 5}, page_content='4.2. Baseline Models for Comparison \\nWe will compare our method against three baselines: \\n1. A \"Lower Bound\" model trained only on source data. \\n2. A state-of-the-art adversarial training method for UDA. \\n3. A standard self-training method without uncertainty awareness. \\n4.3. Evaluation Metrics \\nPerformance will be measured using the standard object detection metric, mean Average Precision \\n(mAP), calculated on the labeled validation set of the target domain. \\n \\n5. Expected Results and Broader Impact \\n5.1. Hypothesized Performance Gains \\nWe expect our uncertainty-aware framework to significantly outperform the baselines. We \\nhypothesize that by reducing the noise in the pseudo-labeling process, our model will adapt more \\neffectively, resulting in a 5-10% absolute improvement in mAP on the target domain compared to \\nstandard self-training methods. \\n5.2. Impact on Autonomous Systems and Robotics \\nA more robust perception system directly translates to increased safety and reliability for \\nautonomous vehicles, drones, and industrial robots. This research could help bridge the gap between \\ndevelopment and real-world deployment of these technologies. \\n \\n6. Plan of Work and Timeline \\nThe project is planned for a 12-month period. \\n• \\nPhase 1 (Months 1-2): In-depth literature review; setting up the computational environment \\nand baseline models.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:54:40+01:00', 'source': '..\\\\data\\\\pdf\\\\proposal.pdf', 'file_path': '..\\\\data\\\\pdf\\\\proposal.pdf', 'total_pages': 8, 'format': 'PDF 1.7', 'title': '', 'author': 'ISMAIL LAMRANI', 'subject': '', 'keywords': '', 'moddate': '2025-10-15T15:54:40+01:00', 'trapped': '', 'modDate': \"D:20251015155440+01'00'\", 'creationDate': \"D:20251015155440+01'00'\", 'page': 6}, page_content='• \\nPhase 2 (Months 3-6): Implementation of the style transfer module and the uncertainty-\\naware self-training loop. \\n• \\nPhase 3 (Months 7-10): Conducting extensive experiments on benchmark datasets, \\nanalyzing results, and performing ablation studies. \\n• \\nPhase 4 (Months 11-12): Writing a research paper for submission to a top-tier computer \\nvision conference (e.g., CVPR, ICCV) and finalizing the project report. \\n \\n7. Ethical Considerations \\nThe primary application of this research is to enhance safety in autonomous systems. However, \\nobject detection technology can also be used for surveillance. Our research will be conducted \\ntransparently, and we will focus our evaluation on publicly available datasets related to driving. We \\nwill not use private or personally identifiable data. The code and models will be made publicly \\navailable to ensure reproducibility and encourage positive use. \\n \\n8. Conclusion \\nThis research proposal addresses the critical problem of domain shift in object detection. By \\nproposing a novel framework that integrates style transfer with a more robust, uncertainty-aware \\nself-training mechanism, we aim to significantly advance the state of the art in unsupervised domain \\nadaptation. The successful completion of this project will produce more reliable perception models, \\nthereby accelerating the safe and responsible deployment of AI in real-world applications. \\n \\n9. References \\n[Hoffman et al., 2018] Hoffman, J., Tzeng, E., Park, T., Zhu, J. Y., Isola, P., Saenko, K., ... & \\nDarrell, T. (2018). CyCADA: Cycle-consistent adversarial domain adaptation. International \\nconference on machine learning.'),\n",
       " Document(metadata={'producer': 'Microsoft® Word\\xa0LTSC', 'creator': 'Microsoft® Word\\xa0LTSC', 'creationdate': '2025-10-15T15:54:40+01:00', 'source': '..\\\\data\\\\pdf\\\\proposal.pdf', 'file_path': '..\\\\data\\\\pdf\\\\proposal.pdf', 'total_pages': 8, 'format': 'PDF 1.7', 'title': '', 'author': 'ISMAIL LAMRANI', 'subject': '', 'keywords': '', 'moddate': '2025-10-15T15:54:40+01:00', 'trapped': '', 'modDate': \"D:20251015155440+01'00'\", 'creationDate': \"D:20251015155440+01'00'\", 'page': 7}, page_content='[Saito et al., 2017] Saito, K., Watanabe, K., Ushiku, Y., & Harada, T. (2017). Asymmetric tri-\\ntraining for unsupervised domain adaptation. International conference on machine learning. \\n[Gal & Ghahramani, 2016] Gal, Y., & Ghahramani, Z. (2016). Dropout as a Bayesian approximation: \\nRepresenting model uncertainty in deep learning. International conference on machine learning. \\n[Zou et al., 2018] Zou, Y., Yu, Z., Kumar, B., & Wang, J. (2018). Unsupervised domain adaptation \\nfor semantic segmentation via class-balanced self-training. European conference on computer vision.')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Directory loader \n",
    "from langchain_community.document_loaders import PyPDFLoader,PyMuPDFLoader\n",
    "## load all text files from the directory\n",
    "dir_loader=DirectoryLoader(\n",
    "    \"../data/pdf\",\n",
    "    glob=\"*.pdf\",\n",
    "    loader_cls=PyMuPDFLoader,\n",
    "    #loader_kwargs={\"encoding\":\"utf-8\"},\n",
    "    show_progress=False\n",
    "    \n",
    ")\n",
    "pdf_documents = dir_loader.load()\n",
    "pdf_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a00aa95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "YTRAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
